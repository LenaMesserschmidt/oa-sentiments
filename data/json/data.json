{
    "data": [
        {
            "created": "2010-07-21",
            "from": "reuss",
            "content": "Die Anhörung des Bundesjustizministeriums am 13. Juli zur möglichen Novellierung des Urheberrechts und zur Rolle von ›Open Access‹ zeitigt schon jetzt erstaunliche Früchte. Früh hatte sich in der Anhörung abgezeichnet, daß die verfassungs- und wirklichkeitsfremden Wünsche der ›Allianz der deutschen Wissenschaftsorganisationen‹ und des Deutschen Bibliotheksverbands nach Einführung einer allgemeinen Wissenschaftsschranke keine Chance auf Verwirklichung haben werden. Diese Schranke hätte wissenschaftliche Autoren zentraler Rechte ihrer Urheberschaft beraubt. Nun konzentriert man sich auf das Vorführen eines Taschenspielertricks. Daß zu denjenigen, die ihre Verstaatlichungsphantasien wenn schon nicht in der wirklichen Welt, dann doch wenigstens in der virtuellen ›realisieren‹ wollen, auch CDU-Bildungspolitiker gehören, ist dabei ein Novum. Der Trick, den Michael Kretschmer und Tankred Schipanski mit Pressemitteilung der CDU/CSU-Fraktion vom 13. Juli der erstaunten Öffentlichkeit vorführen, heißt: unabdingbares Zweitveröffentlichungsrecht. Mit der Forderung nach ihm wollen sie den zunehmend erlahmenden ›Open Access‹-Zug wieder in Fahrt bringen und zugleich denen ein Schnippchen schlagen, die ihn vermeintlich behindern, den selbständigen Verlagen. Was ist mit »unabdingbarem Zweitveröffentlichungsrecht« gemeint? Die ›Allianz der deutschen Wissenschaftsorganisationen‹ hatte in ihrer Stellungnahme zur Prüfbitte des Bundesjustizministeriums den Köder mit folgenden Worten ausgeworfen: »Als zwingende Regelung im Urhebervertragsrecht sollte wissenschaftlichen Autoren nach einer angemessenen Embargofrist ein unabdingbares und formatgleiches Zweitveröffentlichungsrecht für ihre Aufsätze und unselbständig erschienenen Werke eingeräumt werden. Dieses Zweitveröffentlichungsrecht, das für den Wissenschaftler keine Pflicht bedeutet, ist notwendig, um ihn in seiner Verhandlungsposition gegenüber großen wissenschaftlichen Verlagen zu stärken. Der Wissenschaftler erhält durch das Zweitveröffentlichungsrecht die Möglichkeit, selbst über den Grad der Sichtbarkeit seiner Forschungsergebnisse zu entscheiden. Er übt dabei in besonderer Weise das Grundrecht der Wissenschaftsfreiheit aus.« Was hier pathetisch im Sinne einer Ausweitung individueller Freiheitsspielräume der Wissenschaftler gefordert wird, schwächt in Wahrheit die Position des Autors beträchtlich. Denn wenn der Autor einem Verlag, der in seine Publikation investiert, kein zeitlich begrenztes ausschließliches Nutzungsrecht mehr anbieten kann, wird seine Souveränität nicht gestärkt, sondern beschnitten. Er verliert seine Vertragsfreiheit. Das Investionsrisiko des Verlags wird zu groß – und dem Autor wird dann in der Regel nur übrigbleiben, sein unlektoriertes und unbeworbenes ›Papier‹ im ach so überschaubaren Netz allein »sichtbar« zu machen. All das ist längst reflektiert und öffentlich breit diskutiert, aber die beiden Bildungs-Experten der CDU/CSU scheinen davon noch nichts gehört zu haben. Anstelle von fragwürdigen Interventionen in diesen heiklen Sektor von Individualrechten wäre es an der Zeit, daß die Bildungspolitiker aller Fraktionen und die Verantwortlichen in den Ländern ein wenig Selbstkritik an den Tag legten. Wenn die Wissenschaftsministerien aller Bundesländer in den letzten Jahren die Bibliotheksetats ohne öffentlich vernehmbaren Protest kontinuierlich reduzieren konnten, dann ist darin der vielleicht wichtigste Grund für die aktuelle Misere zu finden. Und es wäre ein Zeichen von Zivilcourage beim Führungspersonal der großen Bibliotheken gewesen, die Öffentlichkeit über den sich abzeichnenden Kollaps ihrer Anschaffungskraft zu informieren – auch auf die Gefahr hin, sich dabei im politischen Feld unbeliebt zu machen. Daß es im Bereich der Naturwissenschaften international agierende Monopolverlage mit mehr oder weniger erpresserischer Preisgestaltung gibt (Elsevier, Thompson, Wiley, Springer etc.), ist unbestritten. Die bekämpft man aber nicht mit Änderungen im nationalen Urheberrecht und auch nicht, indem man Autoren wider deren Willen zu ›Open Access‹-Publizieren drängt. Wie die Erfahrung zeigt, haben gerade die internationalen Großverlage schon längst selbst ›Open Access‹-Modelle adaptiert – ohne daß daraus eine Kostendämpfung resultierte. Im Gegenteil. Und das heißt: Mit der Forderung nach einem unabdingbaren Zweitveröffentlichungsrecht werden genau die getroffen, die angeblich nicht gemeint waren: die kleineren und mittleren Verlage. Die Großen werden sich ins Fäustchen lachen und sich freuen, weitere Labels billig übernehmen zu können. Nach Abwägung von Nutzen und Schaden möglicher Interventionen bleibt zur Bekämpfung des Preismissbrauchs eigentlich nur der Weg des Kartellrechts. In diesem Punkt kann man den USA getrost folgen. In deren Wirtschaftsgeschichte gibt es genügend Beispiele dafür, wie Teilbereiche eines Unternehmens dem Wettbewerb unterworfen oder einer Stiftung übergeben werden können. In diese Richtung sollten sich Autoren, Wissenschaftsorganisationen und Verlage mit angemessener Preisgestaltung gemeinsam bewegen. Versuche, aus Ratlosigkeit oder Populismus die Vertragsfreiheit der wissenschaftlichen Autoren einzuschränken, lähmen die Eigenverantwortung der Betroffenen und sind mit Blick auf Förderung wissenschaftlichen Fortschritts kontraproduktiv.",
            "sentiment_score": 0,
            "magnitude_score": 0
        },
        {
            "created": "2009-02-09",
            "from": "reuss",
            "content": "Niemand, betonen Wissenschaftsrat, DFG und die Kultusministerkonferenz der Länder, kann leugnen, daß der kostenlose Zugang zu wissenschaftlichen Publikationen eine gute Sache ist. Und daß Mittel und Wege gefunden werden müssen, die Produkte deutscher wissenschaftlicher Forschung von den Servern der deutschen Universitätsbibliotheken aus ›global‹, wie es so schön heißt, ›sichtbar‹ zu machen. Und daß dabei auch solche spießig uncoolen Interessen wie die mittelständischer Wissenschaftsverlage hintanstehen müssen. Und daß, angesichts der Herausforderungen globalisierter Wissenschaft (oder so ähnlich), die Rechte der Autoren, und sei’s nur das, über die Publikationsform ihrer Werke selbst bestimmen zu können, letztlich zu vernachlässigen sind. Ich bin dieser Niemand; ich leugne es gerade zu. Es wäre zu wünschen, daß sich Wissenschaftsrat, DFG und die Kultusministerkonferenz der Länder auf ihr eigentliches Geschäft: die Förderung und Ermutigung wissenschaftlicher Initiativen, beschränkten und die Finger von allen Versuchen gelassen hätten, dirigistisch in das komplexe Feld der Publikationsfreiheit und des Urheberrechts einzugreifen. Dem Versuch einer klammheimlichen technokratischen Machtergreifung, die in diesen Bereichen geplant und partiell mit Billigung der unkundigen und hilflosen Opfer (Wissenschaftler) schon vollzogen ist, muß entgegengetreten werden. Aus kulturellen, aus rechtlichen, aber auch aus finanziellen Gründen. Open Access – der Name dieser technokratischen Aktion schreit fast schon nach dem Einschreiten eines Drogenbeauftragten – scheint prima vista unglaublich geil, aber ist wissenschaftliches Publizieren nach diesem Modell wirklich so billig, ja umsonst zu haben, wie seine Herolde die Öffentlichkeit glauben machen wollen? Niemand hat sich bisher der Mühe unterzogen, einmal die gesamtgesellschaftlichen Kosten tatsächlich en detail zu berechnen, obschon sich die Ideologen und Mitläufer der Bewegung nicht nur gerne mit dem Lametta der Demokratisierung schmücken, sondern meist auch noch den vergoldeten Taschenrechnerorden am Band mit sich führen. Dort, wo ein zweiter Niemand, Uwe Jochum, sich einmal die Mühe gemacht hat, den Dingen auf den Grund zu gehen (in einem instruktiven Beitrag zur Ökonomie der sog. »Nationallizenzen«, gedruckt im jüngsten »TEXT«-Heft, 12/2008), trat Erstaunliches zutage. Aus Jochums Recherchen nur ein Beispiel: Die Yale-Universität mußte im Jahre 2007 die Subventionierung jener Autoren, die auf der dem Open Access-Modell folgenden medizinischen Informationsplattform »Biomed Central« publizieren wollten, beenden, weil die Publikationskosten derart in die Höhe zu schießen begannen, daß auch die Finanzkraft von Yale dafür nicht mehr ausreichte. So hatte Yale im Jahre 2005 4.648 Dollar investieren müssen, um interessierten Hochschulangehörigen die Publikation ihrer Arbeiten in »Biomed Central« zu finanzieren. Im Jahre 2006 waren es dann schon 31.625 Dollar und 2007, dem Jahr des Abbruchs, bis Juni 29.635 Dollar, mit weiteren 34.965 Dollar an erwarteten Kosten bis Jahresende, zusammen also 64.600 Dollar für 2007. Das ist ein Betrag, der den Abonnementkosten von rund 14 biomedizinischen Fachzeitschriften entspricht. Geil. Ohnedies basieren die messianischen Verheißungen des Open Access-Wahns auf Milchmädchenrechnungen, deren blendende Grundoperation eine überaus schlichte ist: Öffentlich sichtbare Kosten (Zeitschriftenabonnements, Bücherkäufe, kurz: Außenweltbeziehungen) werden in unsichtbare (immanente, komplett durch Steuermittel beglichen) verwandelt. Server, Eingabegeräte, Bildschirme etc. und tariflich bezahlte Angestellte (meist durch Drittmittel von wem wohl finanziert) scheinen naturwüchsig vorhanden und werden buchungstechnisch in den Rechnungen erst gar nicht aufgeführt, obschon sie laufend beträchtliche Gelder verschlingen. Die Kosten, die Verlage für Satz, Druck und Lektorat (ja, das soll trotz des massiven Kostendrucks, der durch die opportunistische Subventionierung der subprime Digitalisierungsblase entstanden ist, noch existieren) ausgeben, werden zudem komplett auf die Autoren abgewälzt – mit verheerenden Folgen für die Apperzeption wissenschaftlicher Arbeiten. Standard: Times New Roman in Blocksatz ohne Silbentrennung und Dauerfolter durch falsche Apostrophe und Anführungszeichen, kurz: digitale typographische Massengräber. Zu lesen gibt es da nichts mehr. Eine neue Publikation zu bewerben und, wo dies nötig ist, auch gegen Widerstand öffentlich zu etablieren, kann man sich schließlich ebenfalls sparen, denn die server der Universitätsbibliotheken sind ein so überaus attraktiver Ort, daß man sich eigentlich nur wundern kann, warum sich genau dort so viele mittelmäßige Dissertationen breitgemacht haben und die besseren alle gedruckt vorliegen – und gerade nicht auf diesen servern. Nun sollen, ohne jegliche Technikfolgeabschätzung, die ideologischen Forderungen der Open Access-Aktivisten auf breiter Front durchgesetzt werden – mit verheerenden Folgen für das Urheberrecht. Es ist zu bedauern, daß die DFG sich hierbei zunehmend als Lobbyist profiliert und aktiv den Versuch vorantreibt, einen staatsmonopolistischen Verwertungskreislauf in Gang zu bringen. Für diese starke These gibt es mehr als nur schwache Indizien. So erwartet die DFG inzwischen, wie sie in ihren Richtlinien zu Open Access schreibt, »dass die mit ihren Mitteln finanzierten Forschungsergebnisse publiziert und dabei möglichst auch digital veröffentlicht und für den entgeltfreien Zugriff im Internet (Open Access) verfügbar gemacht werden.« Das läuft, ist einem die nichtrückrufbare Natur digitaler Materialien vertraut, auf eine kollektive Enteignung hinaus. Was in den 2006 publizierten Richtlinien noch als Erwartungshaltung formuliert ist und somit Wahlfreiheit suggeriert, zeigt sein wahres Gesicht, wenn man die Konkretionen liest, die der DFG-Unterausschuß für elektronisches Publizieren zum Thema beizutragen hat. Dort hält man das Zögern der Wissenschaftler beim digitalen Umstieg für eine »Generationenfrage«, nennt die Zögernden »konservative Fachvertreter« und stellt mit bedauerndem Unterton fest, daß die Hochschulleitungen, die doch »am ehesten einen gewissen (institutionellen) Druck ausüben könnten«, bislang »eher zurückhaltend« agieren. Um daher eine institutionelle Wende herbeizuführen, spielt man mit dem Gedanken, in Deutschland ein »Cream of Science«-Programm (che bella espressione!) aufzulegen, mit dem man »herausragende Forscherpersönlichkeiten als Vorreiter« der digitalen Informationsdispositive gewinnen will. Bei der Frage, wer diese Wissenschaft con crema servieren soll, denkt man, wer hätte das gedacht, an die Leibniz-Preisträger. Leichtfertig aufs Spiel gesetzt wird mit solchen dirigistischen Aktionen nicht nur die bewährte Infrastruktur mittelständischer Wissenschaftsverlage (eine der wenigen Branchen, auf die Deutschland im internationalen Vergleich wirklich stolz sein kann); in Frage gestellt wird auch eine Errungenschaft, die in den letzten 250 Jahren zum Aufblühen einer wissenschaftlichen Kultur geführt hat, um die uns jeder beneidet: das Recht, als Wissenschaftler im Rahmen der staatlich finanzierten Universitäten und Forschungseinrichtungen frei zu forschen und zu lehren und eben auch darüber zu bestimmen, wo das erscheinen soll, was man erdacht und erforscht hat – gerade auch unter Verwertungsgesichtspunkten. Wer eine Phantasie davon gewinnen will, was damit auf dem Spiel steht, sei auf Lessings denkwürdigen Text aus den siebziger Jahren des achtzehnten Jahrhunderts verwiesen, der den schönen Titel »Leben und leben lassen« trägt. Gründungsurkunde des Urheberrechts. Der Versuch, auf die freie Wahl der Publikationsform Einfluß zu nehmen, begegnet zunächst dort, wo Antragsteller auf Druckkostenzuschüsse zu wissenschaftlichen Publikationen mittlerweile standardmäßig mit der Frage konfrontiert werden, warum etwa eine summa cum laude-Dissertation überhaupt noch gedruckt werden müsse und nicht einfach digital publiziert werden könne – eine Frage, die sich kaum auf einem Formblatt, sondern nur in einer umfänglichen Abhandlung klären ließe, die den gesamten Zusammenhang, der zu ihrer Formulierung geführt hat, ihrerseits noch einmal in Frage stellte. Er begegnet aber auch in den eigentümlich gewundenen Dissimulationen, die sich im Kontext der Exzellenzinitiativen in öffentliche Schreiben einschleichen. So teilt etwa der Rektor der Universität Konstanz seinen Kolleginnen und Kollegen am 13. Januar 2009 mit, die Universität sei »verpflichtet, aufgrund ihres Status als Exzellenzuniversität jährlich einen Evaluationsbericht an den Wissenschaftsrat zu liefern. In diesem Bericht werden u.a. die Publikationen aus der Universität für jeweils einen bestimmten Zeitraum abgefragt.« Der Rektor bittet sodann seine Kolleginnen und Kollegen, »uns bei der Erhebung dieser Daten zu unterstützen. […]. Bitte liefern Sie auch die Volltexte der Publikationen im pdf-Format mit, sofern sie Ihnen vorliegen.« Warum überhaupt die durchaus sinnvolle Einrichtung einer Bibliographie mit dem Beliefern eines Volltextservers gekoppelt wird, bleibt schleierhaft. Es ist schön zu beobachten, wie im Verfolg des Schreibens beide Perspektiven immer wieder kunstvoll vermischt werden: »Die Daten werden im Institutional Repository [oioioi!] der Universität KOPS gespeichert. Wo erwünscht und möglich, werden die Volltexte nach den Prinzipien des Open Access online zur Verfügung gestellt. Bei den Veröffentlichungen, bei denen das nicht erwünscht oder aus urheberrechtlichen Gründen nicht möglich ist, werden nur die beschreibenden bibliographischen Daten (Autor, Titel, Erscheinungsjahr usw.) freigeschaltet. Damit entsteht gleichzeitig ab dem Jahr 2008ff. eine Universitätsbibliographie.« Das Wort »gleichzeitig« ist hier verräterisch, denn man kann das doch nur so verstehen, daß das Hauptaugenmerk auf der Etablierung des Volltextservers liegt. Und wenn man im folgenden dem Singular »Datenbank« begegnet, weiß man buchstäblich nicht mehr, von welcher die Rede ist: »Diese Datenbank bietet sowohl den Fachbereichen als auch den Autoren die Möglichkeit, jederzeit die gewünschten Daten zum Publikations-Output [sic!] einer Person oder eines Fachbereiches gebündelt finden zu können. Die Einzelerhebungen für diverse Evaluationen, Akkreditierungen usw. werden damit erleichtert. Ihre Arbeiten erhalten durch diesen Nachweis eine höhere [obacht:] Sichtbarkeit in Katalogen und Suchmaschinen, und sie werden in KOPS langfristig archiviert und zur Verfügung gestellt, ohne dass Sie sich nochmals darum kümmern müssten.« Das nennt man akronymischen Service. In Zürich ist man schon weiter und semantisch nicht gar so zimperlich – und man hat auch ein deutlich stylisheres Akronym, ZORA. Dort hat die Universität ihre Wissenschaftler (sie heißen dort Wissenschafter) in einer autoritären Diktion, die an einen Gestellungsbefehl gemahnt, dazu »verpflich­tet«, eine digitale Version ihrer Arbeiten auf ZORA »zu hinterlegen«, »sofern«, wie es heißt, »dem keine rechtlichen Hindernisse entgegenstehen«. Wer sich einmal einen Eindruck davon verschaffen will, in welcher Tonlage eine digitale Diktatur vor sich hinsummt, wenn ihr niemand Einhalt gebietet, dem seien eindringlich die Anweisungen der Zürcher Universität zu besagter ZORA empfohlen. An sich geböte es schon die minime Selbstachtung, die unstatthaften und unverschämten Zumutungen dieser bürokratischen Prose bis in die letzte Instanz mit Prozessen zu überziehen. Nicht einmal Friedrich der Große hätte es gewagt, die seinen Hof zierenden Wissenschaftler derart als sein Eigentum zu begreifen. Die Autoren sollen ultimativ dazu gezwungen werden, ihr Recht auf freie Publikation preiszugeben. Ihre Budgets sind funktional gekoppelt an die Veröffentlichungen, die, unter Open Access-Bedingungen, auf dem Uniserver kostenlos zur Verfügung gestellt werden (»Wissenschaftliche Publikationen werden in den Akademischen Berichten nur berücksichtigt, wenn sie in ZORA erfasst wurden«). Damit kommen die Autoren in eine verdammungsvolle Situation. Entweder schwenken sie auf Universitätslinie ein, dann wird kein Verlag mehr ein Interesse daran haben, sie zu drucken. Oder sie stellen ihre Dokumente nicht zur Verfügung – und dann werden sie mittelfristig entweder selbst nicht genug Gelder zugewiesen bekommen oder dem Druck ihrer Kollegen ausgesetzt sein, an deren Institut die Budgetmittel fehlen. Ich nenne diese Politik verwerflich und unsittlich. Das Leitbild des Wissenschaftlers, das hinter einer solchen Strategie steckt, ist das eines abhängigen Metöken. Wie kann man aber denken, daß man von Wissenschaftlern, die derartig gedemütigt werden, mittel- und langfristig ›innovative‹ Forschung serviert bekommt? Und warum läßt sich nicht vernünftig darüber nachdenken, wie man wissenschaftliche Produktivkräfte vor dergleichen Anschlägen einer selbst sehr unproduktiven Bürokratenkaste schützt? Auch dem Einfältigsten sollte doch klar sein, daß all das nur herunterzieht, nicht nach oben. Wenn man die Tendenz bekämpfen will, den Markt für wissenschaftliche Publikationen autistisch werden zu lassen und die Autoren durch Kollektiventeignung ihres Urheberrechts zu berauben, muß man für drei Forderungen eintreten. Erstens: Die mit Steuergelder ausgestatteten (aber vom Steuerzahler nicht kontrollierbaren) trusts müssen aufhören, einseitig und massiv digitale Publikationsformen zu Lasten des Buches zu subventionieren, bloß weil die Digitalia im Augenblick hip erscheinen (und eine Menge Leute auf diesem Ticket Karriere machen). Das Verhältnis von Buch- und Digitalienanschaffung in Universitätsbibliotheken ist wieder auf ein vernünftiges und ausgewogenes Maß zu kalibrieren, das Nachhaltigkeit als zentrale Größe anerkennt, nicht modischen Schnickschnack. Zweitens: Bevor man in die komplexe Okösphäre wissenschaftlichen Publizierens durch Subventionierung bestimmter Techniken eingreift, ist – das sollte eigentlich selbstverständlich sein – eine genaue und vor allem transparente Kosten-, Nutzen- und Risikoanalyse vorzulegen. Was für die Einführung einer gentechnischen Pflanze in freiem Feld Standard ist, muß erst recht für so etwas wie Open Access und seine Nebenwirkungen gelten. Dabei muß scharf zwischen Tatsachen und Propagandaphrasen (Globalisierungsdruckgeschwätz, Jehovaschreie) unterschieden werden. Memo: Nicht nur Bankenaufsichtsräte können sich über Spekulationsobjekte (real estates, oje, ist es englisch, ist es schon gelogen) falschen Vorstellungen hingeben. Drittens: Dem unveräußerlichen Urheberrecht der Autoren ist wieder mit Respekt zu begegnen und allen Versuchen, es auf kaltem Verfahrensweg auszuhebeln, ist entschieden – und zur Not auch juristisch – Widerstand zu leisten. Wer hier anfängt, blind und ohne Reflexion auf die Folgen rumzufuhrwerken, legt die Axt an die Wurzel dessen, was das alte Europa einmal ›selbständiges Individuum‹ genannt hat. Niemand kann das wollen.",
            "sentiment_score": 0,
            "magnitude_score": 0
        },
        {
            "created": "2009-02-19",
            "from": "reuss",
            "content": "Die Vorsitzende des Unterausschusses »Elektronische Publikationen« der Deutschen Forschungsgemeinschaft hat sich also zu Wort gemeldet mit einer Replik zu »con crema« (Originalversion / F A Z). Wir erinnern uns, es ist dieser Unterausschuß, der das Zögern der Wissenschaftler beim digitalen Umstieg für eine »Generationenfrage« hält, die Zögernden »konservative Fachvertreter« nennt und mit bedauerndem Unterton feststellt, daß die Hochschulleitungen, die doch »am ehesten einen gewissen (institutionellen) Druck ausüben könnten«, bislang »eher zurückhaltend« agieren. Machen wir also einmal »sichtbar«, was die Unterausschußvorsitzende, Gudrun Gersmann, zu meinem Artikel zu sagen hat. Zunächst dies, daß sie meint, es nicht nötig zu haben, auf »eine Polemik mit einer Gegenpolemik zu antworten«. Offenbar spürt sie den Wind des Fortschritts so sehr in den geblähten Segeln, daß sie sich in die staubigen Gefilde des Streites – und sind wir hier nicht tatsächlich auf solchem Terrain? – nicht zu bequemen braucht. Warum eigentlich? Was spricht gegen eine Polemik, wenn die Sache es gebietet? Wenn eine »kurze und nüchterne Bestandsaufnahme« reichte, um den Sachverhalt zu erhellen, hätte ich meinen Artikel kaum geschrieben – und sie wäre wohl kaum genötigt gewesen, eine so eigenartige Replik zu formulieren. Man hätte erwarten können, daß sie zumindest zwei Kernpunkte meines Räsonnements der Mühe einer Widerlegung Wert gefunden hätte: Ich kritisiere (a) den Versuch einer Abschaffung des Urheberrechts und (b) den Ansatz zur Zerschlagung einer freien deutschen Verlagsszene. Aber hierauf geht Gersmann erst garnicht ein. Statt dessen erhalten wir den Abriß eines routinierten Vortrags über die Vorzüge von Open Access, wie sie ihn auf die eine oder andere Art abgewandelt (mit oder ohne PowerPoint-Präsentation) wohl schon öfters gehalten hat. Ich halte fest: Gersmann sagt nichts über die Strategie der Erpressung (ich nenne es: Erpressung) mit der die Universität Zürich ihre Wissenschaftler zu Metöken einer Wissensverwaltung machen will, deren Relation potenziell einen rechtsfreien und hierarchisch gegliederten Raum schafft, der nur wenig von jenem differiert, den Lebensmitteldiscounter zu ihren Zuliefern etabliert haben. Und sie sagt auch nichts über die bundesrepublikanischen Tendenzen, diesem illegalen Tun nachahmend zu folgen. Es wäre schön, wenn die Ausschußvorsitzende der interessierten Öffentlichkeit einmal erklärte, warum – wenn, wie behauptet, das Open Access-Modell tatsächlich so erfolgreich ist und von den Wissenschaftlern so emphatisch begrüßt wird – warum es dann einer Rechtsbeugung bedarf, um es durchzusetzen. Enthusiasmus nach Vorschrift sieht einigermaßen komisch aus. Selbst dem uninformiertesten Laien muß doch auffallen, daß da etwas in der Argumentation nicht stimmt. Warum kann man es nicht dabei belassen – unter Verzicht auf jede Art institutioneller Repression –, daß Autoren sich aussuchen können, wo sie publizieren wollen? Das entspricht der Rechtslage, und wer eine andere will, mag seine Lobbyisten in den Bundestag schicken und versuchen, die Rechtslage zu ändern. Das wäre dann ansatzweise ein demokratischer Prozeß und nicht, was wir jetzt beobachten, der Versuch eines Verwaltungsputschs. Und vor allem sollten endlich einmal Zahlen auf den Tisch. Es wird immer wieder behauptet, die Wissenschaftler seien so happy, unter Open Access-Bedingungen zu publizieren, um, ja, o: ›sichtbar‹ zu werden. Das ist eine reine Behauptung ohne jede Unterfütterung durch Empirie, der ich locker meine Erfahrung im Umgang mit Kollegen entgegensetze. Niemand, der das Open Access-Modell in seinen Folgen für die wissenschaftliche Freiheit durchdacht hat, kann mit den vorgezeichneten Konsequenzen für seine Publikationsfreiheit auch nur ansatzweise sympathisieren – von den Personen abgesehen (quelle surprise), die im Rahmen von Open Access-Projekten Gelder von der DFG bekamen. Auch zu dem zweiten Punkt, der in Kauf genommenen Zerschlagung der deutschen Verlagsszene, sagt die Ausschußvorsitzende nichts Stichhaltiges. Statt dessen ein Blick nach England. Wir haben aber (gottseidank!) in diesem Sektor nicht eine so verzweiflungsvolle Verlagslandschaft wie in England, und die Statistiken liefert (gottseidank!) auch noch nicht ein Erster Beweger der »Bewegung«: »An dieser Stelle wird auch gerne die Legende bemüht, dass Open Access zwangsläufig den Bankrott der Verlage nach sich zieht. Dem sei allerdings entgegengehalten, dass eine Erhöhung der Zahl frei zugänglicher Publikationen zu einem signifikanten ›return on investment‹ führte, wie eine von John Houghton und anderen jüngst publizierte, umfassende Analyse zum Publikationsaufkommen Englands eindrucksvoll belegt.« Ich kenne nun nicht wenige deutsche Verleger, wie kommt es nur, daß mir keiner das bestätigen kann? Und wie kommt es nur, daß man mit Staatsgeldern einen solchen desaströsen Freilandversuch wie – um den inneren Widerspruch so scharf wie möglich zu benennen – technokratisch erzwungenes Open Access durchführt und propagiert, ohne sich ein genaues Bild von den Nebenwirkungen verschafft zu haben? Wenn die Überschrift der FAZ-Replik suggeriert, da habe jemand Angst vor Open Access, so kann ich versichern, Angst habe ich davor nicht (die betroffenen Verlage freilich schon, und ihnen gegenüber ist die Floskel zynisch). Ich habe Gründe. Ich will unter anderem, daß es nach wie vor möglich ist, eine zwanzigbändige Edition (analog und digital) eines Autors vorzulegen, mit den bestmöglichen Materialien, dem bestmöglichen Lektorat, dem bestmöglichen Vertrieb und dem bestmöglichen Kommunikationszusammenhang innerhalb und außerhalb der Universität. Ich will nicht, daß der allein gangbare Weg, dies zu erreichen: die Verlagssubskription, an einer Klausel scheitert, die die Verlage (oder die Autoren) dazu zwingt, Band 1 zum Abschuß für Open Access freizugeben, wenn Band 3 erscheint. Wer sollte dann eigentlich noch subskribieren? Und welcher Verlag wäre dann noch so dämlich, Vorinvestitionen zu tätigen? Und welche öffentliche Institution hätte auch nur die leiseste Phantasie (vom Wissen ganz zu schweigen), was an deren Stelle treten könnte? Ein (wie man so sagt) ›Geschäftsmodell‹, das unter den Bedingungen von Open Access sich bewähren könnte, gibt es nicht – und für die (Wissenschafts-)Verlage, die immer noch nicht begriffen haben, daß mit der Akzeptanz der Übergriffe von Wissenschaftsverwaltungen in die Publikationsfreiheit ihr Totenglöcklein geläutet hat, schreibe ich dies auch gerne hier noch einmal hin: Abtauchen geht nicht mehr. Wir sind an einem historischen Punkt angelangt, wo man öffentlich kämpfen muß oder man geht unter – für diesmal ziehen, eine historische Ausnahme, Autoren und Verleger an einem Strang. Coda 1 Was ich betreibe, ist keine Kulturkritik. Es ist Kritik, das stimmt. Aber sie bezieht sich nicht auf Kultur, sondern auf die Barbarei eines Denkens, das nur noch in Statistiken rechnet und in wohlfeilen Wortmünzen sich artikuliert. Wie schwach muß eigentlich eine Position sein, die sich mit dem andauernden Aufruf von Phrasen wie »barrierefrei«, »moving wall«, »Open-peer-review-Verfahren«, »return on invest­ment«, »weltweite ›Sichtbarkeit‹« und dergl. mehr wie mit fremdem Blut stärkt? Muß man mit diesen Transfusionen wirklich seinen Sprachkörper dopen, um Karriere zu machen? Wer in diesem technokratischen slang über geistige Gegenstände redet (und entscheiden will), hat von vorneherein ein Legitimationsproblem. Daß es in der Wissenschaftsbürokratie zum guten Ton gehört, in diesen Jargon zu fallen, hilft nicht, ja es verschärft das Problem nur: Die Leere der Parolen kann noch besser wahrgenommen werden. Coda 2 Es ist nichts törichter, als das Argument, Open Access allein verschaffe freien Zugang zum Wissen für alle. Das kann man nicht nur keinem Afrikaner erzählen (die eine Seite der Medaille); man bekommt außerdem den völlig unangemessenen Eindruck vermittelt, alle öffentlichen Bibliotheken seien geschlossene Anstalten, in die niemand hereingelassen werde (die andere Seite). Meiner Erfahrung entspricht dieses apokalyptische Bild nicht. Man glaubt es kaum: Ich kann problemlos Bücher und Zeitschriften ausleihen, im Lesesaal mich in eine Schrift vertiefen, usw. Zu monieren ist allenfalls, daß ich für Reproduktionen, die ich von Zeit zu Zeit für wissenschaftliche Studien (!) brauche, horrende Summen zahlen darf (man studiere nur einmal die Gebührenordnung des Landes Baden-Württemberg), aber diese Inkonsistenz in der Politik der Bibliotheken paßt genau ins Bild (not so open access). Inkonsistent ist auch, daß die Bibliothekare nicht begriffen haben, daß sie im Augenblick eifrig daran sind, den Ast abzusägen, auf dem sie sitzen. Wird der Tendenz nicht gesteuert (ihr Einsatz für eine Erhöhung des Buchetats wäre ein erster Schritt), kann man sie, nachdem sie die Originale erst eifrig digitalisiert und anschließend ›entsorgt‹ haben (buchstäblich auf den Müll), für die ›Altlasten‹ durch Archivare ersetzen. Um den laufenden Betrieb kümmern sich dann die server und eine Handvoll Informatiker. Auch das ein ›Geschäftsmodell‹. Coda 3 Wenn es so ist (was ich einmal gutwillig annehme), daß einer der Gründe, für Open Access einzutreten und es zu fördern, die hohen Abonnementspreise (für Zeitschriften) einiger weniger multinationaler Konzerne sind, dann muß man diese eben gezielt ins Visier nehmen. »Im Wald, da sind die Räuber« – aber wer kommt deshalb gleich auf den Gedanken, den ganzen Wald brandzuroden? Wenn man gleichwohl genau dazu Anstalten macht, drängt sich der Verdacht auf, daß es nicht um Bekämpfung von beklagenswerten Auswüchsen, sondern um Kontrolle und Gleichmacherei geht. Coda 4 Man hört von Imitatoren des Vorsitzenden des Bundes der Steuerzahler von Zeit zu Zeit die Parole, die Wissenschaftler seien ja vom Staat bezahlt und hätten deshalb umsonst zu liefern – nix da mit Urheberrecht. Lessing hat sich bereits im 18. Jahrhundert ausführlicher mit dieser populistischen Sichtweise auseinandergesetzt (»Leben und leben lassen«). Es ist müßig, das zu repetieren. Ich kann hier die Daten nur noch auf den neuesten Stand bringen: Niemand von den ernsthaft an meiner Universität Lehrenden und Forschenden hat einen Achtstundentag, zwölf bis vierzehn Stunden sind nicht selten. Und wer die Situation an den geisteswissenschaftlichen Instituten der Universität kennt, kann wissen, daß bereits die Lehre das normale Pensum an Arbeitszeit auffrißt. Forschen kann man buchstäblich nur in der ›Freizeit‹. Was aber ein Forscher in seiner freien Zeit schreibt, darauf hat kein Staat der Welt einen Anspruch. Und das ist noch ein sehr defensiver Standpunkt. In Deutschland ist die Gesetzgebung so weise, daß sie die kreativ arbeitenden Urheber unter ihren genauen Schutz stellt. Begriffen (wenngleich von manchen offenbar verdrängt) ist, daß Interventionen in die Produktion und Publikationsform geistiger Gegenstände dem Staatsinteresse prinzipiell zuwiderlaufen. Und daß hohe Eigenmotivation der zentrale Angelpunkt wissenschaftlichen Forschens und ergo zu fördern ist. Es ist an der Zeit, diesen Begriff wieder zur Geltung zu bringen und der Öffentlichkeit zu vermitteln.",
            "sentiment_score": 0,
            "magnitude_score": 0
        },
        {
            "created": "2020-09-05",
            "from": "faz",
            "content": "Wissenschaftliche Arbeit wird zum großen Teil an Universitäten und Forschungsinstituten vom Staat und damit mit öffentlichen Geldern bezahlt. Aber um sie zu lesen und frei nutzen zu können, muss man teure wissenschaftliche Zeitschriften bezahlen. Zwar werden die meisten dieser Zeitschriften wiederum von Universitäten und Forschungsinstituten abonniert, aber auch diese müssen eben dafür bezahlen. Seit langem versuchen die Forschungseinrichtungen mit den großen Verlagen, die die wichtigen Zeitschriften herausgeben, eine Einigung zu erzielen. Nach und nach werden seit einigen Jahren Vereinbarungen zwischen Verlagen und Universitäten oder Verbünden wie dem Projekt „Deal“ unter Federführung der Hochschulrektorenkonferenz abgeschlossen: Die Forschungseinrichtungen sollen nicht mehr für den Kauf oder das Abo der Zeitschriften bezahlen, sondern für die Veröffentlichung des Forschungsartikels. Der Zugang zu den Inhalten der Zeitschriften soll dann aber allgemein kostenlos und frei sein. Insbesondere dem Open Access, dem freien Online-Zugriff auf die Forschungsarbeiten, steht dann nichts mehr im Wege. Anfang des Jahres wurde ein solcher Vertrag zwischen Springer Nature und Deal abgeschlossen, auch die University of California hat sich mit Springer Nature geeinigt. Ist das tatsächlich die bessere Lösung? Für all diejenigen, die nicht an Forschungseinrichtungen arbeiten und selbst nicht forschen, scheint das zunächst so zu sein: Journalisten und Autoren von populärwissenschaftlichen Büchern, Unternehmen, die ganze Öffentlichkeit hätte unmittelbar vollen Zugriff auf die neuesten Ergebnisse der Wissenschaft – wenn sie sie denn verstehen. Anders sieht es mit der Möglichkeit aus, selbst zu publizieren. Jede Veröffentlichung in einem Forschungsjournal kostet dann ein paar tausend Euro, die an den Verlag zu zahlen sind, und umso größer die Reputation des Journals ist, desto teurer dürfte die Publikation eines Papers sein. Kleine Institute und finanzschwache Universitäten werden ihre Schwierigkeiten haben, die finanziellen Mittel für die Spitzentitel aufzubringen, ganz zu schweigen von Einzelforschern, die ihre Arbeit selbst finanzieren. Mit doppelter Blindheit geschlagen Sicherlich ist die Meinung verbreitet, dass Spitzenforschung ohnehin nur noch an finanzstarken Instituten in großen Teams möglich ist und dass dort die paar tausend Euro für eine Veröffentlichung nicht ins Gewicht fallen. Aber das ist nicht ganz richtig. Für die University of California geht es bei einer jährlichen Zahl von 1400 Artikeln und einer durchschnittlichen Bearbeitungsgebühr von 3200 Dollar immerhin um rund viereinhalb Millionen Dollar im Jahr. Für kleinere Forschungseinrichtungen kann die Publikation in einem führenden Journal bei diesem Prinzip schnell an den Kosten scheitern. Und nicht nur in der Philosophie und in den Geisteswissenschaften gibt es noch einzelne private Wissenschaftler, die auf eigene Kosten an Spezialthemen oder am großen Wurf forschen, den Spezialisten, der nach dem Studium und neben seinem Broterwerb an einer revolutionären Idee arbeitet. Insbesondere in den theoretischen Bereichen der Naturwissenschaften gibt es auch freie Forscher, die mehr oder weniger eingebunden in das Netzwerk der Universitäten und Institute eigene Forschungsvorhaben verfolgen. Diese Wissenschaftler haben nach dem bisherigen Verfahren wenigstens grundsätzlich die Möglichkeit, in führenden Journalen ihrer Disziplinen zu veröffentlichen. Die anonyme Einreichung und das doppelt-blinde Peer-Review-Verfahren sorgen zumindest im Prinzip dafür, dass jede eingereichte Arbeit geprüft wird und dass nur nach ihrer Qualität und Originalität über eine Veröffentlichung entschieden wird. Wenn zu diesen fachlichen Hürden nun allerdings noch eine finanzielle Hürde von einigen tausend Euro kommt, dürfte es kaum noch möglich sein, dass eine freie Forscherin oder auch nur ein kleines Institut ohne großen Etat in einem solchen Journal die Möglichkeit zur Veröffentlichung hat. Spätestens jetzt stellt sich die Frage, wozu das überhaupt nötig ist. Warum nicht einfach auf der privaten Website oder in einer Open-Access-Datenbank die bahnbrechenden Theorien oder die Ergebnisse langer beharrlicher Analysen veröffentlichen? Die Antwort auf diese Frage klärt auch, warum die führenden Verlage überhaupt so viel Geld für ihre Arbeit verlangen können, sei es nun fürs Abonnement oder für die Publikation der Paper. Die Antwort ist, dass sie eine Reihe von Dienstleistungen erbringen, die staatliche Forschungseinrichtungen zwar brauchen, aber selbst nicht erbringen wollen. Das beginnt schon mit der puren Organisation des Peer-Review-Verfahrens: Wissenschaftler reichen ihre Arbeiten bei Journalen ein. Diese müssen nun zwei bis drei passende Gutachter bestimmen; dazu muss der Herausgeber, meistens auch ein ausgewiesener Wissenschaftler, aber erst einmal wissen, worum es in der Arbeit geht und wer sie beurteilen kann. Dann wird die Arbeit anonymisiert verschickt. Die Reviewer müssen gemahnt werden, dass sie ihre Bewertungen pünktlich, genauer gesagt: mit nicht zu großem Verzug abgeben. Manchmal müssen weitere Reviewer beauftragt werden. Die Prüfer haben oft Kritik am Werk der Kollegen, diese muss aufbereitet, zurückgegeben werden, neue Termine werden gemacht; die Sache geht von vorn los. Eine erhebliche Korrespondenz und Verwaltungsarbeit sind nötig. Ist die Arbeit dann tatsächlich angenommen, muss sie redaktionell publikationsreif gemacht, in den Publikationsprozess eingebracht und schließlich veröffentlicht werden. Qualitätssicherung kostet eben „Nature“, das weltweit führende Journal der naturwissenschaftlichen Forschung, beschäftigt beispielsweise ausschließlich professionelle Herausgeber, dazu ein paar Dutzend Redakteure, Leute für die Produktion, für die digitale Ausgabe und anderes. Das alles ist Teil des wissenschaftlichen Qualitätssicherungsprozesses, und auch wenn an diesem Prozess vieles schlecht läuft, ist noch niemandem etwas Besseres eingefallen, um dafür zu sorgen, dass weltweit doch ziemlich viele Forschungsarbeiten hoher Qualität veröffentlicht werden. Es ist ja nicht so, wie die Wissenschaft vielleicht auch manchmal gern glauben machen möchte, dass die Forschung schon von sich aus einfach fehlerfreie, neue und vor allem wichtige Erkenntnisse am Fließband produziert. Die möglichst unvoreingenommene strukturierte Kollegenkritik ist dafür enorm wichtig, und der Aufwand dafür liegt eben nicht nur bei den Kollegen aus anderen Instituten, die kritisieren, sondern bei den Journalen, die das organisieren. Ohne diese Arbeit würde zwar, vor allem online, unendlich viel veröffentlicht, aber kein Wissenschaftler hätte Lust, die Arbeit der Kollegen zu lesen, weil er fürchten würde, sich durch Ungereimtheiten, Fehler und Doppelungen oder Nebensächlichkeiten quälen zu müssen. Damit wird die wichtigste Funktion der Zeitschriftenverlage sichtbar: Sie sorgen dafür, dass sich die Forscher bei der Sichtung der Arbeit ihrer Kollegen auf das konzentrieren können, was weltweit oder auch lokal als das Wichtigste angesehen wird. Auch hier kann man sich nicht ganz sicher sein, ob das wirklich funktioniert; aber weitgehend scheint es so zu sein, dass das, was es in die anerkannten, führenden Journale schafft, eben auch gut, neu und wichtig ist. Deshalb brauchen die Wissenschaften diese Journale – und da es ihnen über die Jahrzehnte der modernen Forschung nicht gelungen ist, diese Journale als staatliche, öffentliche Einrichtungen einzurichten, haben private Verlage überhaupt die Macht, sich diese Ordnungs- und Sortierfunktion hoch bezahlen zu lassen. Gäbe es Alternativen? Natürlich sind Journale als öffentlich-rechtliche Körperschaften denkbar, die aus steuerlichen Etats für Forschung finanziert werden, unabhängig von den einzelnen Universitäten und Instituten. Da sind vor vielen Jahrzehnten, als die großen Wissenschaftsjournale entstanden, allerdings die Weichen falsch gestellt worden. Bis solche Journale die Reputation führender Wissenschaftsverlage aufgebaut haben, die international anerkannt wären, vergehen Jahrzehnte. Aber da nützt alles Jammern nichts: Wer will, dass staatlich finanzierte Forschung kostenlos allen frei zugänglich ist, die sich dafür interessieren, muss ein völlig neues System aufbauen, das leistet, was die Verlage heute tun: Qualität sichern und nachvollziehbar Reputation ermöglichen. Das geht vermutlich kostengünstiger, als es Privatverlage wie Springer, Elsevier und Wiley machen; aber mit einer offenen Plattform für freies Publizieren ist es eben nicht getan. Da wäre ein abgestimmtes Vorgehen nötig, und das ist mehr, als einen Deal mit großen Verlagen auszuhandeln. Der Autor ist Philosoph und Unternehmer. issenschaftliche Arbeit wird zum großen Teil an Universitäten und Forschungsinstituten vom Staat und damit mit öffentlichen Geldern bezahlt. Aber um sie zu lesen und frei nutzen zu können, muss man teure wissenschaftliche Zeitschriften bezahlen. Zwar werden die meisten dieser Zeitschriften wiederum von Universitäten und Forschungsinstituten abonniert, aber auch diese müssen eben dafür bezahlen. Seit langem versuchen die Forschungseinrichtungen mit den großen Verlagen, die die wichtigen Zeitschriften herausgeben, eine Einigung zu erzielen. Nach und nach werden seit einigen Jahren Vereinbarungen zwischen Verlagen und Universitäten oder Verbünden wie dem Projekt „Deal“ unter Federführung der Hochschulrektorenkonferenz abgeschlossen: Die Forschungseinrichtungen sollen nicht mehr für den Kauf oder das Abo der Zeitschriften bezahlen, sondern für die Veröffentlichung des Forschungsartikels. Der Zugang zu den Inhalten der Zeitschriften soll dann aber allgemein kostenlos und frei sein. Insbesondere dem Open Access, dem freien Online-Zugriff auf die Forschungsarbeiten, steht dann nichts mehr im Wege. Anfang des Jahres wurde ein solcher Vertrag zwischen Springer Nature und Deal abgeschlossen, auch die University of California hat sich mit Springer Nature geeinigt. Ist das tatsächlich die bessere Lösung? Für all diejenigen, die nicht an Forschungseinrichtungen arbeiten und selbst nicht forschen, scheint das zunächst so zu sein: Journalisten und Autoren von populärwissenschaftlichen Büchern, Unternehmen, die ganze Öffentlichkeit hätte unmittelbar vollen Zugriff auf die neuesten Ergebnisse der Wissenschaft – wenn sie sie denn verstehen. Anders sieht es mit der Möglichkeit aus, selbst zu publizieren. Jede Veröffentlichung in einem Forschungsjournal kostet dann ein paar tausend Euro, die an den Verlag zu zahlen sind, und umso größer die Reputation des Journals ist, desto teurer dürfte die Publikation eines Papers sein. Kleine Institute und finanzschwache Universitäten werden ihre Schwierigkeiten haben, die finanziellen Mittel für die Spitzentitel aufzubringen, ganz zu schweigen von Einzelforschern, die ihre Arbeit selbst finanzieren. Mit doppelter Blindheit geschlagen Sicherlich ist die Meinung verbreitet, dass Spitzenforschung ohnehin nur noch an finanzstarken Instituten in großen Teams möglich ist und dass dort die paar tausend Euro für eine Veröffentlichung nicht ins Gewicht fallen. Aber das ist nicht ganz richtig. Für die University of California geht es bei einer jährlichen Zahl von 1400 Artikeln und einer durchschnittlichen Bearbeitungsgebühr von 3200 Dollar immerhin um rund viereinhalb Millionen Dollar im Jahr. Für kleinere Forschungseinrichtungen kann die Publikation in einem führenden Journal bei diesem Prinzip schnell an den Kosten scheitern. Und nicht nur in der Philosophie und in den Geisteswissenschaften gibt es noch einzelne private Wissenschaftler, die auf eigene Kosten an Spezialthemen oder am großen Wurf forschen, den Spezialisten, der nach dem Studium und neben seinem Broterwerb an einer revolutionären Idee arbeitet. Insbesondere in den theoretischen Bereichen der Naturwissenschaften gibt es auch freie Forscher, die mehr oder weniger eingebunden in das Netzwerk der Universitäten und Institute eigene Forschungsvorhaben verfolgen. Diese Wissenschaftler haben nach dem bisherigen Verfahren wenigstens grundsätzlich die Möglichkeit, in führenden Journalen ihrer Disziplinen zu veröffentlichen. Die anonyme Einreichung und das doppelt-blinde Peer-Review-Verfahren sorgen zumindest im Prinzip dafür, dass jede eingereichte Arbeit geprüft wird und dass nur nach ihrer Qualität und Originalität über eine Veröffentlichung entschieden wird. Wenn zu diesen fachlichen Hürden nun allerdings noch eine finanzielle Hürde von einigen tausend Euro kommt, dürfte es kaum noch möglich sein, dass eine freie Forscherin oder auch nur ein kleines Institut ohne großen Etat in einem solchen Journal die Möglichkeit zur Veröffentlichung hat. Spätestens jetzt stellt sich die Frage, wozu das überhaupt nötig ist. Warum nicht einfach auf der privaten Website oder in einer Open-Access-Datenbank die bahnbrechenden Theorien oder die Ergebnisse langer beharrlicher Analysen veröffentlichen? Die Antwort auf diese Frage klärt auch, warum die führenden Verlage überhaupt so viel Geld für ihre Arbeit verlangen können, sei es nun fürs Abonnement oder für die Publikation der Paper. Die Antwort ist, dass sie eine Reihe von Dienstleistungen erbringen, die staatliche Forschungseinrichtungen zwar brauchen, aber selbst nicht erbringen wollen. Das beginnt schon mit der puren Organisation des Peer-Review-Verfahrens: Wissenschaftler reichen ihre Arbeiten bei Journalen ein. Diese müssen nun zwei bis drei passende Gutachter bestimmen; dazu muss der Herausgeber, meistens auch ein ausgewiesener Wissenschaftler, aber erst einmal wissen, worum es in der Arbeit geht und wer sie beurteilen kann. Dann wird die Arbeit anonymisiert verschickt. Die Reviewer müssen gemahnt werden, dass sie ihre Bewertungen pünktlich, genauer gesagt: mit nicht zu großem Verzug abgeben. Manchmal müssen weitere Reviewer beauftragt werden. Die Prüfer haben oft Kritik am Werk der Kollegen, diese muss aufbereitet, zurückgegeben werden, neue Termine werden gemacht; die Sache geht von vorn los. Eine erhebliche Korrespondenz und Verwaltungsarbeit sind nötig. Ist die Arbeit dann tatsächlich angenommen, muss sie redaktionell publikationsreif gemacht, in den Publikationsprozess eingebracht und schließlich veröffentlicht werden. Qualitätssicherung kostet eben „Nature“, das weltweit führende Journal der naturwissenschaftlichen Forschung, beschäftigt beispielsweise ausschließlich professionelle Herausgeber, dazu ein paar Dutzend Redakteure, Leute für die Produktion, für die digitale Ausgabe und anderes. Das alles ist Teil des wissenschaftlichen Qualitätssicherungsprozesses, und auch wenn an diesem Prozess vieles schlecht läuft, ist noch niemandem etwas Besseres eingefallen, um dafür zu sorgen, dass weltweit doch ziemlich viele Forschungsarbeiten hoher Qualität veröffentlicht werden. Es ist ja nicht so, wie die Wissenschaft vielleicht auch manchmal gern glauben machen möchte, dass die Forschung schon von sich aus einfach fehlerfreie, neue und vor allem wichtige Erkenntnisse am Fließband produziert. Die möglichst unvoreingenommene strukturierte Kollegenkritik ist dafür enorm wichtig, und der Aufwand dafür liegt eben nicht nur bei den Kollegen aus anderen Instituten, die kritisieren, sondern bei den Journalen, die das organisieren. Ohne diese Arbeit würde zwar, vor allem online, unendlich viel veröffentlicht, aber kein Wissenschaftler hätte Lust, die Arbeit der Kollegen zu lesen, weil er fürchten würde, sich durch Ungereimtheiten, Fehler und Doppelungen oder Nebensächlichkeiten quälen zu müssen. Damit wird die wichtigste Funktion der Zeitschriftenverlage sichtbar: Sie sorgen dafür, dass sich die Forscher bei der Sichtung der Arbeit ihrer Kollegen auf das konzentrieren können, was weltweit oder auch lokal als das Wichtigste angesehen wird. Auch hier kann man sich nicht ganz sicher sein, ob das wirklich funktioniert; aber weitgehend scheint es so zu sein, dass das, was es in die anerkannten, führenden Journale schafft, eben auch gut, neu und wichtig ist. Deshalb brauchen die Wissenschaften diese Journale – und da es ihnen über die Jahrzehnte der modernen Forschung nicht gelungen ist, diese Journale als staatliche, öffentliche Einrichtungen einzurichten, haben private Verlage überhaupt die Macht, sich diese Ordnungs- und Sortierfunktion hoch bezahlen zu lassen. Gäbe es Alternativen? Natürlich sind Journale als öffentlich-rechtliche Körperschaften denkbar, die aus steuerlichen Etats für Forschung finanziert werden, unabhängig von den einzelnen Universitäten und Instituten. Da sind vor vielen Jahrzehnten, als die großen Wissenschaftsjournale entstanden, allerdings die Weichen falsch gestellt worden. Bis solche Journale die Reputation führender Wissenschaftsverlage aufgebaut haben, die international anerkannt wären, vergehen Jahrzehnte. Aber da nützt alles Jammern nichts: Wer will, dass staatlich finanzierte Forschung kostenlos allen frei zugänglich ist, die sich dafür interessieren, muss ein völlig neues System aufbauen, das leistet, was die Verlage heute tun: Qualität sichern und nachvollziehbar Reputation ermöglichen. Das geht vermutlich kostengünstiger, als es Privatverlage wie Springer, Elsevier und Wiley machen; aber mit einer offenen Plattform für freies Publizieren ist es eben nicht getan. Da wäre ein abgestimmtes Vorgehen nötig, und das ist mehr, als einen Deal mit großen Verlagen auszuhandeln.",
            "sentiment_score": 0,
            "magnitude_score": 0
        },
        {
            "created": "2018-09-14",
            "from": "faz",
            "content": "Open Access : Blinde Offensive Die europäische Forschungskommission will Wissenschaftler mit Sanktionsdrohungen zur Publikation in Open ASieht so der europäische Kampf für Wissenschaftsfreiheit aus? Die Europäische Kommission und elf europäische Wissenschaftsorganisationen wollen die von ihnen geförderten Forscher zwingen, von Januar 2020 an nur noch Open Access zu publizieren. Andernfalls drohen ihnen nicht näher benannte Sanktionen. Wie die Umstellung in der hoffnungslos knappen Frist zu bewältigen ist, wird nicht verraten. Auch der schwere Eingriff in die Wissenschaftsfreiheit, den die Maßnahme bedeutet, scheint die europäische Allianz nicht zu stören, er könnte aber bald die Verfassungsgerichte beschäftigen. Der europäische Vorstoß wird wissenschaftliche Großmächte wie China und die Vereinigten Staaten freuen, die alle wissenschaftlichen Publikationen aus der EU gratis bekommen sollen und nicht im Traum daran denken, sich für das freundliche Geschenk zu revanchieren. Wie die europäischen Bürger, die dann freien Zugang zu wissenschaftlichen Publikationen haben sollen, die sie mehrheitlich nicht verstehen, davon profitieren, wird man ihnen hoffentlich noch erklären. Vielleicht sollte man die Zahl der Citizen Scientists einmal messen, um zu sehen, wie wirtschaftlich diese Rechnung ist. Der europäische Vorstoß wirkt anachronistisch zu einer Zeit, in der die Schattenseiten von Open Access immer deutlicher hervortreten. Der Eklat um Raubverlage hat gezeigt, welche Gefahr von Journalen ausgeht, die an der wissenschaftlichen Qualitätssicherung vorbeiproduziert werden. Die mit Open Access einhergehende Umstellung vom Abonnementmodell auf die Autorengebühr hat die Raubverlage erst möglich gemacht. Gut geölte Lobbymaschine Deutsche Forschungsorganisationen sind in der europäischen Allianz nicht vertreteten. Die Alexander-von-Humboldt-Stiftung distanziert sich von Sanktionsdrohungen, die Deutsche Forschungsgemeinschaft verweist auf die steigenden Publikationsgebühren von Open-Access-Plattformen. Im Unterschied zur Europäischen Forschungskommission scheint man hier zu ahnen, dass Open Access am Ende nicht billiger, sondern teurer kommt. Und dass die Rendite ausgerechnet die internationalen Großverlage Elsevier, Springer und Wiley einfahren werden, die Wissenschaftler weltweit mit astronomischen Preisen erpressen. Freundliche Unterstützung leisten dabei die mehr als zweihundert deutschen Wissenschaftsorganisationen, die mit den Großverlagen unter Ausschluss der mittelständischen Konkurrenz eine Nationallizenz aushandeln. Sobald der Mittelstand ausgeschaltet ist, werden die Großverlage, deren Monopol eigentlich gebrochen werden sollte, noch besser an der Preissschraube drehen können. Open Access ist ein Kind der Internetgratiskultur und eine gut geölte Lobbymaschine, die so tut, als habe es die Kommerzialisierung des Internets nie gegeben. Das jüngste Produkt ist ein als Dokumentarfilm ausgewiesener Propagandafilm der von George Soros finanzierten Open Society Foundation. Der Film mit dem bezeichnenden Titel Paywall ist eine Feierstunde für Open Access, über steigende Produktionskosten und Monopoltendenzen wird kein Wort verloren. Die Zeitschriften wachsen auf den Bäumen, wie wunderbar! Die Rechnung für diese Blindheit werden die Großverlage präsentieren. Sie wird hoch sein.",
            "sentiment_score": 0,
            "magnitude_score": 0
        },
        {
            "created": "2016-08-28",
            "from": "faz",
            "content": "Open Access : Neues aus der Forschung zum Nulltarif Wissenschaftler veröffentlichen ihre Ergebnisse für jeden zugänglich im Internet - Open Access nennt sich das Modell. So verlockend es klingt, ein paar Schwierigkeiten gibt es doch. An Open Access führe kein Weg vorbei, sagt Matthias Kettemann, der an der Frankfurter Universität zum Internetrecht forscht. Open Access heißt „freier Zugang“ und steht für das Konzept, der Öffentlichkeit alle wissenschaftlichen Publikationen unentgeltlich zur Verfügung zu stellen. Traditionell haben Akademiker ihre Forschungsergebnisse in Fachzeitschriften veröffentlicht. Doch schon seit den neunziger Jahren setzt sich die Open-Access-Bewegung dafür ein, dass die Texte für jedermann frei zugänglich online veröffentlicht werden. Dabei gehe es in erster Linie darum, dass weltweit jeder über das Internet auf das Wissen und kulturelle Erbe der Menschheit zugreifen könne, sagt Georg Botz, Koordinator für Open Access Policy der Max-Planck-Gesellschaft. Dies bedeute nicht nur, dass auch Wissenschaftler in Entwicklungsländern von den Ergebnissen anderer Forscher profitieren und mit ihrer eigenen Arbeit darauf aufbauen könnten. Der gleiche Effekt zeige sich natürlich auch schon innerhalb Europas, weshalb der Rat der Europäischen Union im Mai offiziell dazu aufgerufen hat, Open Access zu fördern. Momentan sind laut Max-Planck-Gesellschaft 13 Prozent der wissenschaftlichen Publikationen frei zugänglich, und es gibt im Netz Tausende reine Open-Access-Zeitschriften von den unterschiedlichsten Herausgebern. Darunter sind etablierte Verlage, private Anbieter sowie Fachgesellschaften wie zum Beispiel die Deutsche Physikalische Gesellschaft. Im Unterschied zu den traditionellen Zeitschriften, für die der Leser Geld ausgibt, muss bei Open-Access-Zeitschriften der Autor für die Veröffentlichung seines Artikels zahlen. So kann der Leser kostenlos auf den Artikel zugreifen. Dies gilt auch für das Hybrid-Modell, bei dem der Forscher dafür zahlt, dass sein Text dank Open Access frei zugänglich ist, obwohl er ihn in einer herkömmlichen Fachzeitschrift veröffentlicht. Da dies aber dem bisherigen Geschäftsmodell der Fachverlage widerspricht, verlangen sie für die Open-Access-Veröffentlichung in ihren Zeitschriften zum Teil bis zu mehrere tausend Euro pro Artikel. Immer geht es auch ums Geld Laut Botz steht die Politik grundsätzlich hinter Open Access, was erst einmal logisch klingt angesichts der Möglichkeit, dank dieses Modells immer auf dem aktuellsten Stand der Forschung zu sein. Doch wie immer geht es auch ums Geld, selbst wenn beteuert wird, dass dies nicht der primäre Grund sei. Denn Universitäten und ihre Bibliotheken zahlen weltweit jedes Jahr Milliarden für Abonnements der herkömmlichen Fachzeitschriften. Diese sind nach wie vor die Hauptquelle für Wissenschaftler, die sich über die neuesten Forschungsergebnisse ihrer Kollegen informieren wollen. Fachverlage wie Springer, die diese Zeitschriften herausgeben, verdienen daran nicht schlecht. So schreiben die MIT-Bibliotheken, dass die Gewinnmarge von Elsevier, einem der weltweit größten Wissenschaftsverlage, in den vergangenen Jahren um die 36 Prozent betragen habe. Und Wissenschaftler der Max-Planck-Gesellschaft haben ausgerechnet, dass weltweit jährlich 7,6 Milliarden Euro für Fachzeitschriften gezahlt werden. Bei zwei Millionen Artikeln im Jahr bedeute dies einen Artikelpreis von 3800 Euro. Die Veröffentlichung eines Open-Access-Artikels hingegen koste im Schnitt weit weniger als 2000 Euro. Dieser Preiszuschlag etablierter Fachzeitschriften sei nicht gerechtfertigt und momentan nur noch wegen ihres Renommees möglich, meint Botz. Denn in Deutschland sind die meisten Universitäten staatlich, das heißt mit Steuergeldern finanziert. Also sind die Wissenschaftler, die Autoren der Texte sind, vom Staat bezahlt, genauso wie auch die Wissenschaftler, die im sogenannten Peer Review die Qualität der Artikel garantieren sollen. Die Verlage, die die Zeitschriften zusammenstellen, redigieren und herausgeben, schaffen laut Botz eine künstliche Hürde zwischen Autor und Leser. In Zeiten des Internets, in denen sich online jeder selbst verlegen kann, sei ihr Geschäftsmodell überholt - so zumindest die Theorie. In der Praxis gibt es auch Zweifel am Modell des freien Zugangs. „Über Datenbanken kommt man bei der Recherche manchmal auf Open-Access-Artikel, aber in der Regel ist man dann skeptisch“, sagt Marcus Maurer, Professor am Institut für Publizistik der Universität Mainz. Es gebe nämlich einen völlig unübersichtlichen grauen Markt für Open-Access-Journals. Maurer war selbst schon einmal Peer Reviewer für die Open-Access-Zeitschrift „Plos One“ der Public Library of Science. Die ist für ihn aber auch die einzige Open-Access-Zeitschrift, die die Qualitätsstandards erfülle. Max-Planck-Koordinator Botz ist allerdings überzeugt, dass unseriöse Journals nur kurzfristig Erfolg haben können. Denn auf lange Sicht werde wie auch bei herkömmlichen Zeitschriften die Reputation den Markt regulieren. Zudem würden einer kürzlich erschienenen Studie zufolge Open-Access-Artikel deutlich öfter zitiert als Artikel aus Abonnement-Zeitschriften. Qualitätssicherung variiert von Fach zu Fach „Open Access ist zu wichtig, um es übers Knie zu brechen“, sagt dazu der Frankfurter Jurist Kettemann, der selbst fast ausschließlich im Open-Access-Modell publiziert. Er glaubt, dass die Qualitätssicherung von Fach zu Fach sehr stark variiert. Während er selbst als Internetrechtler froh ist über eine kürzere Review-Phase, weil seine juristischen Kommentare dann nicht schon vor der Veröffentlichung wieder veraltet seien, könnte eine zu kurze Prüfung bei naturwissenschaftlichen Publikationen fatal sein. Auch der Berliner Jurist und Open-Access-Experte Till Kreutzer publiziert fast ausschließlich auf diesem Weg. Für ihn ist es kein rechtliches Problem, wenn der Arbeitgeber Wissenschaftlern die Veröffentlichung ihrer Forschungsergebnisse über Open Access vorschreibt. Dies widerspreche dann nicht dem Grundrecht auf Wissenschaftsfreiheit. Forscher der Universität Konstanz hatten sich nach Einführung einer Open-Access-Pflicht in dieser Freiheit eingeschränkt gefühlt. „Open Access 2020“ soll Finanzflüsse ändern Kreutzer erwartet allerdings nicht, dass Open Access flächendeckend Erfolg haben wird, solange sich das wissenschaftliche Reputationssystem nicht ändert. Da das Renommee der Zeitschrift, in der publiziert wird, oft entscheidend für die Karriere ist, müssten hier auch hervorragende Open-Access-Zeitschriften mit Bestnoten bewertet werden. Auch Botz sagt, es sei keine gute Idee gewesen, das Verhalten der Autoren ändern zu wollen. Deshalb wolle man mit der neuesten Initiative „Open Access 2020“ nun die Finanzflüsse ändern, und dabei stünden die Universitätsbibliotheken an erster Stelle. Demnach soll das Geschäftsmodell umgekehrt werden, so dass die Universitätsbibliotheken die Kosten der Open-Access-Publikationen übernehmen und nicht mehr die Autoren. Geld dafür gäbe es nach Ansicht von Botz genug, wenn die bisherigen Abonnements zurückgefahren würden, die zum Teil sowieso schon zu teuer geworden seien. Hierbei arbeite man aber ausdrücklich mit den Fachverlagen zusammen. Die Max-Planck-Gesellschaft koordiniert diese Initiative momentan und hat schon Unterzeichner aus dem In- und Ausland gewonnen - darunter auch die Universität Mainz. Jurist Kreutzer glaubt fest daran, dass sich Open Access etablieren wird. „Ich bin absolut davon überzeugt, dass die Verlage, die nicht mitmachen, untergehen.“",
            "sentiment_score": 0,
            "magnitude_score": 0
        },
        {
            "created": "2016-11-25",
            "from": "faz",
            "content": "Internet und Demokratie (4) : Digitale Wissenschaftskontrolle Open Access läuft auf ein Kontrollsystem hinaus, das jeden Schritt von Wissenschaftlern überwacht. Die Politik verschenkt die Arbeit der Forscher an private Investoren. as größte Missverständnis bei der Digitalisierung der Wissenschaften durch die Einführung eines „Open Access“-Publikationssystems liegt in der Annahme, die Wissenschaft werde dadurch freier und demokratischer. Diesen Befreiungs- und Demokratisierungsschub denkt man sich so: Wenn die Wissenschaftler ihre Aufsätze nicht mehr in gedruckten Fachzeitschriften veröffentlichen würden, sondern digital auf Volltextservern ihrer Universitäten, müssten sie die Verwertungsrechte an ihren Veröffentlichungen nicht mehr an Verlage abtreten, die mit ebendiesen Rechten Geld verdienen. Stattdessen soll die digitale Publikation auf den universitären Volltextservern zu „Open Access“-Konditionen erfolgen, das heißt eine beliebige und für die interessierten Leser kostenfreie Nachnutzung der Veröffentlichung erlauben. Das, so glaubt man, sei die gelungene Synthese aus einer digital sich selbst organisierenden und dank Ausschaltung der Verlage ökonomiefreien und daher billigeren Wissenschaft, die übers Internet mit der interessierten Öffentlichkeit direkt in Kontakt kommen und in diesem Direktkontakt die Demokratisierung der Gesellschaft voranbringen könne. Nun darf man freilich bezweifeln, dass ein Wissenschaftssystem, in dem anstelle der konkurrierenden Intermediäre vom Typ Verlag ein vom Staat als Monopol betriebenes Open-Access-System installiert wird, das Publizieren wirklich billiger macht. Keines der politischen Systeme, die nichtstaatliche Konkurrenz ausgeschaltet haben, hat die systemischen Kosten wirklich gesenkt, sondern immer nur kreativ versteckt, bis zum finalen Ruin des Systems. Bezweifeln darf man allerdings auch, dass ein staatliches Publikationsmonopol die Wissenschaft freier und demokratischer macht. Noch jedes System, das auf eine Monopolisierung der gesellschaftlichen und wissenschaftlichen Kommunikationsprozesse setzte, hat die Freiheitsgrade seiner Bürger und Wissenschaftler drastisch beschnitten. Das sei nun aber, so hören wir, dank des Digitalen ganz anders: Milliarden digitaler Rezeptoren sollen weltweit dafür sorgen, dass die Wirklichkeit so breit und tief wie noch niemals zuvor gerade auch ins System der Wissenschaften gelangt und von dort in verarbeiteter Form über die digitalen Massenkanäle ihren Weg in die Köpfe der Menschen findet. Und weil das System vollständig transparent sein könne, wäre dadurch jedem staatsmonopolistischen Missbrauch vorgebeugt. Kommunikation und Kontrolle Man darf auch das bezweifeln. Denn das Digitale, wie es in den dreißiger und vierziger Jahren grundgelegt wurde, folgt einer Logik, die nicht nur das Buch durch ein assoziations- und denkfreundlicheres Medium ablösen möchte (Vannevar Bushs „Memex“), sondern alles Problemlösen als algorithmische Prozessierung von wohldefinierten Einzelschritten betrachtet (Alan Turings Rechnermodell), die von real baubaren Maschinen erledigt wird (Nobert Wieners Kybernetik). Aus dieser Logik ging der Computer hervor, der seither munter das tut, was er laut Norbert Wiener tun soll: Er betreibt das Geschäft von „Kommunikation und Kontrolle“, indem er die analoge Fülle der Welt in binäre Häppchen zerlegt und daraus Daten generiert, deren Fluss er kontrolliert und überwacht. Die Logik des Computers ist die einer Kontrolltechnik, die via Internet inzwischen weltweit operiert und dabei Welt-, Kommunikations- und Datenkontrolle zu Synonymen gemacht hat. Es liegt auf der Hand, dass das keine guten Voraussetzungen für Demokratie und Freiheit sind. Wie schlecht sie jetzt schon sind, zeigt ein Blick auf die in den Naturwissenschaften populären Nachweisinstrumente für wissenschaftliche Aufsätze, das Web of Science (WoS) und den SciFinder. Sie punkten auf der Nutzerseite mit einem bequemen Zugang zu bibliographischen Informationen und digitalen Volltexten. Und auf der Seite der Universitätsverwaltungen punkten sie damit, dass sich mit ihrer Hilfe leicht ermitteln lässt, welche Aufsätze welcher Professoren wie oft zitiert wurden; daraus lassen sich Kennzahlen ableiten, mit denen man die monetären Belohnungsflüsse in den Universitäten steuern kann. Kippt an dieser Stelle schon die Bequemlichkeit der Nutzung in eine Kontrolle der Nutzer um, zeigt sich der Charakter der Kontrolltechnik vollends, wenn man sich vor Augen führt, dass das WoS und der SciFinder die IP-Adressen der Computer abgreifen und der SciFinder sogar ein personalisiertes Login-Verfahren benutzt, so dass die an einem Computer und von einer Person durchgeführten Recherchen mitgeschnitten und statistisch ausgewertet werden können. Das Verb „können“ meint in diesem Kontext keine schwache Möglichkeit, sondern eine starke Realität: Die wissenschaftlichen Bibliotheken, die ihren Nutzern einen Zugang zu WoS und SciFinder anbieten, erhalten nicht nur monatliche Statistiken mit allgemein gehaltenen Nutzungsdaten dieser Instrumente, sondern können bis zur IP-Adresse und dem einzelnen Nutzer herunter sehen, wer an der jeweiligen Universität wie lange zu welchen Themen recherchiert hat. Instrumente für Wissenschafts- und Industriespionage Man darf vermuten, dass das, was den Bibliotheken als statistisches Servicepaket zur Verfügung gestellt wird, nur ein Bruchteil dessen ist, was im Hintergrund erfasst und ausgewertet wird. Mit anderen Worten: Das internationale Investorenkonsortium, in dessen Besitz sich WoS zukünftig befinden wird, und die American Chemical Society als Eigentümer des SciFinder sind in der Lage, das Rechercheverhalten ihrer Nutzer weltweit minutiös zu beobachten und für kommerzielle, aber natürlich auch für politische Zwecke auszuwerten. Kurz: Wir haben hier keine Instrumente vor uns, die für universitäre Kontrollzwecke und für die Wissenschafts- und Industriespionage missbraucht werden können, sondern Instrumente, zu deren Design die Kontrolle samt der Wissenschafts- und Industriespionage gehören. Es gibt keinen Grund, von Open Access etwas anderes zu erwarten. Denn der von den Bibliotheken zu Open-Access-Konditionen bereitgestellte „Content“ - die Aufsätze und Bücher der Wissenschaftler - wird wie bei WoS und SciFinder weltweit übers Internet zugänglich gemacht. Zugleich aber lassen sich die Download-Zahlen der Aufsätze und Bücher mit den Personendaten der Wissenschaftler verknüpfen, um die in den Naturwissenschaften seit langem schon verbreiteten bibliometrischen Zitationsindizes endlich auch in den Sozial- und Geisteswissenschaften einzuführen. Das läuft auf eine Ausweitung der Kontrollzone der Universitätsverwaltungen hinaus. Damit ist die Logik der Kontrolle aber keineswegs erschöpft. Denn der von den Bibliotheken zu Open-Access-Konditionen ins Netz gestellte „Content“ wird zuletzt von jenen Akteuren angeeignet, die die Datenflüsse im Netz steuern und kontrollieren. Wobei der Staat hier weniger einen Anlass für eine Intervention sieht, sondern die Gelegenheit zu Kooperation, denn über die in amerikanischer Hand befindlichen Monopolisten vom Typ Google, Amazon und Facebook erfährt auch er, was seine Bürger denken und tun. Und so gelingt es diesen Monopolisten, unbehelligt vom Staat und ohne demokratische Legitimation, aber unter dem Zuckerguss der Bequemlichkeit Kontrollsysteme als gesellschaftlich-technischen Normalzustand zu installieren. Daten sind Geld Jedem Internetnutzer sollte inzwischen klar sein: Es sind diese Personen- und Verkehrsdaten, mit denen im Netz das Geld verdient wird, nicht der „Content“. Klar sollte auch sein, dass mit den Gewinnen eine Politik finanziert wird, die auf einen Ausbau der nichtstaatlichen Kontrollzone zielt, in der die Daten und das Geld nur so fließen können. Das erklärt, warum ein als Philanthrop auftretender Investor wie George Soros von Anfang an zu den Finanziers und Impulsgebern der Open-Access-Bewegung gehört. Er hat ganz offensichtlich verstanden, dass Open Access das ideale Instrument darstellt, um Gewinnabsichten nun auch in den Wissenschaften durchzusetzen. Man muss dazu nur erstens der Öffentlichkeit und Politik einreden, dass Open Access dank der Ausschaltung der Verlage aus dem wissenschaftlichen „Content“ ein kostenloses Gut macht. Zweitens kann man es dann den in diesen Köder beißenden staatlichen Akteuren in der Forschungsförderung und den Universitäten überlassen, die auf Seiten des Staates bei der Erstellung von digitalem „Content“ anfallenden Kosten kreativ zu verstecken. Und schon kann man drittens den von den Bibliotheken frei Haus ins Netz gelieferten kostenlosen „Content“ zur Basis von allerlei „Geschäftsmodellen“ machen, die davon leben, dass sie den um diesen freien „Content“ sich formierenden Datenverkehr zur Handelsware machen. Am Ende hat der Staat seine Wissenschaft verschenkt, aber es ist in Wahrheit kein Geschenk an seine Bürger, sondern ein Geschenk an Google und Konsorten. Der Staat verschenkt die Arbeit seiner Wissenschaftler Viele glauben, das sei kein Problem, weil dadurch eine Win-win-Situation entstehe: Die Bürger bekommen im Netz eine kostenlose Wissenschaft, und andere verdienen damit auch noch Geld. Dass die Verlage bei diesem Spiel nichts mehr zu melden haben, sei ein hinnehmbarer Kollateralschaden. Sie übersehen aber, dass der eigentliche Schaden in der Wissenschaft angerichtet wird. Denn um wissenschaftliche Publikationen zu verschenken, muss man über die Geschenke auch verfügen können. Und hier liegt die eigentliche Crux: Die wissenschaftlichen Veröffentlichungen sind das Eigentum der Autoren; so will es das Grundrecht der Wissenschaftsfreiheit, so will es das Urheberrecht, und so wollen es die Autoren. Denn nur wenn das, was sie veröffentlichen, ihr Eigentum bleibt, greift die Logik des Widerstands, die in jedem Eigentum liegt: Es gehört mir, nicht dir und auch nicht dem Staat; und weil das so ist, habe ich in meinem Eigentum die Sphäre meiner Freiheit, die meine Mitbürger und den Staat auf Abstand hält, wenn sie meine Freiheit beschneiden wollen. George Soros und seine Open-Access-Freunde haben auch das verstanden: Sie setzten von Anfang an auf eine Reduktion des Urheberrechts auf das einfache Recht der Autoren, „dass ihre Arbeit angemessen anerkannt und zitiert wird“, wie es in der initialen Budapester Open-Access-Erklärung aus dem Jahr 2001 heißt. In der Tat: Mehr als das Recht auf einen bibliographischen Beleg bleibt den Wissenschaftlern nicht, wenn sich der Staat anmaßt, ihre Arbeiten als einen „Content“ zu verschenken, mit dem die digitalen Kontrolleure ihre Überwachungssysteme am Laufen halten.",
            "sentiment_score": 0,
            "magnitude_score": 0
        },
        {
            "created": "2016-09-30",
            "from": "faz",
            "content": "Open-Access-Strategie : Staatsautoritarismus, groß geschrieben Blauäugigkeit 4.0: Mit seiner Open-Access-Strategie betreibt das Bildungsministerium Forschungspolitik nach Gutsherrenart. Grundrechtsverstöße interessieren nicht. Die Ministerin folgt blind ihren Mitarbeitern. Niemand hat, natürlich, die Absicht, Forscher grundgesetzwidrig zu einer Open-Access-Publikation zu zwingen. Dieser Niemand hat viele Gesichter. Er sitzt in Ministerialstuben in Bundesländern, er sitzt in den noblen Lounges in Brüssel, und er sitzt in den klimatisierten Büros des Bundesministeriums für Bildung und Forschung in Berlin. Von dort erging jüngst, adressiert an die Forscher der Republik, ein „Strategie“ genanntes Open-Access-Reglement. Das Papier war sorgsam flankiert von den üblichen Taktiken bestellter Gutachten und konsensfähig gemacht von den durchweg parteiisch besetzten Podien auf den üblichen Urheberrechts(abschaffungs)konferenzen und Open-Access-T-Shirt-Wochen - finanziert, natürlich, von niemandem. Die „Strategie“, wenn man sie einmal so nennen will, ist sehr einfach. Statt die Vielzahl von Publikationsformen und die in Urheberrecht und Grundgesetz Artikel 5 Absatz 3 verankerten Rechte von Lehrern und Forschern zu pflegen, mindestens aber zu achten, prätendiert das Ministerium, die Weisheit über die Zukunft des Publikationswesens mit Löffeln gegessen zu haben. Es schreibt für Publikationen, die aus BMBF-geförderten Forschungen hervorgehen, eine zwangsweise Open-Access-Veröffentlichung vor. Basta! Kernrecht der Forschung und Lehre Die barock-absolutistische Mischung von Arroganz und Besserwisserei, mit der das Papier verfasst ist, hat Züge des Despotischen. Behauptet wird zunächst, das Ministerium folge dem einhelligen Wunsch „der Wissenschaft“, ohne dass dafür irgendwelche „belastbaren“ Daten vorlägen. Die abgehobene Führungsetage des BMBF versteht sich schon lange als Repräsentant der zugleich von ihm finanziell abhängigen und undurchdringlich mit ihm verwobenen großen Wissenschaftsorganisiationen. Wäre in unserem demokratischen Staatswesen alles in Ordnung, müssten Ministerien die Steuerungsbedürfnisse und Spezialinteressen großer Institutionen eher eingrenzen, als sich als deren Agenten am Kabinettstisch zu gerieren. Das Gegenteil ist der Fall. Kühn wird behauptet, dass sich das Reglement des BMBF keineswegs über geltendes Recht und Grundgesetz hinwegsetze. Schließlich stehe es jedem frei, auf die Mittel des BMBF zu verzichten. Statuiert wird, mit einem beträchtlichen Maß an Heuchelei, es handle sich bei den BMBF-Förderangeboten um fair auszuhandelnde Verträge unter gleichberechtigten Partnern. Angesichts der nahezu vollständigen Drittmittelabhängigkeit der Forscher an Universitäten und der nivellierten Open-Access-„Strategien“ der großen Wissenschaftsförderinstitutionen ist das ein schlechter Witz. In Wahrheit tritt ein vor Struktur- und Finanzmacht strotzendes Fördermonopol einem durch und durch abhängig gemachten Wissenschaftler gegenüber und greift rücksichtslos in dessen verbürgte Grundrechte ein. Die Souveränität eines wissenschaftlichen Autors, über den Ort seiner Publikation autonom zu entscheiden, ist aber keine Marginalie. Sie folgt als Kernrecht aus der Freiheit von Forschung und Lehre und darf niemals von wissenschaftsexternen Interessen abhängig gemacht werden. „Seltsam fortschrittsbeschwipst“ Nähme der deutsche Gesetzgeber seine Rolle im Staate umsichtig wahr, dann dächten demokratische Abgeordnete nicht über Einschränkungen des Urheberrechts nach, sondern genau umgekehrt darüber, wie zu verhindern ist, dass Wissenschaftler auf eine derart anmaßende Weise von Institutionen gegängelt werden, die einmal zur Förderung individueller Forschung eingerichtet wurden, mittlerweile aber immer mehr zu Anstalten von Forschungssteuerung heruntergekommen sind. Hier ist in den immer präpotenter agierenden Fördereinrichtungen etwas völlig außer Kontrolle geraten. Die publikationsbegleitende Audienz, die Bildungsministerin Wanka der „Welt“ in Gestalt eines Interviews gab, lässt diesbezüglich tief blicken. Es handelt sich um ein schwer goutierbares Ragout aus krud neoliberalen Vorstellungen von Wissenschaftsmärkten („Monitoring“ darf, natürlich, nicht fehlen), virtueller DDR 5.0 (mit Enteignung der geistigen Produktion) und Staatsautoritarismus wilhelminischer Anmutung. Man weiß gar nicht, ob man mehr über die Blauäugigkeit oder das hypertrophe Selbstbewusstsein staunen soll, das in der gewährten Fragestunde zum Ausdruck kommt. Open Access erscheint da auf einmal, seltsam fortschrittsbeschwipst, als Einlösung des auf Grund gelaufenen Atomenergieversprechens der fünfziger und sechziger Jahre: „Moderne Innovationen können den Alltag zunehmend vereinfachen, unsere Ökosysteme entlasten und die Gesundheit der Menschen fördern.“ Open-Access-Zirkus Man meint das ZK der SED zu hören, wenn vollmundig und gegen jeden Zweifel immun die kühne These in den Raum gestellt wird, durch Open Access vermögen „die Wissenschaften“ „rasanter voranschreiten als je zuvor“. Und dann kommt, natürlich, auch die Einschärfung des Zwangs zur U-Boot-, pardon: Open-Access-Politik: „Was allerdings gar nicht geht, ist eine Verweigerungshaltung gegenüber präsenten Entwicklungen.“ Wenn man das alles so liest, wünscht man sich schon einmal eine kenntnisreichere Beraterin für eine solche Ministerin ins BMBF als die (beamtete?) Staatssekretärin Bettina Klingbeil. Jemanden, der ihr den einfachen Unterschied zwischen Patent- und Urheberrecht erläutert; der ihr geduldig darlegt, worin die vom Grundgesetz geschützte Differenz zwischen freier Forschung und Auftragsarbeiten liegt. Und, vor allem, was ein staatlich verbrämter Übergriff in verbürgte Grundrechte ist: nämlich ein Fall für den Verfassungsschutz. Warum will im BMBF eigentlich niemand mehr zuhören oder vielleicht sogar etwas lernen? Etwa aus der Studie des österreichischen Pendants zur Deutschen Forschungsgemeinschaft, dass Open-Access-Publizieren nicht nur nicht billiger, sondern deutlich teurer ist als herkömmliches? Und dass der ohne Not veranstaltete Open-Access-Zirkus mit den popeligen T-Shirts nur den großen internationalen Oligopolverlagen zuarbeitet und nicht etwa, wie oft nachgebetet, deren Macht begrenzt? Ist das zu viel verlangt für eine Institution, die das altehrwürdige Wort „Bildung“ in ihrem Namen führt?",
            "sentiment_score": 0,
            "magnitude_score": 0
        },
        {
            "created": "2013-10-05",
            "from": "spiegel",
            "content": "Open-Access-Bewegung Journale akzeptieren anstandslos gefälschte Forschungsergebnisse Der Begriff 'Open Access' steht für freien Zugang zu Studienergebnissen und die Demokratisierung der Wissenschaft. Viele derartiger Zeitschriften verzichten aber offenbar auf die Kontrolle der eingereichten Artikel. Eine frei erfundene Studie wäre zigfach abgedruckt worden. Mehr als 300 Studien hat John Bohannon im vergangenen Jahr bei wissenschaftlichen Verlagen eingereicht, mehr als die Hälfte hätte seine Arbeit auch abdruckt. Letztlich erschienen ist aber keine einzige: Der Wissenschaftsjournalist hat sich die Studien ausgedacht und absichtlich mit Fehlern versehen. Er wollte zeigen, dass bei vielen Open-Access-Zeitschriften eingereichte Arbeiten nicht richtig geprüft werden. Und er hatte Erfolg: Zwar wurden 98 der gefälschten Studien abgelehnt, 157 Journale aber akzeptierten das Fake-Papier. Seine Ergebnisse präsentiert Bohannon nun im renommierten Fachmagazin 'Science'. Dessen Inhalt ist kostenpflichtig, der Verlag steht der Open-Access-Bewegung kritisch gegenüber und dürfte von diesem Ergebnis finanziell profitieren. Das interaktive Feature dürfte die Diskussion um die Vor- und Nachteile der Open-Access-Bewegung wieder befeuern. Open Access meint den freien, unentgeltlichen Zugang zu digitalen wissenschaftlichen Inhalten und Informationen, also auch zu wissenschaftlicher Literatur und Daten, und die unentgeltliche Nutzung unter korrekter Angabe der Urheberschaft. Mehr als 10.000 Open-Access-Journale soll es mittlerweile geben. Die Journals, die die Fälschung nicht bemerkt haben, tragen durchaus plausibel klingende Namen wie 'American Journal of Medical and Dental Sciences' oder 'European Journal of Chemistry', doch sie sitzen in Indien oder Nigeria, wer sie leitet, ist unklar. Sie treten seriös auf, doch es sind betrügerische Publikationen, die auf die Gebühr aus sind, die Wissenschaftler bei Veröffentlichung ihrer Arbeiten zahlen müssen. Krebsrettende Wirkung erfunden, Ortsangaben ausgedacht Jeffrey Beall beobachtet die Entwicklung seit Jahren. Der Bibliothekar an der University of Colorado in Denver führt auf seiner Website eine wachsende Liste mit solchen betrügerischen Journals. Das Gegenteil versucht das Directory of Open Access Journals (DOAJ), das nur seriöse Fachblätter aufnehmen will. Bohannon wählte für sein Experiment 304 Journals aus, 121 stammten aus der Liste von Beall, 167 aus dem DOAJ. 15 waren bei beiden vertreten. Dann schrieb der promovierte Molekularbiologe ein Paper über die angeblich krebshemmende Wirkung von Stoffen, die aus Flechten extrahiert wurden. Bohannon baute bewusst offensichtliche Fehler ein. In einem Experiment beschreibt er, dass der Flechtenextrakt zusammen mit Röntgenstrahlung Zellwachstum hemmt. Doch die Zellen, die als Vergleich dienen sollen, wurden gar nicht bestrahlt. Da Bohannon vermutete, dass hinter einigen der Journals dieselben Verantwortlichen stecken, fertigte er verschiedene Versionen der Studie an. Durch ein Computerprogramm ließ er den Namen der extrahierten Substanz, der Flechtenart und der getesteten Zelllinie austauschen. Ähnlich ging der Journalist auch bei den ausgedachten Autorennamen vor. Er entschied sich für fiktive Autoren aus Afrika, weil es bei ihnen nach seinen Angaben am wenigsten auffallen würde, wenn sie nicht im Internet zu finden sind. Gefälschte Studien wurden durchgewunken Viele der gesammelten Daten, auch Original-E-Mails mit Redakteuren, hat Bohannon zur Verfügung gestellt. 'Ich habe auf Open Access bestanden', sagt der Journalist. Wenn die Antwort des Journals positiv ausfiel, gab es selten eine echte wissenschaftliche Begutachtung. Oft wurden die Studien einfach durchgereicht, am ehesten sollte bei der Formatierung nachgebessert werden. Nachdem Bohannon eine Zusage bekam und um die Überweisung der Gebühr gebeten wurde, zog er das Paper zurück mit der Begründung, er habe Fehler darin entdeckt. 'Die Ergebnisse bestätigen, was ich schon seit Jahren beobachte', sagt Bibliothekar Beall. 'Open Access ist schwer beschädigt.' Er sieht das Problem darin, dass diese Publikationsform den Journalen einen Anreiz gebe, so viel wie möglich zu publizieren. Solange es diese Publikationsform gibt, bleibt das Problem bestehen, glaubt Beall. Den renommierten Verlagen dürfte die Studie Bohannons viel Material für Kritik liefern - sie sind qua Geschäftsmodell automatisch gegen die kostenfreie Veröffentlichung wissenschaftlicher Arbeiten per Open Access.Der Genetikprofessor Michael Eisen sieht dagegen das Problem nicht nur bei Open Access. 'Viele konventionelle Journals wollen auch so viel wie möglich publizieren, um Bibliotheken zu einem Abonnement zu bewegen', sagt der Forscher der University of California in Berkeley. Eisen ist ein prominenter Fürsprecher von Open Access, vor ein paar Tagen machte er von sich reden, als er Studien über die Nasa-Marsmission auf seiner Homepage veröffentlichte. Im Journal 'Science' waren sie hinter einer Bezahlschranke versteckt, doch nach Eisens Argumentation sollten sie allen zugänglich sein. Schlechte Veröffentlichungen mit toxischen Effekt 'Das wahre Problem ist, dass wir es den Journals überlassen, zwischen guten und schlechten Studien zu unterscheiden', sagt Eisen. Der Mitgründer des Open-Access-Verlags 'Public Library of Science' sieht eine Lösung darin, Ergebnisse frei zu veröffentlichen und erst danach von der Forschergemeinde begutachten zu lassen. Schlechte Veröffentlichungen hätten einen toxischen Effekt, sagt dagegen Bernd Pulverer. 'Es wird heutzutage so viel publiziert, dass es für einen einzelnen Forscher äußerst schwierig geworden ist, relevante Studien zu bewerten', sagt der Leiter der Abteilung Scientific Publications der European Molecular Biology Organisation (EMBO). Er habe sich von Bohannons Recherche einen Vergleich mit konventionellen Journals gewünscht, sei aber besorgt über die hohe Fehlerquote von Journals, die bei DOAJ geführt werden. Dabei ist DOAJ ein Gütesiegel in der Branche. Die DFG beispielsweise verweist in einem Merkblatt bei der Frage, bei welchen Journals die Publikationskosten erstattet werden, auf die Datenbank. Die Liste sei nicht bindend, sagt Christoph Kümmel von der Abteilung Wissenschaftliche Literaturversorgungs- und Informationssysteme bei der DFG. Vielmehr fordere die Forschungsgesellschaft, dass bei den Journalen 'anerkannte, strenge Qualitätssicherungsverfahren' herrschten. Die Verantwortung liege letztlich bei den Universitäten. Sie müssen also dafür sorgen, dass öffentliche Gelder nicht an Betrüger fließen.",
            "sentiment_score": 0,
            "magnitude_score": 0
        },
        {
            "created": "2012-11-12",
            "from": "spiegel",
            "content": "Internet Einfluss der großen Wissenschaftsmagazine sinkt Lange haben einige einflussreiche Wissenschaftszeitschriften den Markt dominiert. Doch in Zeiten des Internets schwindet ihre Vormachtstellung allmählich. Denn immer mehr Forschungsergebnisse sind kostenfrei zugänglich. Die Informationsgesellschaft profitiert davon. Wissenschaft, hehres Streben nach Erkenntnis, stets dem Gemeinwohl gewidmet, frei von ökonomischen und ideellen Zwängen? Mitnichten. Auch die Forschungsgemeinschaft kennt Konkurrenzdruck und Knebelverträge. Mehr als hundert Jahre lang haben einige große wissenschaftliche Journale wie 'Nature', 'Science' und 'Jama' den Markt dominiert. Wer als Forscher Anerkennung genießen und durchstarten wollte, musste seine Ergebnisse in einem der angesehenen Magazine publizieren, damit diese möglichst oft zitiert werden. Doch in Zeiten des Internets bröckelt die Vormachtstellung der etablierten Magazine allmählich. Wie eine Gruppe um Vincent Larivière von der Universität Montreal berichtet, geht die Bedeutung der großen wissenschaftlichen Zeitschriften langsam aber stetig zurück. Demnach zitieren seit einigen Jahren weniger Forscher aus Arbeiten, die in besonders hochrangigen Publikationen erscheinen. Das schmälert deren Einfluss. Mit Hilfe der Internetsuche seien nun auch Artikel aus kleineren Fachjournalen einfacher zu finden. Die noch immer besondere Position des britischen Journals 'Nature' und der US-amerikanischen Zeitschrift 'Science' zeigt sich an ihrem hohen Impact Factor. Dieser gibt an, wie oft andere Zeitschriften Artikel aus diesen Journalen im Verhältnis zur Gesamtzahl der darin veröffentlichten Artikel zitieren. Eine Publikation in 'Nature' ist somit mehr wert als in anderen Magazinen. Wird beispielsweise ein Professor neu berufen, spielt die Zahl seiner Veröffentlichungen bei solch hochwertigen Adressen eine Rolle. Gerade aufstrebende Wissenschaftler fühlen sich dadurch oft gezwungen, bei den Etablierten zu publizieren und deren Konditionen zu akzeptieren. Dementsprechend konnten 'science', 'Jama', 'Cell' und andere ihre Themen stets aus einem breiten Angebot wählen. Open Access auf dem Vormarsch Wie Larivière nun herausgefunden hat, schwächt sich diese Tradition ab. 1990, auf dem Höhepunkt ihrer Dominanz, erschienen noch 45 Prozent der am meisten zitierten Artikel auch in den einflussreichsten Journalen. 2009 waren es nur noch 36 Prozent. Die Untersuchung basiert auf rund 820 Millionen Zitaten aus fast 30 Millionen Artikeln, die zwischen 1902 und 2009 publiziert wurden. 'Von 1902 bis 1990 wurden die herausragenden Entdeckungen in den prominentesten Journalen präsentiert', erklärt Larivière. Dieser Zusammenhang sei heutzutage nicht mehr so ausgeprägt. Als Grund sieht er die neuen Möglichkeiten, mit denen sich Informationen elektronisch verbreiten lassen. 'Die digitale Technik hat die Art und Weise verändert, in der Forscher über neue Texte informiert werden. Historisch haben wir alle Journale auf Papier abonniert. Die Periodika waren die Hauptquelle für wissenschaftliche Arbeiten, und wir mussten nicht außerhalb von ihnen schauen', erklärt der Informationsexperte. Das habe sich mit dem Aufkommen von Suchmaschinen wie Google Scholar verändert: 'Suchmaschinen verleihen Zugang zu allen Artikeln, ob sie nun in den prestigeträchtigen Journalen publiziert sind oder nicht', sagt Larivière. Um die Bedeutung eines Artikels und damit der Arbeit eines Forschers zu beurteilen, sollte seiner Meinung nach vor allem die Anzahl der Zitate seiner Artikel herangezogen werden und nicht mehr vorrangig die Tatsache, in welchem Fachmagazin sie erschienen sind. Damit würde der Einfluss der Impact Factors der großen Journale sinken. Auch Wissenschaftler kritisieren Praktiken der Großverlage Seit Längerem schon zeigen sich Wissenschaftler unzufrieden mit einigen Praktiken der etablierten Zeitschriften und der kooperierenden Großverlage. So hat sich Anfang 2012 ein Widerstand gegen das britisch-niederländische Unternehmen Elsevier formiert, das mit wissenschaftlichen Zeitschriften und Datenbanken viel Geld verdient. Seither haben auf der Seite thecostofknowledge.com ('Der Preis des Wissens') knapp 13000 Wissenschaftler aus allen Fachrichtungen zum Boykott des einflussreichen Unternehmens aufgerufen. Als Grund nennen sie die inakzeptabel hohen Preise, die der Verlag für Zeitschriftenabos verlange, wobei er seine Marktmacht missbrauche. Kurze Zeit später schloss sich auch die TU München dem Aufstand an und kündigte das Elsevier-Paket ihrer Bibliothekfür das kommende Jahr. Neben den hohen Kosten seien die darin enthaltenen Publikationen 'mehrheitlich nicht sehr bedeutend', wie ein Sprecher der Uni SPIEGEL ONLINE mitteilte. Komm zum ITZBund und führe Deutschland in die digitale Zukunft – mit webbasierten IT-Anwendungen und schnellen Kommunikationswegen, damit der Gang zum Amt überflüssig wird. Als IT-Dienstleister des Bundes bieten wir dir spannende Aufgaben und viele Entwicklungsmöglichkeiten. Bewirb dich jetzt und gestalte Deutschlands digitale Zukunft! Als Gegenentwurf zu den kostenpflichtigen Diensten gelten sogenannte Open-Access-Seiten wie die 'Public Library of Science (Plos)', die Publikationen kostenlos bereitstellen und mitunter viele Fachrichtungen vereinen. Im Sommer 2012 hat sich auch die EU in der Frage eingeschaltet. Von 2014 an sollen Ergebnisse von Forschungsprojekten, die aus Steuermitteln finanziert wurden, für jedermann frei verfügbar sein. Geht es nach Larivière und seinen Kollegen, werden solche nicht-kommerziellen Projekte in Zukunft weiter an Zustimmung gewinnen. Passend zu ihrem Ergebnis erscheint die Studie der kanadischen Forscher auch nicht in 'Nature', 'Science' oder 'Jama', sondern im 'Journal of the American Society for Information Science and Technology'. Die Zeitschrift wird von der amerikanischen Organisation für Informationswissenschaft herausgegeben. Laut den Experten sind die großen Magazine dem Computer in einem Punkt aber weiterhin voraus: Ihre Redaktionen lassen die eingereichten Arbeiten von Experten des jeweiligen Feldes begutachten. Diese Qualitätskontrolle unter dem Namen 'Peer Review' ist bei den renommierten Journalen Standard, bei Open-Access-Angeboten jedoch nicht immer gegeben.",
            "sentiment_score": 0,
            "magnitude_score": 0
        },
        {
            "created": "2012-07-17",
            "from": "spiegel",
            "content": "Open Access EU will Forschungsergebnisse frei zugänglich machen Die Macht der wissenschaftlichen Großverlage wäre gebrochen: Ergebnisse von Forschung, die aus Steuermitteln finanziert wurde, sollen künftig allen zur Verfügung stehen. Doch Open Access stellt junge Forscher vor ein Dilemma. Forscher, Studenten und Unternehmen sollen künftig freien Zugang zu allen öffentlich geförderten wissenschaftlichen Studien in der EU bekommen. Frei zugängliche Artikel und Daten machten es Forschern und Firmen leichter, die Ergebnisse zu nutzen und die Wissenschaft voranzubringen, teilte die EU-Kommission am Dienstag bei der Präsentation ihres Vorschlags in Brüssel mit. Der Steuerzahler habe schon aus Prinzip ein Recht auf diese Daten, sagte die für Digitales zuständige EU-Kommissarin Neelie Kroes: 'Man zahlt für die Forschung - dann sollte man auch Zugriff auf die Ergebnisse haben.' Die EU unterstützt damit einen Vorstoß der Regierung Großbritanniens. David Willetts, britischer Hochschul- und Wissenschaftsminister, hatte am Montag Pläne vorgestellt, wonach innerhalb von zwei Jahren alle aus öffentlich finanzierter Forschung hervorgegangenen Publikationen für jedermann frei zugänglich werden sollen. Bibliotheken sollen keine Abos mehr abschließen Laut Willetts sollen die Kosten, die die großen kommerziellen Verlage wie Reed Elsevier oder die Nature Publishing Group für das Begutachten, Publizieren oder die Veröffentlichung im Netz verlangen, dann durch die Forschungseinrichtungen und die Universitäten getragen werden. Zum Ausgleich bräuchten deren Bibliotheken dann keine kostenintensiven Zeitschriftenabonnements mit den Großverlagen mehr abschließen. Aktuell machen renommierte wissenschaftliche Journale ihren Umsatz hauptsächlich mit den Abonnements, die sie mit Universitätsbibliotheken abschließen. Würden die Pläne der EU-Kommission oder auch der britischen Regierung umgesetzt, würde dies einen großen Wechsel für das Geschäftsmodell der Großkonzerne bedeuten. Parallel setzen immer mehr Verlage und auch Universitäten auf das Prinzip des Open Access: Wissenschaftlicher Inhalt wird frei und kostenlos im Internet zur Verfügung gestellt. Die großen Open-Access-Verlage, wie etwa die Public Library of Science (PLoS), prüfen ihre Inhalte zwar nach demselben Verfahren wie konventionelle Herausgeber. Ihre Verwaltungskosten legen die Open-Access-Magazine jedoch auf die jeweiligen Autoren um, sofern diese nicht ein Sponsor übernimmt. Junge Forscher in der Klemme Viele Wissenschaftler gehen allerdings nicht davon aus, dass sich an der gängigen Praxis des Publizierens schnell etwas ändert: Schließlich schreiben sie ihre Artikel, für die sie oft jahrelang geforscht haben und in die viel Steuergeld geflossen ist, um diese dann auch in einer möglichst häufig zitierten, angesehen Fachzeitschrift zu veröffentlichen. Viele dieser Zeitschriften werden noch von den großen Wissenschaftsverlagen wie Elsevier verlegt. Gerade junge Wissenschaftler könnten es sich nicht leisten, nicht bei den renommierten Verlagen zu veröffentlichen. Doch die Versuche, die Marktmacht der großen Verlage zu brechen, nehmen zu. Im März protestierten 8000 Wissenschaftler aus aller Welt gegen die aktuellen Bedingungen des wissenschaftlichen Publizierens. Auf der Seite thecostofknowledge.com erklären sie, dass sie nicht mehr mit Elsevier zusammenarbeiten wollen, darunter etwa 1500 Mathematiker. Im Mai sorgte diemathematische Fakultät der Technischen Universität München (TUM) für hitzige Debatten. Die Fakultät hatte sich entschieden, ein Abo-Paket des Elsevier-Verlags zu kündigen. 'Aufgrund unzumutbarer Kosten und Bezugsbedingungen hat das Direktorium des Zentrums Mathematik beschlossen, alle abonnierten Elsevier-Zeitschriften ab 2013 abzubestellen', hieß es auf der Webseite. Damit war die nächste Stufe eines seit langem schwelenden Konflikts zwischen Unis, Bibliotheken, Forschern und dem Verlag erreicht. Der Verlag reagierte mit einem offenen Brief auf den Protest: Einige Fakten würden falsch interpretiert, steht darin - trotzdem würde die Petition sehr ernst genommen. Die EU-Kommission gibt sich unbeeindruckt. Die Umstellung auf Open Access solle von 2014 an nach und nach umgesetzt werden - und zwar für Projekte, die von der EU oder vom Staat finanziert werden. Bis 2016 sollen 60 Prozent der veröffentlichten Ergebnisse in Europa frei zugänglich sein.",
            "sentiment_score": 0,
            "magnitude_score": 0
        },
        {
            "created": "2010-07-14",
            "from": "spiegel",
            "content": "Open Access Union will Urheberrecht von Forschern stärken In Deutschland werden Texte durch das Urheberrecht geschützt. Doch Wissenschaftsmagazine setzen zunehmend auf die freie Verbreitung im Internet. Verlage und Autoren fühlen sich bedroht - Open Access hat viele Kritiker. Jetzt will die Union wissenschaftliche Urheber besser absichern. Wie verhindert man Fehler und Betrug in der Wissenschaft? Der Standard lautet 'Peer Review', ein Verfahren zur Qualitätssicherung einer wissenschaftlichen Arbeit, bei dem diese von anderen Forschern kritisch begutachtet wird. Doch das Verfahren steht immer wieder in der Kritik, denn es dauert oft lange und ist teuer - was zur Folge hat, dass wissenschaftliche Fachblätter oder Abonnements mitunter nur sehr teuer zu kaufen sind. Open Access Dennoch denken immer mehr Fachverlage darüber nach, wissenschaftlichen Inhalt frei und kostenlos im Internet zur Verfügung zu stellen, was als bezeichnet wird. Die großen Open-Access-Verlage wie etwa die Public Library of Science (PLoS) prüfen ihre Inhalte zwar nach demselben Verfahren wie konventionelle Herausgeber. Ihre Verwaltungskosten legen die Open-Access-Magazine jedoch auf die jeweiligen Autoren um, sofern diese nicht ein Sponsor übernimmt. Kritiker bemängeln allerdings, dass diese Kosten teilweise derart in die Höhe schießen, dass sie die Abonnementkosten für Fachzeitschriften bisweilen sogar übersteigen. Ein weiteres Problem des Open Access: Die Autoren sehen oft ihre Urheberrechte nicht ausreichend geschützt. Das will die Regierung nun ändern und Open Access damit weiter vorantreiben: Wissenschaftliche Urheber sollen nach Ansicht der Union künftig besser rechtlich abgesichert werden. Das forderten der stellvertretende Vorsitzende der CDU/CSU-Bundestagsfraktion, Michael Kretschmer, und der zuständige Berichterstatter Tankred Schipanski nach einer Anhörung von Verbänden und Sachverständigen zum Thema Open Access durch das Bundesjustizministerium. 'Wir brauchen neue rechtliche Rahmenbedingungen für das wissenschaftliche Publizieren im Informationszeitalter', erklärten sie. Es bestehe Gesetzgebungsbedarf, um einerseits Open Access zu fördern und andererseits die Stellung der wissenschaftlichen Urheber rechtlich stärker abzusichern. Ein verbindliches Zweitveröffentlichungsrecht wäre der geeignete Weg, um Open Access zielführend und wissenschaftsfreundlich auszubauen. Mit dem Zweitveröffentlichungsrecht hätten Wissenschaftler die Möglichkeit, ihre Forschungsergebnisse neben der herkömmlichen Verlagspublikation, beispielsweise auch im Rahmen von Online-Aktivitäten, zu veröffentlichen, hieß es weiter. So würde eine neue Kommunikations- und Publikationskultur in der Wissenschaft ermöglicht. Auch der Verbreitungsgrad von Forschungsergebnissen könnte deutlich erhöht werden. Jetzt müsse noch über die konkrete Ausgestaltung des Gesetzes, insbesondere über die notwendigen Sperrfristen diskutiert werden. Erst kürzlich hatte ein Schweizer Hirnforscher eine softwarebasierte Peer-Review-Methode patentieren lassen, um das teure Gutachterverfahren grundlegend zu erneuern - und Open-Access-Magazinen so ebenfalls neue kostengünstigere Möglichkeiten zu bieten.",
            "sentiment_score": 0,
            "magnitude_score": 0
        },
        {
            "created": "2016-09-20",
            "from": "spiegel",
            "content": "Open Access Zugang zu wissenschaftlichen Publikationen soll besser werden Forschungsarbeiten, die von Steuergeldern ermöglicht wurden, sollen leichter einsehbar sein. Forschungsministerin Wanka hat eine entsprechende Initiative gestartet. Wissenschaftliche Publikationen sollen künftig für jeden leichter zugänglich sein. Das Bundesforschungsministerium startete dazu am Dienstag eine Open-Access-Strategie, damit Veröffentlichungen der Allgemeinheit unentgeltlich über das Internet zur Verfügung gestellt werden. 'Freier Zugang zu Wissen ist ein Sprungbrett für die gesellschaftliche Entwicklung', erklärte Forschungsministerin Johanna Wanka (CDU). Zu der Strategie gehört etwa eine Klausel für alle durch das Forschungsministerium geförderten Projekte. Wissenschaftliche Artikel zu diesen Projekten sollen demnach entweder gleich unter einem Open-Access-Modell publiziert oder nach Ablauf einer Embargofrist in einen geeigneten Dokumentenserver eingestellt werden können. Das Ministerium will zudem Länder, Hochschulen und Forschungseinrichtungen beim Ausbau ihrer Aktivitäten in diesem Bereich unterstützen. 'Wichtig ist mir, dass die Ergebnisse von Forschung, die mit Steuergeld gefördert wurden, für die Allgemeinheit unentgeltlich verfügbar werden', erklärte Wanka. Die digitalen Medien ermöglichten solche Publikationen, 'und wir müssen es schaffen, dass diese Chancen stärker ergriffen werden'.",
            "sentiment_score": 0,
            "magnitude_score": 0
        },
        {
            "created": "2018-09-06",
            "from": "sz",
            "content": "Open Access in der Forschung:Wissen soll für alle sein Damit Studierende und Forscher auch künftig noch Zugang zum gesamten Wissen der Welt haben, setzen sich viele Universitäten für Open Access ein. Forschungsförder-Einrichtungen aus elf Ländern finanzieren von 2020 an nur noch Projekte, deren Ergebnisse frei zugänglich im Internet veröffentlicht werden. Mit diesem Open-Access-Modell sollen Experten und Laien weltweit stets lesen können, was andere Wissenschaftler herausgefunden haben. Bislang verlangen die Verlage wissenschaftlicher Magazine oft horrende Preise für Abonnements. Die Daten der Welt liegen bei Google und Facebook, aber das Wissen der Welt, das liegt woanders. Ein großer Teil davon zumindest ist in der Hand einiger weniger Wissenschaftsverlage, die den Zugang zu ihren Fachmagazinen in Form teurer Abonnements verkaufen. Schon seit Jahren fordern Universitäten und Forschungseinrichtungen, das Modell zu ändern und wissenschaftliche Artikel im Internet frei zugänglich zu machen, man nennt das Open Access. Mit der Zeit wurde der Ruf immer lauter, nun wurde er erhört. Am Dienstag beschlossen elf nationale Forschungsförderer und der Europäische Forschungsrat, künftig nur noch solchen Projekten Geld zu geben, deren Ergebnisse frei zugänglich im Internet veröffentlicht werden. Die jetzt aktiv gewordenen Forschungsförderer sind Vereine und Organisationen, die in ihren Ländern mit oft Hunderten Millionen Euro jährlich wissenschaftliche Projekte fördern. Das Geld kommt in der Regel vom Staat. Wer an einer Universität forscht, kann bei ihnen Zuschüsse beantragen, mit denen etwa eine Stelle, benötigte Geräte oder auch Reisen zu Konferenzen bezahlt werden. Organisationen aus elf verschiedenen Ländern haben nun die Initiative cOAlition S gegründet und angekündigt, von 2020 an nur noch Projekte zu fördern, wenn die Ergebnisse Open-Access-publiziert werden. Auch der Europäische Forschungsrat schloss sich der Initiative an. Die Förderer werden den Forschern die Gebühren für ihre Veröffentlichungen bezahlen, die sie beim Open Access-Modell selbst zu tragen haben. Dabei soll es einen Maximalbetrag geben, damit die Verlage nicht zu hohe Preise verlangen. Zudem sollen Kriterien erarbeitet werden, bei welchen Magazinen die Forscher veröffentlichen dürfen, um zu verhindern, dass sie ihre Arbeiten bei unseriösen Journalen publizieren. Die Förderer sollen das überwachen. Ziel der Initiative ist, dass künftig jeder Wissenschaftler und auch interessierte Laien Zugang zumindest zu jenen Forschungsergebnissen bekommen, die mithilfe öffentlicher Gelder zustande kamen. Bislang werden Studien meistens in Fachmagazinen veröffentlicht, die nur lesen kann, wer bei den herausgebenden Verlagen Abonnements abschließt. Weil einige wenige Verlage einen Großteil des Markts kontrollieren - 2016 erschienen 40 Prozent aller Artikel bei Elsevier, Springer Nature, Wiley oder Taylor & Francis -, haben die Abos enorme Preise. Zuletzt ließen immer mehr deutsche Universitäten ihre Verträge mit den Verlagen auslaufen, weil sie ihnen zu teuer wurden. Publikations-Paywalls enthalten einem großen Teil der Wissenschaftlichen Community und der ganzen Gesellschaft einen substanziellen Anteil von Forschungsergebnissen vor, sagt Marc Schiltz, Präsident der Forschungsförderer-Vereinigung Science Europe, die cOAlition S gründete. Keine Wissenschaft sollte hinter Paywalls verschlossen bleiben. Deutsche Organisationen zögern Die elf beteiligten Einrichtungen stammen aus Frankreich, Großbritannien, Schweden, Norwegen oder den Niederlanden. Aus Deutschland ist bislang keine dabei. Die Deutsche Forschungsgemeinschaft setzt sich auch für Open Access ein, begrüßt in einem Statement auch das koordinierte Zusammenwirken. Man nehme aber an, dass Open-Access-Verpflichtungen zu erhöhten Publikationsgebühren führen können, ein Effekt, den es zu minimieren gilt. Dennoch wolle man Forscher künftig dazu auffordern, ihre Ergebnisse öffentlich zugänglich zu machen; die Open Access-Richtlinie werde gerade überarbeitet. Peter-André Alt, Präsident der Hochschulrektorenkonferenz, nannte die Initiative einen wichtigen Baustein auf dem Weg zu einem Open Access-Modell, da es die Schlüsselrolle der Forschungsförderer betone. Hans-Christian Pape, Präsident der Alexander von Humboldt-Stiftung, sagte, die Humboldt-Stiftung unterstütze Open Access finanziell und ideell. Es widerspreche aber den Grundsätzen, Forschern vorzuschreiben, wo und wie sie veröffentlichen sollen. Dem erwiderte Ralf Schimmer, stellvertretender Leiter der Max Planck Digital Library, die sich seit über zehn Jahren für Open Access einsetzt, dass Forscher immer noch veröffentlichten könnten, wo sie mögen. Die entsprechenden Kosten könnten dann in diesen Fällen immer noch von den Universitäten oder Einrichtungen übernommen werden. Wir begrüßen das Projekt cOAlition S sehr. Immerhin: Der cOAlition S-Initiative können auch künftig noch Förderorganisationen beitreten.",
            "sentiment_score": 0,
            "magnitude_score": 0
        },
        {
            "created": "2018-12-06",
            "from": "sz",
            "content": "Wissenschaftsverlage:Ohne Open Access gibt es keine Zukunft Bibliotheken versammeln das Wissen der Welt. Bisher mussten sie den Verlagen dafür viel Geld zahlen. Das soll sich ändern. Erstmalig haben auf der Open-Access-Konferenz in Berlin Vertreter der Forschung mit den drei größten Wissenschaftsverlagen diskutiert. Ziel der Verhandlungen ist ein freier Zugang zu allen wissenschaftlichen Publikationen. Ein Lösungsmodell existiert, jedoch zeigen sich nur zwei Verlage kooperativ. Es ist kurz nach halb zehn Uhr am Morgen, als Jeffrey MacKie-Mason die Zustimmung der ganzen Welt einholt. Ist Schweden dabei?, fragt der Wirtschaftsprofessor von der kalifornischen Berkeley-Universität und wendet sich nach links zu Astrid Söderbergh Widding, Präsidentin der Universität Stockholm. Schweden ist dabei, antwortet sie. MacKie-Mason nickt zufrieden. Nach und nach geht er alle auf dem Podium sitzenden Teilnehmer durch. Auch China, die Niederlande, Südafrika, Japan, die europäischen Forschungsförderer und Universitäten sind dabei. Die kleine Show soll eine Botschaft vermitteln: Wir, die Wissenschaft, stehen zusammen und wollen Open Access. Empfänger der Nachricht ist Ron Mobed, der CEO des weltweit größten Wissenschaftsverlags Elsevier, der freundlich lächelnd am Rednerpult steht. MacKie-Mason dreht sich zu Mobed: Ron, sind Sie auch dabei? Es ist die große Frage der Open-Access-Bewegung. Meinen es die Verlage ernst, wenn sie sagen, auch sie arbeiteten daran, den freien Zugang zu Wissen zum weltweiten Standard zu machen? Um einer Antwort näher zu kommen, hat die von der Max-Planck-Gesellschaft koordinierte Initiative OA2020 am vergangenen Dienstag erstmals die drei größten Verlage zur 14. Berliner Open-Access-Konferenz in ihr Tagungszentrum geladen. 2003 fand hier die erste Konferenz statt. Im Goethe-Saal, wo sich nun Mobed den Fragen stellt, unterzeichneten die Teilnehmer damals die Berliner Erklärung über den offenen Zugang zu wissenschaftlichem Wissen, das Manifest der Open-Access-Bewegung. Seither kamen verschiedene Initiativen und Ideen auf, die Gemeinschaft diskutierte und stritt. Heute ist man sich größtenteils einig, wie der Wechsel weg vom Abo-System hin zu Open Access gelingen kann: mit Transformationsvereinbarungen. Nicht mehr soll jede Bibliothek ihre eigenen Abonnements haben. Stattdessen soll es für ein paar Jahre, bis alles Open Access ist, ein Riesen-Abo für jedes Land geben. Die Bedingungen: Forscher sollen ihr Urheberrecht nicht länger an die Verlage abgeben müssen. Alle Artikel von Forschern aus dem jeweiligen Land sollen frei zu lesen sein. Und weil beim Open-Access-Modell die Forscher bezahlen, was die Verlage leicht aufschlagen könnten, gilt außerdem, dass die Länder insgesamt nicht mehr zahlen als vorher. Über die Details hört man auf der Konferenz hier und da ein paar Bedenken, doch im Großen sind sich die 170 Teilnehmer aus 37 Ländern einig. Diese Einigkeit sollen nun auch die Verlage mitbekommen, und dazu hat die OA2020-Initiative die größten drei erstmals zur Konferenz hinzu gebeten. So stellen sich nach und nach Daniel Ropers, CEO von Springer Nature, und Guido Herrmann, Verlagsgeschäftsführer von Wiley in Deutschland, den Fragen der Wissenschaftsgemeinde. Und Elsevier-Chef Ron Mobed, der dem Ökonomen MacKie-Mason gerade erklären soll, ob er bei den Transformationsvereinbarungen dabei ist. Mobed, der zunächst eingesteht, Elsevier habe vor einigen Jahren den Einstieg verpasst, versuche nun aber aufzuholen, sagt: Wir haben eine Vielzahl von Angeboten, die zu den Anforderungen passen. MacKie-Mason, Astrid Söderbergh Widding und Xiaolin Zhang aus China fragen nach: Wird Elsevier komplett auf Open Access umbauen? Wird es Transformationsvereinbarungen geben? Wie viele Abonnenten müssten noch kündigen, bis Elsevier reagiere? Stets verweist Mobed auf die Vielzahl der Elsevier-Angebote, die zu den Anforderungen passen, und mit jedem Mal wird das Stöhnen im Saal etwas lauter. Schließlich meldet sich aus der ersten Reihe Ulrich Pöschl, Direktor des Mainzer Max-Planck-Instituts für Chemie, im Namen des Publikums zu Wort: Sie bieten keine Optionen an, Sie verweigern den Dienst. Warum bieten Sie nicht das an, was wir verlangen? Applaus. Bei Herrmann von Wiley wird die Stimmung weniger angespannt sein, bei Ropers von Springer Nature wird sogar gemeinsam gelacht. Gerade Letzterem glauben einige im Saal, dass der Verlag den Wechsel wirklich will. Schon 2008 übernahm der Konzern etwa den Open-Access-Verlag Biomed Central. Wir haben mehr für die Entwicklung von Open Access getan als all die anderen Verlage, so Ropers. Einer von drei Open-Access-publizierten Artikeln erscheine bei Springer Nature. Wiley und Springer Nature haben bereits Vorformen der Transformationsvereinbarungen umgesetzt, etwa in den Niederlanden. Für Deutschland wird Ähnliches verhandelt. Im Herbst 2016 schlossen sich zahlreiche Unibibliotheken und andere Abonnenten im Projekt Deal zusammen und ließen ihre Verträge bei den großen Verlagen auslaufen, um Druck aufzubauen. Wiley und Springer Nature boten Übergangslösungen ohne Preissteigerung für 2018 an. Am Dienstag gab Springer Nature bekannt, diese auch für 2019 zu verlängern, weil die Verhandlungen so weit fortgeschritten seien, dass man spätestens Mitte des Jahres eine bundesweite Einigung erzielen werde. Mit Elsevier hingegen gibt es keine Übergangslösung, und seit Juli 2018 hat der Verlag mehr als 200 deutschen Institutionen den Zugang abgeschaltet. Doch auch Wiley und Springer Nature treiben den Wechsel nicht um jeden Preis voran. Ropers und Herrmann verweisen, wie auch Mobed, stets darauf, dass nicht alle Forscher und Förderer den schnellen Umstieg wollen. Einerseits liegen sie nicht ganz falsch. Als Science Europe, der Zusammenschluss der europäischen Forschungsförderer, im September seine cOAlition S vorstellte - in der sich mittlerweile 13 europäische Förderer dazu verpflichtet haben, von 2020 an nur noch frei zugänglich publizierte Projekte zu fördern -, gab es nicht nur Jubel. Eine schwedische Biochemikerin verfasste einen offenen Brief dagegen, den mehr als 1400 Forscher unterschrieben. Sie kritisiert, dass die Forscher nicht mehr in renommierten Zeitschriften wie Science oder Nature veröffentlichen können, weil die kein Open Access anbieten. Andererseits lohnt es sich für die Verlage schlicht nicht, zu schnell zu viel umzustellen. Die Hälfte aller Magazine sind derzeit Hybrid-Journale - man muss sie abonnieren und kann in ihnen dennoch einzelne Artikel nach dem Open-Access-Prinzip publizieren, wenn die Forscher dafür zahlen. Ohne Transformationsvereinbarungen kassieren die Verlage hier gleich zwei Mal ab.  Insgesamt aber steigt der Open-Access-Anteil beständig. 16 Prozent sind es aktuell etwa. 100 Prozent werden es wohl nie, aber bei 80 bis 90 Prozent könnte man von einem kompletten Wandel sprechen. Wann es soweit sein wird? MacKie-Mason sagt: Drei bis fünf Jahre ist möglich. Ropers ist etwas zurückhaltender: Wenn alle Beteiligten an einem Strang ziehen und in fünf Jahren 40 bis 50 Prozent des Marktes Open Access wären, dann könnten in zehn Jahren 80 bis 90 Prozent Open Access sein.",
            "sentiment_score": 0,
            "magnitude_score": 0
        },
        {
            "created": "2016-06-06",
            "from": "sz",
            "content": "Open-Access:EU will freies Wissen für alle Bibliotheken wie jene des Queen's College in Oxford geben weltweit jährlich 7,6 Milliarden Euro für wissenschaftliche Zeitschriften aus. Für die meisten wissenschaftlichen Aufsätze muss der Leser zahlen - grotesk viel in manchen Fällen. Ab 2020 soll sich das ändern. Es fragt sich nur, wie. Die Frage ist einfach und plausibel: Wenn Wissenschaftler mit Steuermitteln forschen und neue Erkenntnisse gewinnen, sollten diese Ergebnisse dann nicht öffentlich zugänglich sein? Open Access - freier Zugang zu Forschungsergebnissen - heißt das Thema, das seit etwa 15 Jahren immer wieder diskutiert wird. Bislang erscheinen laut einem Papier der Max-Planck-Gesellschaft nur etwa 13 Prozent aller Fachaufsätze in Zeitschriften, die im Internet frei zugänglich sind. Den Rest vermarkten Fachjournale, deren Abonnement zum Teil grotesk teuer ist. So entsteht die fragwürdige Situation, dass für die bereits mit öffentlichen Mitteln bezahlte Forschung ein zweites Mal bezahlt werden muss, dann nämlich, wenn Universitätsbibliotheken und Institute die Fachjournale einkaufen. Nun ist ein Datum in der Welt, zu dem sich alles ändern soll: Der Rat für Wettbewerbsfähigkeit der EU beschloss Ende Mai den Übergang zu Open Access bis zum Jahr 2020. Alle wissenschaftlichen Publikationen aus öffentlich finanzierter Forschung müssten dann ohne finanzielle und juristische Barrieren zugänglich sein. Und zwar unverzüglich. Auf diese starken Worte folgen allerdings schwammige Formulierungen: Es könnten verschiedene Modelle des Open Access genutzt werden; Sperrfristen solle es nicht geben, oder so kurze Sperrfristen wie möglich. Open Access ist ein dehnbarer Begriff. So können die Verlage Forschern das Recht einräumen, ihre Aufsätze nach einer Sperrfrist auf eigene Faust, etwa auf einem Dokumentenserver, zu veröffentlichen. Insider nennen dies den Grünen Weg. Schon die vage Forderung des Wettbewerbsrats nach möglichst kurzen Fristen ruft nun den Widerspruch der Wissenschaftsverlage hervor: Die Sperrfristen müssten lang genug sein, um das Geschäftsmodell der Verlage nicht zu gefährden, mahnt die International Association of Scientific, Technical & Medical Publishers, deren Mitglieder etwa zwei Drittel aller wissenschaftlichen Artikel veröffentlichen. Der Goldene Weg bedeutet, dass ein Aufsatz sofort nach dessen Erscheinen von der Fachzeitschrift digital und barrierefrei publiziert wird. In diesem Fall bezahlen nicht die Leser dafür, sondern diejenigen, die veröffentlichen, finanziert meist aus Mitteln ihrer Universität oder Forschungseinrichtung. Ärzte könnten sich jederzeit auf den neuesten Stand bringen. Das käme auch Patienten zugute Seit vielen Jahren werden Wissenschaftler ermuntert, ihre Ergebnisse über Open-Access-Journale zu publizieren oder zumindest auf dem Grünen Weg zugänglich zu machen - nicht zuletzt seitens der Forschungsorganisationen. Der offene Zugang als erstrebenswertes Verfahren setze idealerweise die aktive Mitwirkung eines jeden Urhebers wissenschaftlichen Wissens voraus, heißt es etwa in der Berliner Erklärung über offenen Zugang zu wissenschaftlichem Wissen, die 2003 veröffentlicht wurde. Doch Appelle allein bewegen wenig, stellt Kai Geschuhn von der Max Planck Digital Library fest. Allzu stark habe die Open-Access-Bewegung darauf gesetzt, eine Verhaltensänderung bei den wissenschaftlichen Autoren zu erreichen. Wissenschaftler aber haben andere Dinge im Auge, vor allem die Reputation des Journals, das dem eigenen Ansehen förderlich sein sollte - für Forscher am Anfang ihrer Karriere unverzichtbar. Geschuhn hält daher wenig von moralischen Appellen. Sie sagt: Open Access muss schlichtweg zum Standard-Geschäftsmodell für wissenschaftliche Literatur werden, finanziell und administrativ ermöglicht durch die Wissenschaftseinrichtungen und Bibliotheken. Das gesamte wissenschaftliche Publikationswesen umzukrempeln, fordern auch die bislang 50 Unterzeichner der im März gestarteten internationalen Initiative OA2020 - darunter alle großen deutschen Wissenschaftsorganisationen sowie die Hochschulrektorenkonferenz. Sie alle wollen weg von Abogebühren, hin zu einem Bezahlmodell für die Veröffentlichung. Die Vorteile liegen auf der Hand: Forscherkollegen in aller Welt hätten Zugriff auf den gesamten Schatz des Wissens, nicht nur auf jene Zeitschriften, die ihr Institut abonniert hat; Ärzte könnten ihre Kenntnisse jederzeit auf den neuesten Stand bringen, und wer als interessierter Zeitungsleser Näheres erfahren möchte über ein Thema aus dem Ressort Wissen, kann selbst in die Fachwelt vordringen. Ist die große Freiheit bezahlbar? Gerd Antes, Direktor des Deutschen Cochrane Zentrums in Freiburg, betont: Wenn der freie Zugang zu medizinischen Forschungsergebnissen nicht gewährleistet ist, schadet das der Wissenschaft und den Patienten. Wichtig sei allerdings eine Qualitätskontrolle: Wenn die Verlage für jeden publizierten Fachaufsatz Geld einnehmen, schafft das Anreize, auch weniger gute Arbeiten zu veröffentlichen. Problematisch findet er, die Finanzierung auf der Ebene einzelner Forschungsinstitutionen umzustellen. Kleine Einrichtungen blieben da leicht auf der Strecke. Früher hieß es: 'Wer arm ist, kann nicht lesen'. Künftig könnte es heißen: 'Wer arm ist, kann nicht schreiben.' Die Brüsseler Forderung nach Open Access, sei gut und schön, aber sie sagt leider nichts dazu, wie das umgesetzt werden soll. In Deutschland könnten die zuständigen Bundesministerien gemeinsam mit der Hochschulrektorenkonferenz, den großen Verlagen, etwa dem Branchenriesen Elsevier, ausreichende Verhandlungsmacht entgegensetzen und akzeptable Modelle aushandeln, die allen Wissenschaftlerinnen und Wissenschaftlern im Land ermöglichen, in Open-Access-Journalen zu publizieren und das veröffentlichte Wissen zu nutzen, fordert Antes. Doch Wissenschaft und Politik haben das bislang verschlafen. Zwei große Verlage haben in der vergangenen Woche nicht auf Fragen zum aktuellen EU-Beschluss reagiert. Als wichtigen Meilenstein bezeichnet ihn Georg Botz, Koordinator des Open-Access-Themas bei der Max-Planck-Gesellschaft. Jetzt wird es aber darauf ankommen, wie diese Ziele in den einzelnen Mitgliedsstaaten umgesetzt werden. Leider liegt die umfassende Open-Access-Strategie für Deutschland, die vor zwei Jahren als Teil der 'Digitalen Agenda' der Bundesregierung angekündigt wurde, immer noch nicht vor. Eine aktivere Rolle der Politik - wie in den Niederlanden - würde den Forschungseinrichtungen in Verhandlungen mit den Verlagen den Rücken stärken. Wenn Forschungsergebnisse frei verfügbar sind, lassen sich neue Zusammenhänge erkennen Es geht dabei um weit mehr, als um die Möglichkeit, Fachaufsätze kostenfrei zu lesen. Martin Hofmann-Apitius, Leiter der Abteilung Bioinformatik am Fraunhofer Institut für Algorithmen und Wissenschaftliches Rechnen in Sankt Augustin, betont den Mehrwert, der entsteht, wenn Wissenschaftler Massen von Publikationen mit automatisierten Verfahren durchsuchen und darin verborgene Zusammenhänge zutage fördern. Wir haben Methoden entwickelt, kausale und korrelative Zusammenhänge in Publikationen zu finden. Mit solchen Verfahren wurden Millionen von Abstracts in der Datenbank Medline ausgewertet. Daraus entstand beispielsweise ein Modell der Alzheimer Erkrankung, das nun frei zur Verfügung steht und mit Messungen, zum Beispiel aus klinischen Studien verglichen werden kann. Ein Positionspapier der Allianz der deutschen Wissenschaftsorganisationen betont: Die digital arbeitende Wissenschaft ist darauf angewiesen, dass Publikationen rechtlich und technisch nachnutzbar sind. Verfahren wie Text- und Data-Mining können nur angewendet werden, wenn Forschenden entsprechende Nutzungsrechte an den Publikationen eingeräumt werden. Es sei daher wichtig, dass Open-Access-Publikationen unter Nutzung liberaler Lizenzmodelle erscheinen. Alle Welt schreit 'Digitalisierung!'. Doch jetzt, wo die technischen Möglichkeiten da sind, verhindern politische und juristische Blockaden, dass sie für die Forschung und die medizinische Versorgung nutzbar gemacht werden, bedauert Gerd Antes. Doch ist die große Freiheit bezahlbar? Es geht um viel Geld: Weltweit geben Bibliotheken jährlich etwa 7,6 Milliarden Euro für Abonnements wissenschaftlicher Zeitschriften aus. Publiziert werden pro Jahr etwa 1,5 bis zwei Millionen Fachaufsätze. Damit, so rechnet die Max Planck Digital Library vor, werden pro Aufsatz etwa 3800 bis 5000 Euro im System des wissenschaftlichen Publikationswesens umgesetzt. Das ist deutlich mehr, als die rund 1300 Euro, die Open-Access-Journale durchschnittlich für die Veröffentlichung eines Artikels verlangen. In Deutschland geben Bibliotheken weit mehr als 200 Millionen Euro für Fachzeitschriften aus. Im Jahr 2014 wurden rund 70 000 wissenschaftliche Artikel mit einem deutschen Hauptautor veröffentlicht. Selbst bei einem Artikelpreis von 2000 Euro (soviel zahlt die DFG maximal) lägen die Gesamtausgaben somit bei 140 Millionen Euro. Das neue System käme also billiger. Vorausgesetzt, die Gebühren für die Publikation ersetzen die Kosten für Abonnements und fallen nicht zusätzlich an. Aus der Perspektive eines Fachjournals kann das anders aussehen. Ein so aufwendig produziertes Magazin wie Nature kann mit Bearbeitungsgebühren, wie sie derzeit gezahlt werden, nicht als 'goldenes' Open- Access-Journal existieren, sagt Nature-Chefredakteur Philip Campbell. Wir beschäftigen einen Stab von Redakteuren, die nicht nur jene Artikel bearbeiten, die gedruckt werden - etwa acht Prozent der eingereichten Manuskripte - sondern auch die, die nie erscheinen. Gleichwohl unterstütze die Verlagsgruppe Springer Nature die Open-Access-Idee. Campbell verweist auf Nature Communications, ein Tochter-Journal, das erfolgreich nach dem Goldenen Weg publiziert wird. Um seine kritische Auswahl aufrechtzuerhalten, müsste Nature aber noch weit mehr verlangen als die stolzen 5200 Pfund pro Artikel, die eine Veröffentlichung bei Nature Communications kostet, erklärt Campbell. Das ist derzeit nicht realistisch. Doch sei viel im Fluss auf dem Zeitschriftenmarkt. Campbell glaubt, dass eines Tages die vollständige Umstellung aller Fachjournale auf den Goldenen Weg gelingt. Doch da lässt sich kein Termin von oben herab aus Brüssel anordnen. Ein Grund mehr für den Brexit? Auf keinen Fall, wehrt Campbell ab, er sei unbedingt für den Verbleib in der EU.",
            "sentiment_score": 0,
            "magnitude_score": 0
        },
        {
            "created": "2013-01-14",
            "from": "sz",
            "content": "Open Access:Forscher zollen Swartz #pdftribute Der freie Zugang zu Informationen war eines der Lieblingsthemen des Hackers Aaron Swartz. Nach seinem Tod tobt nun eine Debatte über Open Access im Netz. Manche Wissenschaftler veröffentlichen aus Protest ihre Arbeiten für alle zugänglich. Eigentlich hatte Micah Allen nicht damit gerechnet, dass sein Aufruf irgendjemand interessieren würde. Also ging der Neurowissenschaftler erst einmal schlafen. Als er dann am nächsten Morgen aufwachte, hatte er es schon geschafft: Unter der Reddit-Meldung vom Tod des Hackers Aaron Swartz stand sein Kommentar an prominentester Stelle. Ganz oben, weil Dutzende Mitglieder der Community, die Swartz mitgegründet hatte, den Kommentar für den besten hielten. Ein passendes Andenken an Swartz könne es sein, nicht zugängliche wissenschaftliche Artikel online zu stellen, hatte Allen geschrieben. Deshalb solle man diese über einen Online-Speicherdienst veröffentlichen und den Link twittern. Offenbar sahen viele seiner Kollegen das ebenfalls so und verbreiteten den Aufruf - vor allem über Twitter. Dort etablierte sich für den akademischen Massenprotest der Hashtag #pdftribute. Dutzende Wissenschaftler haben dort in den letzten Stunden Dokumente veröffentlicht, eine eigens dafür geschaffene Webseite aggregiert und archiviert Twitter-Links mit dem Hashtag. Auch die Aktivistengruppe Anonymous rief über Twitter dazu auf, sich an der Aktion zu beteiligen. Ein Teil der Gruppe hatte zuvor bereits die Webseite des Instituts für Technologie Massachusetts (MIT) gehackt und zu einer neuen Copyright-Gesetzgebung aufgerufen. Diskussion über Open Access Der Protest gegen die Veröffentlichungspraxis der Wissenschaftswelt ist eine Form des zivilen Ungehorsams, die begründet ist in Schwartz' Kampf für einen freien Zugang zu Informationen. 2011 hatte der Hacker für die Öffentlichkeit unzugängliche wissenschaftliche Arbeiten veröffentlicht. Ihm drohte deshalb zuletzt eine mehrjährige Haftstrafe. Die Familie des Toten gibt den Ermittlungen in diesem Fall eine Mitschuld am Tod. Der Onlineprotest scheint indes unter Forschern eine neue Diskussion über den freien Zugang zu wissenschaftlichen Publikationen auszulösen. Kritiker der Aktion mahnten an, Wissenschaftler sollten generell unter offenen Lizenzen publizieren und nicht nur an solchen Wohlfühlaktionen teilnehmen. Auch dass viele #pdftribute-Twitterer sich zwar solidarisierten, aber nur wenige tatsächlich Dokumente veröffentlichten, wurde kritisiert.",
            "sentiment_score": 0,
            "magnitude_score": 0
        },
        {
            "created": "2010-05-17",
            "from": "sz",
            "content": "Forschung:Freier Zugang Open-Access-Zeitschriften stellen Forschungsergebnisse ins Internet - für alle zugänglich und kostenlos. Die Freiheit der Forschung wird von immer mehr Wissenschaftlern sehr wörtlich verstanden: Unter dem Schlagwort Open Access (freier Zugang) veröffentlichen sie die Ergebnisse ihrer wissenschaftlichen Arbeit für alle zugänglich im Internet. Sie reagieren damit sowohl auf die Mittelknappheit von Forschungsinstituten als auch auf die hohen Bezugspreise für wissenschaftliche Zeitschriften. Jose Carmena und Miguel Nicolelis von der Duke University stellten ihre Forschungsergebnisse ins Netz Das Internet bietet erstmals die Chance für eine weltweite und interaktive Darstellung des menschlichen Wissens und die Garantie eines weltweiten Zugangs, heißt es in einer Berliner Erklärung, die Ende Oktober auf einer dreitägigen Konferenz verabschiedet wurde. Demnach erfüllen Open-Access-Beiträge zwei Bedingungen: Der Autor räumt allen Nutzern ein umfassendes Zugangs- und Weiterverbreitungsrecht ein. Erlaubt ist auch der Ausdruck in einer kleinen Anzahl von Kopien für den persönlichen Gebrauch. Die Nutzer respektieren die Urheberschaft des Autors und weisen in einer Weise darauf hin, die den eingebürgerten Standards der wissenschaftlichen Gemeinschaft entspricht. Der Beitrag wird zusammen mit einer Erklärung zum Open-Access-Charakter an mindestens ein Online-Verzeichnis geschickt, das den ungehinderten Zugang und die langfristige Archivierung gewährleistet. Aufsehen erregende Ergebnisse Für eine solche Stelle bietet sich etwa die Public Library of Science (PLoS) an. Als das Forscher-Netzwerk in San Francisco eine erste Open-Access-Zeitschrift ankündigte, wurde diese Idee von einem großen Teil der wissenschaftlichen Verlagsbranche als zu idealistisch und unrealistisch zurückgewiesen. Nach der ersten Ausgabe von PLoS Biology sind die Kritiker aber erst einmal verstummt. Eine Forschergruppe der Duke University um Miguel Nicolelis und Jose Carmena stellte dort Aufsehen erregende Ergebnisse vor: Die Wissenschaftlern entwickelten ein Gehirn-Maschine-Interface, mit dessen Hilfe Affen allein über ihr neurologisches System einen Roboterarm in Gang setzen konnten. Die Forschungsergebnisse können möglicherweise zur Entwicklung neuartiger Behindertenhilfen verwendet werden. Es war vielleicht unser bisher bestes Paper, sagt Nicolelis, der seine Forschungsergebnisse zuvor auch schon in renommierten Fachzeitschriften wie Nature und Science veröffentlicht hat. Innerhalb von einem Tag war trotz Open Access der Zugang zu dem wissenschaftlichen Aufsatz versperrt: Den Ansturm von mehreren zehntausend Interessenten innerhalb weniger Stunden konnten die Web-Server von PLoS nicht bewältigen. Dies ist das bisher stärkste Argument für die Open-Access-Veröffentlichung, sagte Michael Eisen vom Lawrence Berkeley National Laboratory. Zusammen mit Nobelpreisträger Harold Varmus und dem Biochemiker Patrick Brown gehört Eisen zu den Begründern der Public Library of Science. Die Idee entstand aus dem Ärger über steigende Abonnementgebühren für Fachzeitschriften und über dicke Gewinnmargen der wissenschaftlichen Verlage im Geschäft mit den Zeitschriften. Außerdem machen die Verfechter von Open Access geltend, dass die Forschung mit Steuermitteln unterstützt wird - allein in den USA 57 Milliarden Dollar im Jahr. Daher sollte die Öffentlichkeit auch den freien Zugang zu den Ergebnissen erhalten. Der Autor zahlt Bei der Public Library of Science wird der Spieß umgedreht: Hier zahlen nicht die Leser, sondern die Autoren. Die Veröffentlichung der Papers kostet pro Forschungsteam und Jahr 1.500 Dollar (1.300 Euro). Für die Startfinanzierung konnte das Projekt auf eine Spende der Gordon and Betty Moore Foundation über neun Millionen Dollar zurückgreifen. Mitte nächsten Jahres ist ein zweites Open-Access-Journal geplant: PLoS Medicine. Von den Bibliotheken werde die Entwicklung mit großem Interesse verfolgt, sagt Barbara Epstein von der medizinischen Bibliothek der Universität Pittsburgh. PLoS wird sicherlich vielen Einrichtungen helfen, denen die Budgets zusammengestrichen wurden. Allerdings zweifeln Epstein und viele andere, ob so viele Wissenschaftler bereit sein werden, die PLoS-Gebühr von 1.500 Dollar zu zahlen. Für forschungsintensive Universitäten mit mehreren tausend Papers im Jahr sei das kaum machbar, befürchtet Epstein. Und hier liegt auch die Hoffnung der etablierten Verlage, dass ihnen Open Access nicht das Geschäft verderben wird. Marike Westra vom Wissenschaftsverlag Elsevier erklärte auf E-Mail-Anfrage, das Subskriptionsmodell sollte solange nicht aufgegeben werden, bis eine Alternative ihren Nutzen nicht dauerhaft unter Beweis gestellt habe. Elsevier glaubt nicht, dass das Modell 'der Autor zahlt' auf lange Sicht einen wirtschaftlich gangbaren Weg darstellt.",
            "sentiment_score": 0,
            "magnitude_score": 0
        },
        {
            "created": "2019-01-27",
            "from": "tagesspiegel",
            "content": "Wissenschaftliche Artikel Mehr Berliner Forscher veröffentlichen im Open Access Fast 40 Prozent der Berliner Wissenschaftler veröffentlichen ihre Texte inzwischen frei zugänglich im Internet - deutlich mehr noch als vor zwei Jahren.  Bis online Forschungsartikel für alle jederzeit zugänglich sind, wird es noch dauern.Foto: Mike Wolff Jede Forscherin und jeder Forscher kann im Internet ohne Hürden auf wissenschaftliche Artikel zurückgreifen: Was einfach klingt, ist im Alltag selten der Fall. In Berlin werden inzwischen aber immer mehr Artikel von Wissenschaftlerinnen und Wissenschaftlern in frei zugänglichen Magazinen veröffentlich. Im Jahr 2017 lag der Anteil der im „Open Access“ erschienen Texte aus Berliner Hochschulen bei 38,5 Prozent. Das sind 7,3 Prozentpunkte mehr als noch im Jahr 2016, teilt die Freie Universität unter Berufung auf eine Erhebung des Berliner Open-Access-Büros mit. Berlin sei damit „auf einem guten Weg“ zu dem selbstgesteckten Ziel, im Jahr 2020 die Quote von 60 Prozent frei verfügbarer wissenschaftlicher Artikel zu erreichen, heißt es. Berlin hatte das Ziel in einer 2015 verabschiedeten Open-Access-Strategie festgelegt und erfasst als erstes Bundesland überhaupt, wie viele Texte frei verfügbar veröffentlicht sind. Die Umsetzung des Open Access gestaltet sich noch immer zäh An den Open-Access-Gedanken knüpfen sich große Hoffnungen: Dahinter steht die Idee, dass Forschungsergebnisse eben nicht mehr in Bibliotheken versteckt oder durch teure Fachzeitschriften vom Publikum ferngehalten werden. Viele Unis haben sich dem Gedanken verschrieben, allerdings gestaltet sich die Umsetzung bis heute zäh. Obwohl schon heute selbst die Artikel irgendwann frei verfügbar gemacht werden können, die zunächst in einem kostenpflichtigen Magazin erscheinen, geschieht das nur bei einem Bruchteil der Arbeiten. 18,7 Prozent der von Berliner Wissenschaftlern publizierten Texte erschienen nun direkt in Open-Access-Zeitschriften. Unter Experten wird das als „Goldener Weg“ des Open Access bezeichnet. Dabei müssen die Forscher oder die Hochschule oft für die Veröffentlichung zahlen, der Senat hat dafür einen Publikationsfonds aufgebaut. Bei 13,4 Prozent handelte es sich um eine Parallelveröffentlichung auf dem „Grünen Weg“: Dabei erscheinen Texte in der Zweitverwertung auf den Webseiten von Forschern oder in Online-Archiven der Unis. 6,4 Prozent wurden auf dem „hybriden“ Weg veröffentlicht, wo es neben einer gedruckten kostenpflichtigen Version auch eine kostenfreie Onlineversion gibt. Insgesamt erhofft sich das Land Berlin von seiner Strategie auch, dass mehr Menschen außerhalb der Wissenschaft auf Forschungsergebnisse zurückgreifen können.",
            "sentiment_score": 0,
            "magnitude_score": 0
        },
        {
            "created": "2014-06-18",
            "from": "tagesspiegel",
            "content": "Wie Open Access die Forschung verändert Weg mit den Wissenskonserven Aufsätze und Monografien als Auslaufmodelle: Digitales Publizieren könnte die Arbeit von Forschern grundlegend verändern. Noch verhindern das die großen Verlage. Sie fürchten um ihre Monopole. Neues Forschen. Im Internet können Wissenschaftlerinnen und Wissenschaftler ihre Arbeit viel einfacher Schritt für Schritt... Werden sich analoge und digitale Medien auf Dauer ergänzen? Oder ersetzt die digitale Ära am Ende doch das Gutenbergzeitalter? Immer wieder stoßen in der bildungs- und medienpolitischen Diskussion diese Positionen aufeinander, die nicht gegensätzlicher sein könnten. Analoges Wissen ist über Jahrhunderte vom gedruckten Buch geprägt. Es bietet dem Leser ein eher stilles, geordnetes Reservoir an Inhalten für den intellektuellen Diskurs. Demgegenüber erscheint digitales Wissen hochgradig dynamisch, multimedial und ebenso vielfältig, allerdings auch flüchtig. Es treibt den „User“ in eine rasant steigende Flut vernetzter, interaktiver Wissensbrocken. Bei der Belletristik haben Verlage bereits reagiert Bei Belletristik- und Sachbüchern haben große Publikumsverlage bereits reagiert. Ein hervorragendes Beispiel dafür ist die von Stephan Füssel verfasste Johannes-Gutenberg-Biografie, die 2013 erschienen ist und den Text mit einer Fülle von Bildern und Fotografien, mit gescannten Materialien sowie mit Audios und Videos multimedial illustriert. Weitere Beispiele sind Erzählungen und Romane, bei denen der Leser über den Fortgang der Story entscheidet oder mit dem Autor in Kontakt treten kann. „Enhanced Publications“ nennt die Branche diese Form der multimedialen und interaktiven Veröffentlichung. Das erweist sich auch für wissenschaftliche Publikationen als hochrelevant. Für Forscher sind Erarbeitung und Verbreitung von Ergebnissen wesentliche Faktoren für den Erfolg ihrer Arbeit. In Anbetracht einer weltweit ungebremst wachsenden Wissensproduktion sind sie auf schnelle Produktions- und Distributionsverfahren angewiesen. Auf den ersten Blick sollte das explizit digitale Formen und Verfahren des Publizierens bevorzugen. Doch bei näherem Hinsehen sind diese bei Weitem noch nicht so entwickelt, wie es das Arbeiten mit Computern und dem Internet vermuten lässt. Viele wissenschaftliche Zeitschriften haben ein Monopol Warum ist das so? Die Geschichte wissenschaftlichen Publizierens mag das erklären, die auf gedruckten Büchern und Zeitschriften als verkäuflichen Waren beruht. Zu früheren Zeiten wandten sich wissenschaftliche Verlage dabei nicht primär an institutionelle Kunden wie Bibliotheken, sondern durchaus erfolgreich auch an das Privatkundensegment. Doch das ist lange her. Seit Mitte des 20. Jahrhunderts kommt es vor allem auf dem Markt für wissenschaftliche Zeitschriften zu massiven Konzentrationen, indem wirtschaftlich gefährdete kleinere Verlage von schlagkräftigeren aufgekauft wurden. Dabei entwickelten sich viele wissenschaftliche Zeitschriften zu Publikationsmonopolen. Forschungsergebnisse wurden und werden nur in dem nunmehr einzigen Fachjournal veröffentlicht. Die Verlage begleiten diese Entwicklung, indem sie sich intensiv um das Renommee von Autorinnen und Autoren bemühen – etwa mithilfe von Zitationsindices, die sich auf Zeitschriftentitel beziehen –, und durch qualitätssichernde Maßnahmen für eingereichte Beiträge, zu denen vor allem deren Begutachtung (Peer Reviewing) gehört. Von daher ist es wenig erstaunlich, dass die Zeitschriftenmonopole den Wechsel von der analogen zur digitalen Veröffentlichungspraxis ohne Verlust überstanden. Die Zeitschriftenbeiträge mögen jetzt zwar auch digital verfügbar sein. Doch wirklich multimedial aufgearbeitet sind Forschungsergebnisse dabei nur eingeschränkt. Mit der Überführung gedruckter Artikel in elektronisch verfügbare Zeitschriften sind die bestehenden Potenziale des digitalen Zeitalters bei Weitem nicht ausgeschöpft, sondern aus Gründen der wirtschaftlichen Verwertung sogar eher reduziert. Eine von vielen Folgen ist dabei, dass Forschungsergebnisse über Google oder Facebook verbreitet und dort auch gesucht und gefunden werden – oft noch bevor ein verlässliches Peer-Reviewing für die Veröffentlichung erfolgt. Doch das ist sicher keine zukunftsfähige Lösung für die Wissenschaftskommunikation. Open Access: Die Uni wird zur Verlegerin Mit dem Open-Access-Publizieren wird seit gut zehn Jahren der Versuch unternommen, vom traditionellen Publikationsmodell für digitale Veröffentlichungen Abstand zu nehmen. Im Regelfall erfolgt Open-Access-Publizieren so: Autorinnen und Autoren lassen ihre Beiträge für eine Veröffentlichung zunächst begutachten. Dann laden sie diese auf den universitätseigenen Publikationsserver (Repositorium), auf dem die „Werke“ auf Dauer online verfügbar und kostenfrei zugänglich sind. Dieses Verfahren wird an Universitäten häufig für Dissertationen genutzt. Dabei entfällt die traditionelle Rolle von Verlagen, weil diese für Herstellung und Verbreitung von Publikationen nicht mehr erforderlich ist. Anstelle des Verlages agiert beim Open-Access-Publizieren nun der Betreiber eines „Repositoriums“ als Serviceinstanz. Forschende recherchieren und suchen die Open-Access-Veröffentlichungen im Web, sofern ihnen aktuelle Publikationen nicht über ihre fachlichen „Netzwerke“ zugespielt werden. Für Autorinnen und Autoren bieten sich neue Gestaltungsoptionen für Publikationen, die auf Papier nicht realisierbar sind. Sie können etwa ihre Forschungsergebnisse mit Videos, Bildern oder Scans anreichern. Oder ihre Dokumente für maschinell erfolgende Analysen und Auswertungen (Data- und Textmining), Verlinkungen und Volltextrecherchen aufbereiten. Digital ist analog weit überlegen Trotz dieser Vorteile konnte das Open-Access-Modell die weiterhin stark an Druckwerken orientierte Praxis des wissenschaftlichen Publizierens bisher nicht ablösen. Dabei ist das digitale Paradigma dem analogen weit überlegen. Gedruckte Publikationen „frieren“ die Ergebnisse wissenschaftlicher Forschung gleichsam ein und machen sie zu „Wissenskonserven“. Mithilfe digitaler Medien lassen sich dagegen nicht nur Forschungsergebnisse publizieren, sondern der gesamte Forschungsprozess. Alle Verfahren und Wege, die zu den Forschungsergebnissen führen, können gespeichert und der Öffentlichkeit zugeführt werden. So lassen sich archäologische Publikationen zu Ausgrabungsprojekten mit digitalen Texten antiker Autoren, geografischen Daten und 3-D-Fotografien von Grabungsfunden veranschaulichen oder mit digitalen Animationen, die Gebäude oder sogar ganze Städte der Grabungsumgebung rekonstruieren. Die Ergebnisse astrophysikalischer Forschungen werden mit Beobachtungsreihen, Experimenten und Messwerten publiziert, um die Forschungsergebnisse überprüfen und nachnutzen zu können. Der Arbeitsprozess steht im Mittelpunkt, nicht das Endergebnis Historiker reichern ihre Veröffentlichung mit digitalisiertem Bild- und Textmaterial an, die ihren Forschungsvorhaben zugrunde lagen, sowie mit Veröffentlichungen vorausgegangener Forschungsergebnisse, Kommentare und Rezensionen. Im Mittelpunkt stehen dabei rundherum multimediale Veröffentlichungen von Forschungsergebnissen mit Animationen, Bildern, Experimenten, Messwerten, Simulationen, Texten und Forschungsdaten, die im Zuge der wissenschaftlichen Arbeit erzeugt oder genutzt wurden. Hinzu kommen Fassungen zu Erkenntnis- und Forschungsständen bis hin zu Diskussionen, die zu einzelnen Fragen des Vorhabens mit anderen Wissenschaftlerinnen und Wissenschaftlern geführt worden sind. Lässt sich in solchen Szenarien noch von „traditionellem Publizieren“ sprechen? Wohl kaum. Eine Internetplattform, auf der Forschungsprozesse und -ergebnisse in ständiger Wechselbeziehung stehen, macht die gängigen Formen der Ergebnispublikation eher zum Nebenprodukt. In Anbetracht einer dauerhaft angelegten Verfügbarkeit solcher Arbeitsplattformen könnten Monografien und Zeitschriftenbeiträge sogar komplett entfallen. Und wenn diese Formen wissenschaftlicher Arbeit, die derzeit noch in den Anfängen stecken, sich zu einer flächendeckenden Praxis entwickeln, werden sich die heute üblichen Publikationsverfahren deutlich verändern. Die Forschungsergebnisse stehen dann als dynamische Objekte oder als „Enhanced Publications“ zur Verfügung, ohne dass eine nochmals ausdrückliche Ergebnisveröffentlichung zwingend erforderlich ist. Forschung könnte transparenter werden In diesem Zusammenhang zeichnen sich auch neue Modelle für die Begutachtung von Beiträgen ab. So könnten für das weiterhin unerlässliche Peer Reviewing Bewertungs- und Kommentarfunktionen genutzt werden, wie sie in sozialen Netzwerken verfügbar sind. Durchaus vorstellbar ist, dass „Social networks for science“ wie ResearchGate oder Academia.edu über Autorenprofile, Kommunikationsangebote und Volltextveröffentlichungen hinaus auch wissenschaftsspezifische „Features“ für die Begutachtung von Publikationen zur Verfügung stellen. Die Chancen der Partizipation an Wissen werden durch vernetzte Arbeitsverfahren nochmals größer. Digitales Wissen integriert Daten, Inhalte, Medien und Prozesse, ermöglicht wechselseitige Interaktion und Zusammenarbeit und motiviert vor allem zu Beteiligung am Wissensprozess. Wer will da ausschließen, dass digitale Wissensgüter die analogen künftig ersetzen – dies als Prognose, deren Risiken und Nebenwirkungen aktuell niemand kennt oder abzusehen vermag.",
            "sentiment_score": 0,
            "magnitude_score": 0
        },
        {
            "created": "2015-10-19",
            "from": "tagesspiegel",
            "content": "Open Access Strategie Berlins Forschung soll sich öffnen Der Senat hat eine Open-Access-Strategie für die Wissenschaft in Berlin verabschiedet. Bis 2020 sollen zwei Drittel aller Fachzeitschriftenartikel aus Berliner Unis öffentlich zugängig sein. Bis online Forschungsartikel für alle jederzeit zugänglich sind, wird es noch dauern. 60 Prozent aller Fachzeitschriften-Artikel, die aus Berliner Hochschulen und Instituten kommen, sollen bis zum Jahr 2020 online frei zugänglich sein. Das sieht die „Open-Access-Strategie“ für die Berliner Wissenschaft vor, die der Senat jetzt verabschiedet hat und die zuvor von einer Arbeitsgruppe von Forschern der Stadt aufgestellt wurde. Die Ergebnisse der vielen Berliner Forschungseinrichtungen sollten viel mehr als bisher für alle Bürgerinnen und Bürger im Internet verfügbar sein, erklärte Wissenschaftssenatorin Sandra Scheeres (SPD). Die Umsetzung von Open Access gestaltet sich zäh An den Open-Access-Gedanken knüpfen sich große Hoffnungen: Steht doch dahinter die Idee, dass Forschungsergebnisse nicht mehr in Bibliotheken versteckt oder durch teure Fachzeitschriften vom Publikum ferngehalten werden. Viele Unis haben sich dem Gedanken verschrieben, allerdings gestaltet sich die Umsetzung oft zäh. Obwohl schon heute selbst die Artikel irgendwann frei verfügbar gemacht werden können, die zunächst in einem kostenpflichtigen Magazin erscheinen, geschieht das nur bei einem Bruchteil der Arbeiten. Monografien und Sammelbände sind noch seltener online verfügbar. Und der freie Zugang zu Forschungsdaten, etwa aus naturwissenschaftlichen Experimenten, steht weltweit ganz am Anfang. Einige Grundlagen sind immerhin in Berlin vorhanden. So verfügen die Berliner Einrichtungen über 15 „Repositorien“. Das sind Datenbanken, auf denen Forscher ihre Publikationen hinterlegen können, etwa für die kostenfreie Nutzung in der Zweitverwertung. Diese Repositorien sollen ausgebaut werden. Open-Access-Beauftrage sollen Wissenschaftler verstärkt auf ihre Rechte zur freien Weiterveröffentlichung hinweisen. Ein Berliner Publikationsfonds Befördert werden soll auch die Möglichkeit, schon bei der Erstveröffentlichung in einem für die Nutzer kostenfreien Open-Access-Journal zu publizieren. Dabei erhalten Wissenschaftler oft einen Druckkostenzuschuss von der öffentlichen Hand. Der Senat will dafür einen Publikationsfonds aufbauen. Über die Höhe des Fonds schweigt sich das Papier aber aus. Es heißt lediglich, Landesmittel sollten „nicht unnötig“ belastet werden, besser sollten Unis Anträge bei entsprechenden Programmen der Deutschen Forschungsgemeinschaft stellen. Unis werden aufgerufen, Forscher zu ermutigen, auch ihre Forschungsdaten zugänglich zu machen. Der Senat will das kulturelle Erbe der Stadt digital archivieren. Das Senatspapier sieht zudem vor, die Vergabe von Landeszuschüssen an den Ausbau des Open Access zu knüpfen. Über die Strategie wird jetzt das Abgeordnetenhaus beraten.",
            "sentiment_score": 0,
            "magnitude_score": 0
        },
        {
            "created": "2015-07-15",
            "from": "tagesspiegel",
            "content": "Open Access Freier Forschen für Berliner Unis Bis zum Jahr 2020 will der Senat einen Großteil der Fachartikel, die Berliner Forscher verfassen, online frei zugänglich machen. Doch noch gibt es große Probleme beim Open Access. Bis online Forschungsartikel für alle jederzeit zugänglich sind, wird es noch dauern. Forschungsergebnisse sollen nicht länger in Bibliotheken versteckt oder durch teure Fachzeitschriften vom Publikum ferngehalten werden, sondern im Internet frei verfügbar sein. Das ist der Kerngedanke hinter der Open-Access-Bewegung. Doch obwohl sich viele Hochschulen und Institute Open Access schon auf die Fahnen geschrieben haben, gestaltet sich die Umsetzung eher zäh. Berlin will den Prozess jetzt vorantreiben. Bis zum Jahr 2020 sollen 60 Prozent aller Zeitschriftenartikel, die Forscher in den Einrichtungen der Stadt verfassen, online frei veröffentlicht werden. Berliner Open-Access-Strategie Das geht aus der „Open-Access-Strategie für Berlin“ hervor, die eine Arbeitsgruppe für den Senat aufgestellt hat und die dem Tagesspiegel vorliegt. Auch Monografien und Sammelbände sollen so weit es geht im Open Access vorgehalten werden. Zwar sei die Infrastruktur in Berlin schon gut ausgebaut, heißt es in dem Papier. So verfügen die Berliner wissenschaftlichen Einrichtungen über 15 „Repositorien“: Das sind Datenbanken der Unis und Institute, auf denen Forscher ihre Publikationen für die kostenfreie Nutzung im PDF-Format hinterlegen können. Auch rechtlich ist es inzwischen möglich, selbst die Artikel irgendwann frei verfügbar zu machen, die zunächst in teuren Fachzeitschriften erscheinen. Bei diesem „grünen Weg“ des Open Access können Artikel schon heute nach Ablauf einer mehrmonatigen Karenzzeit als Zweitverwertung auf den Servern der Uni abgelegt werden. Dass in der Praxis hier aber „Defizite“ bestehen, geben auch die Autoren des Strategiepapiers zu. Schätzungen gehen davon aus, dass das allenfalls bei fünf bis zehn Prozent der Artikel der Fall ist. Aufbau eines landesweiten Universitätsverlags Für Berlin wird nun ein „Publikationsfonds“ vorgeschlagen, um die Hochschulen finanziell zu unterstützen. Allerdings werden keine konkreten Zahlen genannt. Bei der Mittelvergabe des Landes könnte Open Access als Indikator eingeführt werden. Die Einrichtungen sollen Open-Access-Beauftragte ernennen, um den Prozess intern zu befördern. Befördert werden soll auch der „goldene“ Weg beim Open Access. Dabei erhalten Wissenschaftler aus öffentlicher Hand Druckkostenzuschüsse, um Artikel in „richtigen“ Open-Access-Journalen veröffentlichen zu können. In diesen Journalen werden die Texte schon bei der Erstveröffentlichung kostenfrei verfügbar gemacht. An Berliner Hochschulen beheimatete Zeitschriften, die noch für Leser kostenpflichtig sind, sollen mittelfristig zu Open-Access-Magazinen umgewandelt werden, heißt es. Angeregt wird der Aufbau eines landesweiten „Universitätsverlags“. Darüber sollen auch Monografien und Sammelbände frei im Internet angeboten werden. Der Bericht fordert stärkere Bemühungen, wenn es um den freien Zugang zu Forschungsdaten geht. Das ist zum Beispiel wichtig, wenn andere Wissenschaftler Experimente nachvollziehen wollen. Das Strategiepapier soll nun bis zum Jahresende im Abgeordnetenhaus diskutiert werden.",
            "sentiment_score": 0,
            "magnitude_score": 0
        },
        {
            "created": "2017-03-21",
            "from": "tagesspiegel",
            "content": "Streit um teure Wissenschaftsjournale Hundert Prozent Open Access Abonnements wissenschaftlicher Journale passen nicht ins digitale Zeitalter. Zum Regelfall des Publizierens muss Open Access werden, schreibt der Präsident der Max-Planck-Gesellschaft. Direkter Draht. Weltweit werden inzwischen 15 Prozent aller Forschungsartikel in Open Access-Verlagen veröffentlicht. Wissensgüter werden immer seltener als Buch und immer häufiger als Datei präsentiert. Ihre Verbreitung kann heute ohne großen logistischen Aufwand über das Internet erfolgen. Das Geschäftsmodell zwischen Verlagen und Bibliotheken stammt jedoch aus einer Zeit, in der die Distribution gedruckter Bände und Zeitschriften die Herausforderung war. Das Leser-Abonnement, die Subskription als Bezahlmodell ist vor diesem Hintergrund nicht mehr angemessen. In Zeiten der Digitalisierung ist es geradezu ein Anachronismus, wenn Forschungsergebnisse hinter elektronischen Bezahlschranken verborgen bleiben. Die wissenschaftliche Zusammenarbeit, der interdisziplinäre und internationale Austausch, die Aggregation von Daten – all das wird gebremst durch die künstlichen Barrieren, die die Verlage mit ihrem Subskriptionswesen errichtet haben. 200 Millionen Euro Subskriptionskosten in Deutschland Nach wie vor wird der Zugang zu Forschungswissen teuer erkauft. Allein in Deutschland wenden die Bibliotheken von Hochschulen und Forschungseinrichtungen jährlich rund 200 Millionen Euro an Subskriptionskosten auf. Global gesehen sind es Jahr für Jahr 7,6 Milliarden Euro, die an die Verlage fließen. Die Preissteigerungen der drei großen, international aufgestellten Fachverlage und ihre Renditen sind horrend. Quasi-monopolistische Strukturen führen zu einer Asymmetrie bei Verhandlungen, die wir unbedingt aufbrechen müssen. Dabei ist die von der Wissenschaft getragene Open-Access-Bewegung gut vorangekommen: Von den 20 Verlagen, bei denen mehr als 80 Prozent der Max-Planck-Publikationen erscheinen, ist bereits jeder vierte ein Open-Access-Verlag. Weltweit werden so inzwischen 15 Prozent aller Forschungsartikel veröffentlicht. Auf dieser Basis wollen wir aufbauen und treiben mit unseren Partnern eine neue Strategie voran. Das zeigen die aktuellen DEAL-Verhandlungen mit Elsevier um eine bundesweite Nationallizenz für den elektronischen Zugang zu dessen Fachzeitschriften. Das alte Ungleichgewicht der „vielen Kleinen gegen die wenigen Großen“ löst sich auf. DEAL hat mehrere hundert deutsche Forschungsinstitutionen hinter sich und setzt dort an, wo der Hebel zur großflächigen Umstellung auf Open Access zu finden ist: bei den Vertragsbeziehungen mit den Verlagen. Mit den Kosten ließen sich 100 Prozent Open Access finanzieren So geht es bei DEAL darum, in die Nationallizenz für die Subskriptionszeitschriften von Elsevier eine Open-Access-Komponente einzubauen. Alle Publikationen von Autoren aus deutschen Einrichtungen sollen damit automatisch Open Access verfügbar sein. Ähnliche Mechanismen wurden bereits in Verträgen auf Länderebene beispielsweise in den Niederlanden etabliert. Sie in weiteren Abschlüssen mit den anderen Verlagen zu verankern und den Open-Access-Anteil auf diese Weise sukzessive auszubauen, öffnet die Perspektive für die großflächige Umstellung der bestehenden Subskriptionszeitschriften, ohne deren Funktion für die Wissenschaft zu gefährden. Studien zeigen, dass mit den momentan für Subskription verwendeten Mitteln tatsächlich eine großflächige, mindestens kostenneutrale Umstellung auf sogenannte APC (Article Processing Charges) und damit Open Access möglich ist. In diesem Fall zahlt der Autor beziehungsweise die jeweilige Forschungsinstitution einmalig die tatsächlichen Kosten für Publikationsleistungen an den Verlag, wie in den bisher erprobten Open-Access-Geschäftsmodellen üblich. Die Geschäftsgrundlage der Verlage bleibt erhalten, im Ergebnis aber sind die Inhalte für alle offen verfügbar. Auch US-Unis schließen sich der Initiative an Open Access muss zum Regelfall des wissenschaftlichen Publizierens werden, wenn wir nicht weiterhin wertvolle Potenziale in der Wissenschaft verschenken wollen. Die Bundesregierung hat dies mit ihrer 2016 verabschiedeten Open-Access-Strategie bekräftigt. Ein starkes politisches Signal haben die EU-Fachminister gesetzt: Sie wollen bezogen auf die Forschung, die mit EU-Mitteln gefördert wird, hundert Prozent Open Access bis zum Jahr 2020 verwirklicht sehen. Darüber hinaus unterstützt die EU-Kommission die von der Max-Planck-Gesellschaft bereits vor einem Jahr gestartete Transformationsinitiative OA2020. Augenhöhe mit den Verlagen ist dabei zentral – und zwar auf internationaler Ebene. 78 Einrichtungen weltweit haben sich der Initiative bereits angeschlossen. Zum Beginn der 13. Berlin-Konferenz zu Open Access an diesem Dienstag kommen nun auch drei renommierte US-amerikanische Institutionen hinzu: die kalifornischen Universitäten Berkeley, Davis und San Francisco. Noch mehr Schwung für Open Access also. Zur Konferenz haben sich übrigens auch zahlreiche Fachverlage angemeldet, was mich freut. Schließlich sehen wir sie als Partner des Wandels. Der Autor ist Präsident der Max-Planck-Gesellschaft. Die MPG richtet am Dienstag und Mittwoch die 13. Berlin-Konferenz zu Open Access aus. Mehr als 220 Vertreter von Wissenschaftseinrichtungen aus 34 Ländern werden erwartet.",
            "sentiment_score": 0,
            "magnitude_score": 0
        },
        {
            "created": "2009-04-29",
            "from": "heise",
            "content": "Angriff der Ahnungslosen Deutsche Verlage fahren eine Kampagne. Neue Monopolrechte, Seit dem Erscheinen des so genannten Heidelberger Appells finden sich in deutschen Tageszeitungen zunehmend Artikel, in denen unter Verweis auf Google Books, YouTube, The Pirate Bay und Open Access politische Maßnahmen zur Wahrung von Urheberrechten gefordert werden. Kennzeichen sowohl des Appells als auch der Kampagnentexte in den Tageszeitungen ist einerseits ein Verquirlen verschiedenster Phänomene. Das zweite Merkmal dieser Texte ist die - offenbar damit zusammenhängende - Ahnungslosigkeit der Autoren, die auf dieses Thema losgelassen werden. Es scheint, dass für die Kampagne gerade jene hervorgezerrt wurden, die sich in den letzten Jahrzehnten alleine aus Zeitgründen mit kaum etwas außerhalb ihres Fachgebiets beschäftigen konnten, und die nun entsprechend gut als - wie man früher in solchen Fällen sagte - nützliche Idioten funktionieren. Mit ihnen wird Druck auf die Politik ausgeübt, damit den Verlagen neue Monopolrechte gewährt werden, die Autoren (und vor allem Naturwissenschaftlern) viel eher schaden als nützen. So durfte beispielsweise der Literaturwissenschaftler Roland Reuß, der auch Leiter des Editionsprojekts Historisch-Kritische-Franz-Kafka-Ausgabe ist (und der es angeblich nie verwunden hat, dass Zweitausendeins den ganzen Kafka für eine Packung Kaffee veröffentlichen durfte), im FAZ-Text Unsere Kultur ist in Gefahr seine Zensurforderung gegen Blogs und News-Aggregatoren allen Ernstes mit dem Schutz der Pressefreiheit in Artikel 5 des Grundgesetzes begründen und dazu noch ungestraft einen Nazivergleich anbringen. Weil es im (in dieser Hinsicht im Vergleich zum Internet unzivilisierten) Feuilleton kein Godwin-Gesetz gibt, ist trotzdem zu befürchten, dass dies nicht der letzte Text von ihm war, den er zu diesem Thema veröffentlichen darf. Der juristische Laie Reuß argumentiert in seinem Rundumschlag unter anderem für Softwarepatente, obwohl gerade sie ein Gegenmodell zum Urheberrechtsschutz darstellen und sich ihre vehementesten Gegner vom FFII gerade auf das Urheberrecht als Schutzinstrument berufen. Man darf (ohne sich hier zu weit aus dem Fenster zu lehnen) angesichts solch einer - vorsichtig formuliert - verworrenen Argumentation durchaus annehmen, dass der Literaturprofessor weder mit rechtlichen Fragen noch mit Produktionsvorgängen außerhalb seines ganz speziellen Fachs besonders gut vertraut ist. Ob die Unterzeichner seines Aufrufs für eine Mehrheit der Autoren sprechen, ist mehr als zweifelhaft: Selbst aus der VG Wort, die eher von Urheberrechtsmaximalisten dominiert wird, ist zu hören, dass sich viele Autoren darüber freuen, dass nicht mehr lieferbare Bücher noch eingesehen werden können und potentiell sogar Werbeeinnahmen abwerfen, während eher wenige Autoren, der Meinung sind, dass alte Bücher auf keinen Fall verfügbar sein sollten. Karrieremutter Kegel Allerdings ist Reuß auch nicht der radikalste Neo-Luddit: Für die Karrieremutter Sandra Kegel, so der Perlentaucher in einer ausgesprochen treffenden Zusammenfassung, spricht nach zwanzigjährigem Experiment alles gegen das Internet. Die Eva Herrmann der FAZ beklagt sogar, dass durch das Internet Trägerformen wie CDs verschwinden würden. Warum das schlecht sein soll, lässt sie in ihrem kulturpessimistischen Furor zwar offen, aber bemerkenswert ist doch, dass vor 20 Jahren ähnliche Schriften erschienen, die das Vinyl an die Stelle der CD und die wiederum an die Stelle des Internets gesetzt hatten. Wobei das Vinyl bisher alles andere als ausstarb und in den letzten Jahren sogar Zuwachsraten verzeichnen konnte. Mit dem kostenlosen Zugang zu schöpferischen Werken, so die Hessisch sprechende Romanistin in ihrer Polemik Unter Piraten2, wird der geistige Arbeiter bestohlen. Ob Kegel hier für eine Schließung von Bibliotheken argumentieren will? Dazu scheint der Artikel dann doch zu wenig durchdacht. Kegel holt nicht nur pauschal gegen alles vom World Wide Web bis hin zu Open Access aus und bezeichnet sowohl Google als auch The Pirate Bay als Texträuber, sie verunglimpft zudem die namhafte Copyrightkritikerin Pamela Samuelson mit einem (über wer weiß wie viele Recherche-Ecken) aus dem Zusammenhang gerissenen Zitat als vermeintliche Schwester im Geiste und gibt so ungewollt ein tatsächliches Argument dafür ab, dass für das Zitieren eventuell doch gewisse Maßstäbe gelten sollten. Samuelson jedenfalls könnte es kaum jemand verübeln, wenn sie gegen ihre Entstellung durch Kegel juristisch vorgehen würde. Wie wenig Ahnung Kegel von der Rechtswissenschaft hat, zeigt sie mit der Bemerkung, das geistige Eigentum sei der Knotenpunkt einer Kultur, die das Urheberrecht als Teil eines universellen Persönlichkeitsrechts betrachtet. Tatsächlich ist die Vorstellung von geistigem Eigentum sogar ziemlich schwer vereinbar mit der kontinentaleuropäischen Urheberrechtstradition. Die deutsche Rechtswissenschaft stellte das bereits fest, bevor dieser Begriff über die Wirtschaft und die Politik aus den USA und Großbritannien importiert wurde, wo es eben keine Urheberpersönlichkeitsrechte gibt, sondern ein Copyright. Für jemanden, der für stärkere Monopolrechte auf Inhalte wirbt, ist Kegels Text bemerkenswert identisch mit einem kurz vorher in der Zeit veröffentlichten von Susanne Gaschke. Auch diese promovierte Kinderliteratur-Expertin ist fast ausschließlich mit Familienthemen befasst und durfte auf der Titelseite verbreiten, dass die Umsonst-Mentalität des Netzes die Produktionsbedingungen von Kultur, Wissenschaft und Journalismus bedroht, weshalb die Kinderpornographie-Stoppschilder auch zur Durchsetzung von Immaterialgüterrechtsansprüchen eingesetzt werden müssten. Eine Idee, die so absehbar war, dass der kürzlich beschlossene Gesetzestext bereits darauf hingetrimmt wurde: Vor der Verabschiedung durch den Bundestag gab es eine kleine, aber weitreichende Änderung in § 8 Abs. 2 TMG-E. Beschränkte sich der Entwurfstext vom 1. April 2009 noch auf Kinderpornografie, so enthielt die Beschlussvorlage keine solche Einengung mehr. Kluft zwischen Naturwissenschaft und Feuilleton Vor allem an der in allen diesen Texten geäußerten Forderung nach Maßnahmen gegen Open Access zeigt sich, wie tief die Kluft zwischen Naturwissenschaft und Feuilleton mittlerweile ist. Bereits vor dem Ersten Korb der Urheberrechtsreform hatten Verlage behauptet, dass der Wissenschaft die Grundlagen entzogen würden, wenn sie (die Verleger) nicht neue Monopolrechte bekämen. Naturwissenschaftler, so wurde wahrheitswidrig suggeriert, würden für Beiträge in Zeitschriften und Büchern bezahlt, könnten davon sogar leben und würden deshalb forschen. Einer, der damals auf diese Propaganda hereinfiel, war der SZ-Literaturkritiker Ijoma Mangold. Seine Karriere ging trotzdem (oder vielleicht gerade deshalb) weiter. Demnächst wird er im ZDF Nachfolger von Elke Heidenreich. In Wirklichkeit sind die Feinde von Wissenschaftsverlagen wie Elsevier und Springer nicht Raubkopierer (die es in diesen Bereichen kaum gibt), sondern die eigenen Autoren. Open Access sichert Qualitäts- ebenso wie die Aufmerksamkeitsstandards und bietet keineswegs weniger, sondern deutlich mehr Publikationsfreiheit. Zudem sorgt die seit einigen Jahren verfügbare Option ganz nebenbei auch noch dafür, dass der Staat potentiell Subventionen an parasitäre Strukturen einsparen und sie stattdessen zum Einstellen neuer Wissenschaftler verwenden könnte. Dass Open Access als Selbsthilfeprojekt von Naturwissenschaftlern entstand, lag auch daran, dass der Bogen überspannt wurde: Besonders naturwissenschaftliche Verlage nutzten ihre Monopole, um die Abonnementpreise für Zeitschriften drastisch zu erhöhen und so nicht nur die Öffentlichkeit, sondern auch ihre eigenen Autoren immer stärker zur Kasse zu bitten: Letztere erhalten nämlich im Normalfall kein Honorar für ihre Texte. Elsevier beispielsweise erwirtschaftete so Monopolrenditen von 40 Prozent und mehr - nicht durch Wettbewerb, sondern durch ein Modell, bei dem die öffentliche Hand die Arbeitskräfte bezahlte und gleichzeitig für Phantasiepreise die Produkte ihrer Arbeit einkaufen musste, welche diese wiederum als Werkzeuge brauchten. Selbst die Qualitätskontrolle wurde noch von aus Steuern oder Studiengebühren bezahlten Naturwissenschaftlern ohne Verlagshonorar übernommen. Mit der Verbreitung entsprechender Software kam schließlich auch noch das Setzen und Layouten der eigenen Artikel hinzu. Elsevier blieb da eigentlich nur noch die politische Lobbyarbeit, bei der es der Verlag allerdings tatsächlich nicht an Kreativität mangeln ließ.",
            "sentiment_score": 0,
            "magnitude_score": 0
        },
        {
            "created": "2009-04-30",
            "from": "heise",
            "content": "FAZ gegen Marktwirtschaft Wie aus einem neoklassischen Preisbildungsmodell eine angemessene Beteiligung wird. Neue Monopolrechte, Manchmal wundert man sich doch, wer in Fächern wie Jura und Betriebswirtschaftslehre promoviert wird. Zu diesem Personenkreis gehört in jedem Fall der Berliner Rechtsanwalt und Honorarprofessor Dr. Jan Hegemann, der in der FAZ und beim Staatsmonopolverlag Juris einen Text ablieferte, der einer recht eigenwilligen Argumentationslogik folgt. Unter dem eines Kinoreißers der 1960er Jahre würdigen Titel Schutzlos ausgeliefert im Internet argumentiert Hegemann erst mit einem Recht, das es gar nicht gibt, nämlich einem geistigen Eigentum von Verlagen, um mit angeblichen Verletzungen dieses Rechts dafür zu plädieren, dass es vom Gesetzgeber neu eingeführt wird. Das macht neugierig, ob (und bei welchen Richtern) Hegemann mit solcherlei Logik Prozesse gewinnt. Man weiß es allerdings nicht und darf es auch nicht herausfinden, weil ein anderer Rechtsanwalt durchgesetzt hat, dass das Führen von Erfolgsstatistiken durch Prozessbeobachter per Gewaltschutzgesetz verboten werden kann. Die Arbeit von Recherche und Informationsdienste hält Hegemann genauso wie die von Pressespiegelherstellern für in Bausch und Bogen rechtswidrig. Wenn sie das wirklich ist, warum fordert Hegemann dann eine Gesetzesänderung? Weil er später indirekt einräumen muss, dass dies offenbar doch nur in seiner Phantasie der Fall ist und nicht vor Gericht: Selbst wenn - etwa wie im Falle von Rip-Offs ganzer Artikel - eine Urheberrechtsverletzung vorliegt, ist die Rechtsverfolgung schwierig. Der Verleger muss im Prozess gegen einen Verletzer das Bestehen ausschließlicher Nutzungsrechte an dem übernommenen Beitrag beweisen. Das ist aufwendig und scheitert spätestens dann, wenn der Journalist dem Verleger, was jedenfalls im Bereich der Tageszeitungen den gesetzlichen Normalfall darstellt, lediglich einfache Nutzungsrechte eingeräumt hat. Anders als der Heidelberger Appell suggeriert, geht es in der aktuellen Kampagne gar nicht um Urheber und deren alte Rechte, sondern um neue Rechte für Verlage. Allerdings nennt auch Hegemann das neue Monopol, das die Verleger seiner Forderung nach bekommen sollen, mit keinem Mal beim Namen und spricht stattdessen lieber von geistigen Eigentumsrechten. Dafür wird versucht, das böse Wort Google zuzuschieben, einer Firma, die angeblich eine Monopolisierung des Weltwissens betreibt. Allerdings fordert Google - anders als viele Verlage - keineswegs ausschließliche Rechte von den Autoren und beteiligt sie zu deutlich besseren Konditionen, als dies die meisten Zeitungen tun. In anderen Ländern, so Hegemann, gibt es vergleichbare Rechte der Verleger längst. Das erweckt den Eindruck, dass Deutschland hier ganz alleine keine Monopolrechte gewährt. Das Gegenteil ist jedoch der Fall. Für Beispiele muss Hegemann denn auch auf Länder wie Indien, Pakistan und Bangladesch verweisen. Ob die Presselandschaft in Dacca oder Islamabad aber durch diese zusätzlichen Monopolrechte wirklich so viel besser ist als hierzulande (und die Demokratie um so vieles stärker), das lässt er offen. Mit seiner Forderung entwertet Hegemann traditionelle Anspruchsmodelle auf geistiges Eigentum, denn der Verlag erbringt ja keineswegs eine schöpferische Leistung. Auf der Suche nach Argumenten muss der Rechtsanwalt deshalb abenteuerlich weit ausholen und behaupten: Schließlich adelt ein Verlag den einzelnen Beitrag allein dadurch, dass dieser unter der Marke einer bestimmten Zeitung oder Zeitschrift mit der daran geknüpften Qualitätserwartung erscheint. Das kann es geben. Aber häufig ist auch das glatte Gegenteil der Fall. Der Autor veröffentlicht in einer Zeitung oder bei einem Verlag, die oder der seinen Ruf potentiell schädigt, nimmt dies aber aufgrund einer Geldzahlung in Kauf. Warum einem Verlag oder einer Zeitung daraus besondere Monopolrechte erwachsen sollen, ist in jedem Fall nicht ersichtlich. Den Druck und die Verbreitung der gedruckten Werke, so Hegemann, kann der Presseverleger nur erbringen, wenn er dafür bezahlt wird: durch den Kaufpreis für das Zeitungs- oder Zeitschriftenexemplar oder durch Werbung. Wo aber für die Nutzung weder ein Kaufpreis noch eine Einnahme für Werbung erzielt werden kann und stattdessen die Gratisnutzung im Internet die Einnahmen aus dem Printvertrieb kannibalisiert, muss der Verleger im Ergebnis wirtschaftlich scheitern. Darauf, dass sich im Kapitalismus nicht so einfach wirtschaften lässt, kamen auch schon andere. Doch statt im Sinne Schumpeters innovativ zu werden, kreativ zu sein, fordert die FAZ lieber das Ersetzen der Marktwirtschaft durch einen Neofeudalismus mit staatlich verliehenen Regalien. Damit soll dann eine angemessene wirtschaftliche Beteiligung durchgesetzt werden. Ein interessanter Gedanke: denn wenn es nicht mehr auf eine Schöpfungshöhe ankommt, dann könnte eigentlich jeder Industriearbeiter bald auf die Idee kommen, 50 Jahre währende Monopolrechte an allen Produkten zu verlangen, bei denen er mitgewirkt hat, damit er eine angemessene wirtschaftliche Beteiligung fordern kann. Möglicherweise müsste man dann in Zukunft beim Drücken jeder Türklinke Galvaniseure und Stanzer um Erlaubnis fragen, weil diese (gerade in Zeiten einer Wirtschafts- oder einer Strukturkrise) auch gerne eine angemessene Beteiligung hätten. Ob so etwas der Produktion oder dem Fortschritt dienlich wäre, ist allerdings fraglich. Wie Konkurrenten zu Raubkopierern gemacht werden sollen Die besonders exponierten Stellung der Frankfurter Allgemeine Zeitung, die am Samstag gleich zwei Polemiken zu dem Thema abdruckte, ist unter anderem deshalb bemerkenswert, weil ihre Argumentation in dieser Frage relativ konträr zu ihren sonstigen wirtschaftspolitischen Positionen verläuft. Tatsächlich diagnostiziert die Zeitung hier nicht nur ein angebliches Marktversagen zu ihren Ungunsten, sondern fordert auch eine Aufgabe der Ermittlung von Gewinn und Verlust durch einen Markt zugunsten einer angemessenen Beteiligung über staatlich verliehene Monopolrechte. Der Blogger Robin Meyer-Lucht merkte zur Beteiligung des Springer-Verlages an der Kampagne trocken an, dass in einer Marktwirtschaft, auf die man gerade im Hause Springer viel hält, der Kunde maßgeblich mitbestimmt, welche Vergütung angemessen ist, und nicht die juristische Abteilung. Wie bereits angedeutet, ist die in den Kampagneartikeln verbreitete Vorstellung unrichtig, das neue Monopolrecht würde notwendig sein, um gegen Raubkopierer vorzugehen. Gerade die Musikindustrie verfügt nämlich seit langem über ein solches Leistungsschutzrecht, ohne dass es ihr beim Problem nicht lizenzierter Kopien weitergeholfen hätte. Tatsächlich dient das neue Leistungsschutzrecht auch ganz anderen Zwecken, die allerdings eher im Stillen und abseits der Kampagnen ausgesprochen werden. Relativ klar formulierte es der norddeutsche Zeitungsverlegerverbandsvorsitzende A. Ashgar Azmayesh. Ihm zufolge ist ein Leistungsschutzrecht für Presseverlage notwendig, um Online-Inhalte vor der Auswertung durch andere Anbieter zu schützen. Kopfzerbrechen machen Zeitungsverlagen nämlich nicht (wie etwa der Musikindustrie) in erster Linie Personen, die Artikel kopieren, sondern Blogs, die sich nicht an Gepflogenheiten halten, und Nachrichten mit Informationen zugänglich machen, welche die Zeitungen aus den verschiedensten Gründen verschweigen. Es ist also eine erwachende Konkurrenz, gegen die neue Leistungsschutzrechte helfen sollen, und nicht diffuse Raubkopierer. Deshalb ist auch das Argument Makulatur, ein darüber angeblich gesicherter Qualitätsjournalismus sei unentbehrlich für die Demokratie. Tatsächlich wurde Meinungsvielfalt im Fernsehen, aber auch in einer sich zunehmend selbst zensierenden Presselandschaft mehr oder weniger zu einer Fiktion. Dafür findet sie sich in Blogs und Foren wieder - aufgrund der relativ umfassenden Zensurmittel wie dem Abmahnrecht allerdings häufig in den USA gehostet, wo die Redefreiheit mehr gilt. Monopolrecht vs. Kulturflatrate WIPO-Generaldirektor Francis Gurry sprach beim Bundesverband der Deutschen Industrie (BDI) zum Tag des geistigen Eigentums etwas ehrlicher als die FAZ davon, dass fraglich sei, inwieweit ein marktbasiertes Modell zur Vergütung schöpferischer Leistungen heute noch funktionieren könne. Eine mögliche Konsequenz daraus wäre die Einführung einer Kulturflatrate, die beispielsweise für einen umfassenden Ausbau der Künstlersozialkasse genutzt werden könnte. Sie könnte direkt schöpferisch Tätigen zugute kommen und nicht jenen jetzt teilweise überflüssigen Institutionen, die im vordigitalen Zeitalter für die nun selbständig laufende Verbreitung von Inhalten sorgten. Solche Lösungen würden auch mit den Ergebnissen der Motivationsforschung konform gehen, nach denen hohe Profite für Wenige keinen Anreiz für kreative Leistungen bieten, während dagegen eine gewisse soziale Absicherung solches durchaus zu leisten vermag. Die Hamburger Kulturwissenschaftlerin Meike Richter brachte zudem die Idee ins Spiel, dass man Google dazu verpflichten könnte, in die Künstlersozialkasse einzuzahlen. Da der Konzern mit Inhalten, die von freien Autoren geschaffen werden, schon jetzt Geld verdient, wären auch die Grundlagen für solch eine Forderung gegeben. Doch weil solche Modelle Autoren unabhängiger von Verlagen machen würden, propagiert ein Teil dieser Mittelsmänner neue Monopolrechte, mit denen Autoren stärker an Verlage gebunden werden sollen: Folge einer Umsetzung der Kampagneziele wäre also nicht mehr Publikationsfreiheit, sondern weniger, weil der Verlag ja neue Rechte an Texten bekommt, die dann unter anderem dem Autor fehlen. Trotzdem wird die Kampagne wahrscheinlich nicht ohne Folgen bleiben: In einer Pressemitteilung vom 24. April gab Bundesjustizministerin Brigitte Zypries bereits bekannt, dass Sie den Heidelberger Appell unterstützt. Nach der vom Justizministerium veranstalteten Urheberrechtskonferenz am 7. und 8. Mai dürften entsprechende Entwürfe an die Öffentlichkeit gehen. Folge wäre wahrscheinlich, wie es Ralf Bendrath in Netzpolitik formulierte, ein weiteres und noch stärkeres Auseinanderklaffen von Recht und Rechtswirklichkeit, das nicht in erster Linie Filesharer, sondern vor allem Blogger betreffen würde und durch das die Idee des Rechts als legitimem Selbststeuerungsmechanismus der Gesellschaft insgesamt in Gefahr gerät. Zudem, so Bendrath, würde über Umgehungsstrategien und -technologien eine neue digitale Spaltung erzeugt - zwischen einer neuen Info-Elite, die weiss, wo sie sich ihre Informationen beschaffen und ungestört kommunizieren kann, und denen, die das nicht können. Ob das, so der Blogger, für eine demokratische Gesellschaft und ihre Ideale von (Chancen-)Gerechtigkeit förderlich ist, wage ich zu bezweifeln.",
            "sentiment_score": 0,
            "magnitude_score": 0
        },
        {
            "created": "2009-05-18",
            "from": "heise",
            "content": "Google Books, Open Access und der Heidelberger Appell: Unklarheiten bei den Unterzeichnern Der viel diskutierte Heidelberger Appell von Roland Reuß, der sich u. a. gegen Googlebooks und das Open-Access-Modell wendet, rühmt sich der Unterstützung durch zahlreiche Autoren. Die Befragung von 10 Der viel diskutierte Heidelberger Appell des Germanistik-Professors Roland Reuß, der sich u. a. gegen Google Books und das Open-Access-Modell wendet, rühmt sich zahlreicher Unterstützer unter prominenten Autoren, die den Aufruf Für Publikationsfreiheit und die Wahrung der Urheberrechte unterzeichneten. Das hat für eine große Medienresonanz gesorgt und zu Kampagnen in verschiedenen Zeitungen wie der FAZ geführt. Dabei wurden unter Verweis auf Google Books, YouTube, The Pirate Bay und Open Access politische Maßnahmen zur Wahrung von Urheberrechten gefordert, aber unterschiedlichste Themen vermischt. Nachdem der Schriftsteller Peter Glaser (Schönheit in Waffen) seine Unterschrift unter dem Heidelberger Appell zurückzog und erklärte, dass er aus Skepsis über die Google-Aktivitäten unterschrieben habe und keineswegs die im Appell vertretene Meinung zu Open Access teile, bat Telepolis zehn prominente Unterzeichner telefonisch um ihre Stellungnahme: Hans-Magnus Enzensberger, Alexander Kluge, Thomas Meinecke, Thomas Palzer, Klaus Theweleit, Eva Demski, Uwe Timm, Matthias Matussek, Hajo Jahn und Bascha Mika. Die Antworten unterschieden sich teilweise deutlich von den Forderungen des Initiators Roland Reuß und der Interpretation von Bundesjustizministerin Brigitte Zypries (siehe den Telepolis-Artikel: Willenserklärungs-Exegese. Deutlich wurde, dass sich die befragten Unterzeichner wohl nicht mit allen Inhalten des Appells beschäftigt beziehungsweise die darin enthaltenen unterschiedlichen Interessen bemerkt hatten. Die von Justizministerin Brigitte Zypries als Reaktion auf den Appell geplanten neuen Leistungsschutzrechte für Verlage sehen sie überwiegend nicht als Lösung der Probleme. Vielmehr sprachen sich alle der angesprochenen Autoren für eine Stärkung von Autorenrechten gegenüber Verlagen aus, wovon im Justizministerium bisher allerdings noch nicht die Rede ist. Zusammengefasst könnte man sagen, dass sogar Deutschlands Eliteliteraten mit ihrer Unterschrift verhältnismäßig locker umgehen, so lange die Schlagworte stimmen, betont der Telepolis-Redakteur Peter Mühlbauer.",
            "sentiment_score": 0,
            "magnitude_score": 0
        },
        {
            "created": "2009-03-26",
            "from": "faz",
            "content": "Open-Access-Foren : Gratislesen Open Access sei keine Kostenloskultur und beschneide keine Urheberrechte, sagt die „Allianz der Wissenschaftsorganisationen“ und reagiert so auf den Heidelberger Appell. Diese Foren seien sogar von Vorteil, da die Autoren dadurch öfter zitiert und schneller veröffentlicht würden. Die organisierte Spitze der deutschen Forschung, die „Allianz der Wissenschaftsorganisationen“, hat sich in einer Antwort auf den von Schriftstellern, Verlegern und Forscherkollegen initiierten „Heidelberger Appell“ massiv gegen die „unakzeptable Unterstellung“ verwahrt, mit der Förderung eines freien elektronischen Zugangs zu Artikeln die Publikationsfreiheit beschneiden zu wollen. „Open access“, der freie Zugang zu Publikationen, werde ausschließlich von Forschungsergebnissen gefordert, die aus Steuermitteln finanziert und „damit zum Nutzen der Forschung und Gesellschaft insgesamt erarbeitet wurden“. In diesem Halbsatz steckt die moralische Begründung, wieso sich zumindest in vielen kostenträchtigen Sparten der Naturwissenschaften und der Medizin seit acht Jahren tatsächlich so etwas wie ein Imperativ der Transparenz ausbreitet. Dabei ist Open Access zunächst weder eine Google- noch eine Kostenloskultur. Im Gegenteil. Der Autor, der seine Ergebnisse zunächst sorgfältig begutachtet haben will – was für seine Karriereplanung absolute Priorität hat –, muss für die Internetpublikation je einige bis viele tausend Dollar an das Journal überweisen. Die belasten das Budget des Instituts, nicht aber jenes der Leser. Die heimliche Hoffnung ist natürlich, auf diesem Wege des freien Publizierens erstens öfter zitiert zu werden (Karriereplanung) und zweitens schneller veröffentlicht zu werden. Keine Verletzung des Urheberrechts Tatsächlich haben sich mit der gewaltigen Steigerung des Forschungsdurchsatzes weltweit bei gleichzeitig begrenzten Ressourcen der hochwertigen Verlage enorme Engpässe ergeben. Die Open-Access-Foren gelten da als willkommene Zusatzoption. Zumal die klassischen Hefte mit Einzelabopreisen von zwanzigtausend Euro oder mehr vielen Bibliotheken zu teuer sind. Auf mehr als dreitausend frei zugänglicher Publikationsforen ist der neue Wissensmarkt inzwischen gewachsen. Von den jährlich gut 1, 3 Millionen Wissenschaftsartikeln, so haben finnische Experten jüngst ermittelt, wurden gut acht Prozent – Stand 2006 – sofort oder mit zeitlicher Verzögerung vom Herausgeber frei ins Netz gestellt. Weitere elf Prozent waren auf persönlichen Homepages der Forscher oder in elektronischen Datenbanken der Institutionen im Volltext einsehbar. Ein Fünftel also des Forschungsoutputs ist schon jetzt unbegrenzt verfügbar. Und kaum einer in der Szene, die Fachverlage eingeschlossen, die längst mit neuen Geschäftsmodellen wie Springers „Open Choice“-Modell nach Antworten suchen, zweifeln an der weiteren Ausbreitung. Über eine Freiheitsberaubung jedenfalls, was die Publikationsmöglichkeiten angeht, hat niemand geklagt. Die energische Reaktion, die in der gemeinsamen Erklärung von Deutscher Akademie Leopoldina, Max-Planck-Gesellschaft, Deutscher Forschungsgemeinschaft, Helmholtz-Gemeinschaft und der sechs anderen Großorganisationen zum Ausdruck kommt, dürfte deshalb vor allem den „Vorwurf der Enteignung des Urhebers“ betreffen. Er ist vor allem Gegenstand des Heidelberger Appells. Von einer Verletzung des Urheberrechts mit Open Access könne hingegen gar keine Rede sein, meint die Allianz, denn der Autor behält seine je national festgelegten eigenen Urheberrechte auf Wort und Bild. Im Unterschied, wäre hinzuzufügen, zu den Verhältnissen in den traditionellen kommerziellen Verwertungsketten.",
            "sentiment_score": 0,
            "magnitude_score": 0
        },
        {
            "created": "2008-12-04",
            "from": "spiegel",
            "content": "Wikipedia 100.000 historische Fotos aus dem Bundesarchiv online Eine Kooperation zwischen Wikipedia und dem Bundesarchiv setzt auf die Weisheit der Massen: 100.000 Fotos werden öffentlich verfügbar und im Gegenzug mit Personendaten verknüpft. Erstmalig will sich das Bundesarchiv mit Wikipedia zusammentun. Die Kooperation umfasst 100.000 digitalisierte Fotografien aus den Arsenalen der Staatssammlung, die künftig bei Wikimedia Commons, eine Zentralstelle für freie Mediendateien, unterkommen sollen. Das Bundesarchiv hat den gesetzlichen Auftrag, Archivgut öffentlich zur Verfügung zu stellen, erläutert Oliver Sander, Leiter des Referats Bilder, Karten, Töne beim Bundesarchiv. Wie könnte man das besser machen als über Wikipedia? Jetzt kann man also online zurückblicken: 1906 posierten Angehörige des Wasaramo-Stammes im ehemaligen Kolonialgebiet Deutsch-Ostafrika vor der Kamera. 1960 empfing Bundespräsident Heinrich Lübke Thailands attraktive Königin Sirikit. 1987 zog das Kombinat Robotron kleine Wägelchen mit volkseigenen Computern zur 750-Jahrfeier durch Ostberlin. Von all dem gibt es Bilder - bloß konnte bislang kaum jemand diese Fotos sehen. Sie lagern im Bundesarchiv in Koblenz hinter dicken Wänden. Die fangen nun langsam an zu bröseln. Der beherzte Schritt in Richtung Öffentlichkeit und Öffnung folgt dem Open-Access-Gedanken der Berliner Erklärung über den offenen Zugang zu wissenschaftlichem Wissen (PDF-Dokument) aus dem Jahr 2003. Diesem Gedanken fühlt sich nun auch das Bundesarchiv verpflichtet. Erst im September 2007 hatte man einen Teil der eigenen Bildersammlung online gestellt. Mit dem Zeitgeschichte-Portal von SPIEGEL ONLINE, einestages, kooperiert das Nationalarchiv ebenfalls schon seit Ende 2007. Nun macht macht das Archiv gewissermaßen den nächsten Schritt auf die Bürger zu. Mit Hilfe von Wikipedia kann der Bekanntheitsgrad unserer eigenen Seite deutlich gesteigert werden, sagt Sander. Kostenlose Nutzung Zunächst musste man sich jedoch auf einen Modus operandi einigen. Für Wikimedia kamen nur Creative-Commons-Lizenzen in Betracht. Sie regeln die freie Nutzung von Bildmaterial, ohne eine kommerzielle Nutzung auszuschließen. User können fortan Bilder aus dem Bundesarchiv, unter Angabe von Quelle und Urheber, kostenlos in ihre Website oder eine persönliche Präsentation einbinden. Bei kommerzieller Nutzung muss ein Entgelt entrichtet werden. Wir haben gesehen, dass es möglich ist, einen Teil der Bilder vom Bundesarchiv unter freier Lizenz herauszugeben, sagt Mathias Schindler von der Wikimedia Foundation. Gleichzeitig bieten wir an, die Personendaten im Bundesarchiv mit Wikipedia-Artikeln und der sogenannten Personennamendatei zu verknüpfen. Ein attraktives Angebot für das Bundesarchiv. Denn dort wurde bislang nicht mit der Personennamendatei (PND) gearbeitet, weil schlicht die Manpower fehlte. Sie wird von Bibliotheken eingesetzt und beinhaltet eine Identifikationsnummer, die auf einen Datensatz mit Angaben zu Personen von öffentlichem Interesse verweist. Jetzt sitzen die fleißigen Wikipedianer daran, die vorhandenen Angaben des Bundesarchivs mit der Personennamendatei abzugleichen und zu ergänzen. Etwa 27 Prozent der 100.000 Fotos sind bereits verknüpft, wozu etwa zwei Monate Zeit benötigt wurden. Das Tempo wird noch zunehmen, wenn die Kooperation öffentlich wird, ist sich Schindler sicher. Darüber hinaus sollen in einer Art Bilderschnitzeljagd fehlende Angaben zu einzelnen Fotos, für die Spezialwissen notwendig ist, durch die User ergänzt werden. Die vielbeschworene Weisheit der Massen kommt hier zum Tragen. Eine ähnliche Funktion hat übrigens das Fundbüro bei einestages. Das Bundesarchiv erhält auf diesem Weg nun saubere Daten, und Wikimedia vergrößert seinen Bestand an freien Fotografien, die jeder nutzen kann. Das Bundesarchiv besitzt phantastische Fotos aus Zeitabschnitten, auf die Wikipedia momentan keinen Zugriff hat, schwärmt Mathias Schindler, der in der Zusammenarbeit auch einen Testballon sieht, um zu schauen, ob nicht mit anderen Bereichen des Bundesarchivs ebenfalls kooperiert werden kann. Die Wikipedianer mögen sich bei dieser Arbeit vielleicht sogar zu neuen Einträgen in die Online-Enzyklopädie inspiriert fühlen. Die Wasaramo-Krieger fehlen noch.",
            "sentiment_score": 0,
            "magnitude_score": 0
        },
        {
            "created": "2010-07-13",
            "from": "sz",
            "content": "Psychologie: Konzentration lenkt ab Das berühmte Gorilla-Video hatte es bewiesen: Wir übersehen selbst die auffälligsten Dinge, wenn wir uns auf etwas anderes konzentrieren. Jetzt gibt es das Nachfolge-Experiment. Dieses Experiment ist ein moderner Klassiker der Psychologie. Zählen Sie die Pässe, die sich die Spieler mit den weißen Hemden gegenseitig zuspielen. Ignorieren Sie die Basketballspieler in den schwarzen Trikots. Wer das erste Gorilla-Video kennt, der sieht natürlich auch im zweiten Video den Gorilla. Aber Diese Anweisung gab der Psychologe Daniel Simons von der Universität Illinois vor etwa zehn Jahren seinen Probanden, bevor sie sich ein Video ansahen. Die Teilnehmer zählten artig, wie oft der Ball hin und her gespielt wurde - und merkten in der Mehrzahl nicht, dass während des Spiels ein Mann im Gorilla-Kostüm zwischen die Spieler schlurfte, sich auf die Brust trommelte und dann aus dem Bild ging. Nun legt Simons gemeinsam mit dem Psychologen Christopher Chabris vom Union College in New York noch einmal nach: Wir empfehlen, das Experiment unbedingt selbst zu machen: Sie finden das Video hier. Reingelegt Menschen entgehen sogar dann extrem offensichtliche Details und Begebenheiten, wenn sie eine Überraschung erwarten. Die Psychologen zeigten ihren Probanden abermals einen Gorilla-Film. Die Hälfte der Testpersonen wusste, dass gleich ein Mann im Affen-Kostüm auftauchen würde. Niemand von ihnen übersah den Gorilla, dafür entgingen 83 Prozent der Probanden nun andere Details. So verließ einer der drei schwarzgekleideten Basketballspieler das Bild, als der Gorilla auftauchte. Zudem veränderte sich des Farbe des Vorhangs, vor dem die Spieler standen. Etwa 50 Prozent derjenigen, die zuvor nichts vom Mann im Affen-Kostüm wussten, übersahen diesen zwar, dafür entgingen nur 71 Prozent von ihnen die anderen Details. Das ist immer noch eine miserable Quote, aber sie liegt über der Leistung der anderen Gruppe. Der Mensch lasse sich sogar dann hereinlegen, wenn er wisse, dass dies gleich geschehe, kommentiert Simons. Die Studie soll demnächst als erste Veröffentlichung in dem neuen Open-Access-Journal i-Perception erscheinen.",
            "sentiment_score": 0,
            "magnitude_score": 0
        },
        {
            "created": "2009-05-31",
            "from": "tagesspiegel",
            "content": "Elektronische Medien In der Dämmerung des Gutenberg-Zeitalters Noch sind das Buch und die Zeitung sehr lebendig. Aber die digitale Ära wird die alte Print-Kultur verändern. Am 8. Mai 2009 gedachte das freitägliche Magazin der „Süddeutschen Zeitung“ schon auf dem Titel einer epochalen Zäsur: „Das Internet macht Druck, die Auflagen schwanken, die Einnahmen sinken: Die klassischen Medien müssen sich der größten Sinnkrise ihrer Geschichte stellen.“ Also fragte das Heft der Zeitung: „Warum Zeitung?“ Auch im Fernseh- und Internetzeitalter sind die klassischen Medien noch immer die gedruckte Zeitung und das gedruckte Buch. Doch die großen Papiertiger sind erschüttert. Im letzten Jahr häuften sich die Meldungen über immer neue Gewitterzeichen bei internationalen Zeitungs- und Zeitschriftenverlagen. Am spektakulärsten dabei die Millionenverluste und Kürzungen beim Flaggschiff der amerikanischen Presse, der stolzen „New York Times“. Die „Times“ musste sogar das eigene Haus im Zentrum von Manhattan verkaufen und von einem Investor zurückmieten. Eine Demütigung, ein Menetekel? Manche hielten das bis vor kurzem nur für eine Folge der allgemeinen Wirtschaftskrise. Gewiss, sagten sie, gibt es seit Jahren das Onlineproblem. Zeitungen und Zeitschriften machen sich mit ihren immer besseren Internet-Portalen selber immer heftiger Konkurrenz, ohne damit wirklich Geld zu verdienen oder so nennenswert neue Leser auch für ihre gedruckten Hauptprodukte zu gewinnen. Hinzu kommt, dass die zu etwa 60 Prozent von ihren Anzeigenerlösen abhängigen Printmedien einen Teil der klassischen Einnahmen an die Internetmärkte verloren haben. Solche Sorgen aber hatten den Sinn der Zeitung nicht grundsätzlich infrage gestellt. Die elektronischen Medien mögen schneller sein, doch sie gelten bislang auch als oberflächlicher. Hintergründe, tiefer schürfende Analysen, brillantere Kommentare, verbindliche Gewichtungen – das sind die angeblich unersetzlichen Vorzüge der Zeitung, und im kulturellen Kontext gilt dies umso mehr noch für das gedruckte Buch. Zwar war auf der Frankfurter Buchmesse im Oktober 2008 das E-Book mit seinem auch im Sonnenlicht flimmerfreien, angenehm zu lesenden Bildschirm ganz plötzlich das große Medienthema. Doch kaum jemand hat einen der Prototypen des digitalen Buchs, das im Taschenformat den Inhalt kleinerer Bibliotheken zu speichern weiß, schon leibhaftig in der Hand gehabt. In Frankfurt musste man es fernab der großen Hallen und Verlage an entlegensten Ständen suchen. Das wirkte bei der Leipziger Buchmesse im März 2009 schon anders. Das eben noch virtuelle Medium gilt nun als Realität, und die E-Book-Macher kommen der Simulation eines gedruckten Buchs immer näher, inzwischen gibt’s schon digitale Eselsohren zum Seiten-Markieren. Die Dämmerung des fünfeinhalb Jahrhunderte währenden Gutenberg-Zeitalters ist so fortgeschritten. Der Brockhaus, in Deutschland einst Inbegriff der lexikalischen Buchkultur, erscheint künftig nur noch digital. Das war, das ist nur der Anfang. Kürzlich hat der Wissenschaftsjournalist und Darwin-Biograph Jürgen Neffe in einem Großaufsatz in der „Zeit“ das bevorstehende Ende der klassischen Druck-Kultur verkündet. Unterm Titel „Es war einmal“ und mit dem evolutionären Zusatz „Kein Grund zur Trauer“. Wovon Autoren wie er selber freilich künftig leben werden im schrankenlosen Reich des urheberrechtlich kaum zu kontrollierenden, gegen Raubkopien nie gefeiten Internets, das konnte auch Neffe nicht so recht erklären. Nur für die Zeitungen, diese „vierte Gewalt“ der Demokratie, hatte er eben jene Idee, die früher schon der Sozialphilosoph Jürgen Habermas ins Spiel gebracht hatte. Man müsse, sagt Neffe, bei einer so „systemrelevanten Branche“ wie der freien Presse künftig über „öffentlich-rechtlichen Printjournalismus“ nachdenken, zumindest über staatliche Subventionen wie beim Theater, bei Museen oder „Autobahnen“. Radikaler sieht die Sache dagegen Sascha Lobo. Der 33-jährige Berliner Superblogger und Autor eines Schlüsselwerks über die „digitale Bohème“ verkündete jüngst im Sonntags-Interview des Tagesspiegels zwar noch nicht allen von ihm so genannten „Holzmedien“ den Untergang. Aber: „Den Tageszeitungen auf jeden Fall.“ Nur für besondere Magazine und Bücher werde es weiter „Nischen“ geben. Dafür bringt er, wie Jürgen Neffe, den Vergleich mit der guten alten Schallplatte. Diese Analogie indes trifft daneben. Beim Wechsel von Schellacks zu CDs und Chips, die allemal Tonträger sind, ging es nur um die technologische Verfeinerung des im Prinzip identischen Mediums. Wenn aus gedruckten Zeitungen und Büchern dagegen Bildschirmbotschaften werden, bleiben Schrift und Bild zwar erhalten, doch das Medium und die Lesekultur sind verschieden. Bis vor kurzem, in der Gutenberg-Galaxis, galt die digitale Welt des Googelns, Surfens und Mailens, der Websites und der Onlineportale nur als willkommener Zusatz. Nie als Ersatz. Neuerdings aber wird die elektronische Parallelwelt schon als das neue, das eigentliche Universum gesehen. Eben deswegen reagieren jetzt immer mehr Autoren erschrocken und wütend: wie zuletzt beim „Heidelberger Appell“, dem Protest gegen die Anmaßung des Google- Konzerns, Texte künftig pauschal und universell zu verwerten. Unternehmen wie Google versuchen immerhin, ihre Geschäftsbasis noch juristisch abzusichern. Dagegen ist gegen die allgemeine Internetpiraterie längst kein Kraut mehr gewachsen. Trotz einzelner erfolgreicher Gerichtsprozesse gegen feste Tauschbörsen hat die Musik- und Filmindustrie ihren Kampf gegen das Raubkopieren längst verloren. Wer will und weiß, der kann sich die Attraktionen der jüngst zu Ende gegangenen Filmfestspiele von Cannes mit allen Tarantinos schon jetzt auf den heimischen Laptop runterladen. Und so illegal geht’s (demnächst) auch mit den E-Books, deren Inhalte zugleich aufs Handy, in den Blackberry oder das i-Phone wandern können. Weshalb manche Kenner den Kosmos der elektronischen Bücher in zwei Visionen beschreiben. Als Welt von morgen – und schon jetzt von gestern. Die Printverlage, ob Zeitungshäuser oder Büchermacher, stehen vor dem nämlichen Dilemma. Sie wollen oder müssen mitspielen in der unheimlich schönen neuen Welt. Aber noch keiner hat den goldenen Schlüssel und dazu das Schloss gefunden, das die eigenen Produkte wirtschaftlich wirksam sichern könnte: vor Netzpiraten oder dem von Internet-Lobbyisten geforderten, heute ohnehin fast überall bestehenden „open access“, dem kostenlosen Zugang für jedermann. In diesen Wochen sucht gerade die angeschlagene „New York Times“ nach einem Modell, wie sie ihre Website künftig nur noch gegen Geld öffnen kann, ohne gleichzeitig die vor allem jungen Leser ihrer Online-Ausgabe zu verlieren. Favorisiert wird dabei die Gründung einer „NYT Community“, deren Mitgliedern außer zusätzlichen Informationen noch besondere Events und ermäßigte Tickets für Unterhaltung, Kultur und Sport geboten werden. Zeitungen, auch in Deutschland, müssen neben dem journalistischen Kerngeschäft immer mehr lebensnahe Derivate bieten. Das hauseigene Weingeschäft, die spezielle DVD-Edition, die klassische Leserreise sind da nur der Beginn einer allgegenwärtigen Entwicklung. Doch die Hauptfrage bleibt: Wozu die Zeitung? Warum noch das Buch? In Amerika hat die Wirtschafts- und Medienkrise bereits zahlreiche Traditionsblätter, vom „Boston Globe“ bis zum „San Francisco Chronicle“, an den Rand der Existenz gebracht; viele US-Printtitel wechseln nun ganz ins Internet, und etliche Millionenstädte in Amerika werden bald keine eigene Tageszeitung mehr haben. Zur selben Zeit ist der politische und intellektuell-kulturelle Einfluss der Online-Medien seit der Kampagne von Barack Obama immens gestiegen. Bei der letzten Präsidentenwahl hat die Netzzeitung „Huffington Post“ bereits eine prägendere Rolle gespielt als die gedruckte „Washington Post“. Längst also gibt es Qualitäten, gibt es journalistische, künstlerische, intellektuelle Spitzenleistungen im Internet, denen das kulturelle Bild eines nur schnellen, flachen Mediums nicht gerecht wird. Das haben auch so unterschiedliche Geister wie der Thrillerkönig Stephen King, die Literaturnobelpreisträgerin Elfriede Jelinek oder der politische Essayist Slavoi Zizek verstanden, die es sich leisten (können), einen Teil ihrer neuen Werke frei ins Netz zu stellen. Im Internet findet sich jetzt auch eine der Gründungsurkunden des modernen Romans: Französische Literaturwissenschaftler haben soeben alle 4500 Manuskriptseiten von Gustave Flauberts „Madame Bovary“ mit Korrekturen, Transkriptionen und Kommentaren im Netz präsentiert – eine Ausgabe, die auf Papier kaum mehr bezahlbar wäre. Oder das: Der gerade 70 gewordene Harvard-Professor Robert Darnton, einer der bedeutendsten Kulturwissenschaftler unserer Zeit, arbeitet seit mehr als zehn Jahren an einem elektronischen „Überbuch“, das ein Haus des Wissens bildet, mit unzähligen Geschossen und Verbindungen, mit Portalen und Links im Horizontalen und Vertikalen. Etwas, das sich auf Papier zwischen zwei Buchdeckeln räumlich und geistig nicht mehr darstellen ließe. Der große alte Darnton sagt: „Ich habe zu viel Informationen.“ Eben dies berührt freilich auch einen medien- und bildungspolitisch entscheidenden Punkt. Mit zu viel Informationen droht für gewöhnlichere Geister so etwas wie ein „weißes Rauschen“. Eine Informationsgesellschaft ist noch keine Wissensgesellschaft, das erkennt man bereits an jener Internet-Generation, die statt in Bücher und Zeitungen fast nur noch auf Bildschirme schaut. Das Haptische eines Buchs oder einer Zeitung wird nun oft beschworen. Wie deren letztes, sinnlich-sinnhaftes Privileg. Das trifft zu beim Blättern, bei der individuellen Bearbeitung und Aneignung, bei der Wahrnehmungsästhetik einer (Doppel-)Seite oder eines Kunstdrucks. Dagegen ist das Internet unschlagbar im Aktuellen, beim Googeln von Sachinformationen; auch das E-Book ist mit all seiner im Handformat gespeicherten (Sekundär-)Literatur ein immenser Vorteil für Studenten, Forscher und Sachbuchleser. Wo es aber nicht um zweckgerichtete Lesearbeit geht, sondern um Poesie, Kunst und Genusslektüre, da bleibt die Elektronik dem gedruckten Papier materiell und spirituell unterlegen. Das gilt in der Essenz auch für die Zeitung. Online bedeutet im Tagesjournalismus: die gleichförmige, gleichzeitige, grenzenlose Flut des Geschehenen, des unaufhörlich Neuen. Gute Online-Portale sortieren, gewichten, kommentieren das auch. Trotzdem liegt dem Medium mit seiner hintereinander geschalteten Abfolge der nie endenden Aktualität etwas potentiell Unbegrenztes, letztlich Undefiniertes zu Grunde. In der Wertung und Gewichtung, in der auf jeder Zeitungsseite schon visuell zu Tage tretenden Hierarchie der Themen und Argumente, nicht im möglichst besseren Text, sondern erst im Kontext, liegen die medialen Stärken der Zeitung. Sie ist immer von gestern. Aber sie hält die Zeit auch mit jeder neuen Ausgabe symbolisch fest; man nimmt sich als Leser die Zeit mit der Zeitung – anders als beim Dauerdurchlauf im Internet. Die Vermutung ist: Bürger einer Wissensgesellschaft brauchen trotz aller aktuellen Online-Information eben dieses „Es war einmal“. Sie brauchen Geschichten wie Kinder Märchen. Wobei das Gestrige in der Zeitung zugleich ins Präsens gehoben wird („Merkel erklärt“, „Hertha gewinnt“, „Vulkan bricht aus“). Die Zeitung verkörpert jene imaginäre Gegenwart, die in der Echtzeit online oder im Fernsehen immerzu flüchtig ist. Dabei steigen die Auflagen der Zeitungen gerade dann, wenn am Vortag etwas geschehen ist, über das die anderen, aktuelleren Medien schon besonders viel berichtet haben (eine Wahl, ein Verbrechen, ein Endspiel). Kein Wunder – oder vielleicht gerade ein Wunder: Wer selber bei einem Ereignis dabei war oder über eine allgemeine Sensation schon vermeintlich alles erfahren hat, sucht häufig genau das am nächsten Tag noch einmal in der Zeitung: als gegenwärtige Reflexion des Gestrigen, des „Es war einmal“. Erst wenn es auch in der Zeitung steht, war es wohl wirklich.",
            "sentiment_score": 0,
            "magnitude_score": 0
        }
    ]
}